---
slug: ml-textbook-3
title: 머신러닝 분류 모델들
description: 머신러닝에 사용하는 다양한 분류 모델들에 대해서 얘기해 보겠다.
thumbnail: 
categories:
  - ML
tags:
  - SVM
  - 커널-SVM
  - 결정-트리
  - 랜덤-포레스트
createdAt: 2025/01/06
updatedAt: 
featured: true
---
머신러닝에 사용하는 다양한 분류 모델들에 대해서 얘기해 보겠다.
# 서포트 벡터 머신 (SVM)
SVM(Support Vector Machine)이라 불리는 이 분류 모델은 클래스를 구분하는 hyper-plane(초평면)과 이 hyper-plane에 가장 가까운 훈련 샘플 사이의 거리로 정의한다.
이러한 샘플을 서포트 벡터라고 한다.
![](https://i.imgur.com/6A0ZkK8.png)


SVM의 최적화 대상은 마진을 최대화 하는 것이다. 여기서 마진이란, 양성 쪽 hyper-plane과 음성 쪽 hyper-plane 사이의 거리를 뜻한다.
마진을 최대화하면 일반화 오차가 낮아지는 경향이 있기 때문에 마진의 최대화가 SVM의 최적화로 이어진다.

<Callout type="info">
마진 최대화를 정확하게 이해하기 위해서는 초평면들의 수식 계산을 해야 하는데 결과적으로 $\frac{2}{\|w\|}$가 최대화하고 싶은 마진을 의미한다는 결론이 도출된다.
따라서 SVM의 목적 함수는 샘플이 정확하게 분류된다는 제약 조건 하에서 $\frac{2}{\|w\|}$를 최대화함으로써 마진을 최대화하는 것이다.
</Callout>

결론적으로 SVM은 모든 양성 클래스 샘플은 양성 쪽 초평면 너머에, 모든 음성 클래스 샘플은 음성 쪽 초평면 너머에 있도록 하는 것이고 이를 수식으로 정리하면 다음과 같다.
$$
w_0 + w^Tx^{(i)} \geq 1 \; y^{(i)}=1일 \,때\\
w_0 + w^Tx^{(i)} \leq -1 \; y^{(i)}=-1일 \,때\\
i=1 ... N 까지
$$
## SVM의 슬랙 변수
SVM은 선형적으로 구분되지 않는 데이터에서 한계가 있습니다. 이를 해결하기 위해 슬랙 변수 $\zeta$를 사용하여 선형 제약을 완화시킨다.
이를 소프트 마진 분류라고 부른다. 슬랙 변수를 추가하여 수식화하면 다음과 같다.
$$
w_0 + w^Tx^{(i)} \geq 1-\zeta^{(i)} \; y^{(i)}=1일 \,때\\
w_0 + w^Tx^{(i)} \leq -1+\zeta^{(i)} \; y^{(i)}=-1일 \,때\\
i=1 ... N 까지
$$
## SVM의 하이퍼 파라미터 C
C 는 $\frac{1}{\lambda}$으로 규제 파라미터 $\lambda$의 역수이다. 이 C를 줄이면 규제가 강해지고 이는 SVM에서 분산은 줄어들고 편향은 커진다(마진이 줄어들기 떄문).
![](https://i.imgur.com/PTPS2i3.png)

```python
from sklearn.svm import SVC

# kernel='linear': 선형 분류 SVM을 사용하겠다.
# C=1.0 : 규제의 역수 하이퍼 파라미터
svm = SVC(kernel='linear', C=1.0, random_state=1)
svm.fit(X_train_std, y_train)

plot_decision_regions(X_combined_std, 
                      y_combined,
                      classifier=svm, 
                      test_idx=range(105, 150))
plt.xlabel('petal length [standardized]')
plt.ylabel('petal width [standardized]')
plt.legend(loc='upper left')
plt.tight_layout()
# plt.savefig('images/03_11.png', dpi=300)
plt.show()
```
![](https://velog.velcdn.com/images/jinsoolve/post/4f9fd9d8-403a-48f5-bc0d-110cbe8d5c9f/image.png)
# 커널 SVM
커널 SVM을 사용하면 비선형 분류 문제도 해결이 가능하다.
![](https://i.imgur.com/4QM5TD9.png)

위 그림처럼 비선형 분류 문제와 같은 경우는 일반적인 선형 SVM으로는 해결이 불가능한데 이를 **커널 방법**으로 해결할 수 있다.
## 커널 방법(Kernel Method)
쉽게 설명하면, 매핑 함수 $\phi$를 사용하여 비선형 데이터들을 선형 구분이 가능한 고차원 공간에 투영하는 것이다.
$$
\phi(x_1,x_2) = (z_1,z_2,z_3) = (x_1,x_2,x_1^2+x_2^2)
$$
![](https://i.imgur.com/7dM2ITm.png)

위에 그림에서 보는 것처럼 원래 선형으로 분리가 안 되는 데이터셋을 선형적으로 분리할 수 있게 되었다.
하지만 이러한 매핑 방식은 새로운 특성을 만드는 계산 비용이 매우 비싸다는 단점이 있다. 이를 커널 기법(kernel trick)으로 해결한다.
## 커널 기법(Kernel Trick)
수학적으로 여러 복잡한 과정이 있지만 여기서는 자세히 다루지 않고 간단한 과정을 요약하도록 하겠다.

> 실전에서 필요한 것은 점곱 $x^{(i)T}x^{(j)}$를 $\phi(x^{(i)})^T\phi(x^{(j)})$로 바꾸는 것입니다.
> 이떄 점곱을 계산하는데 드는 비용을 절감하기 위해 커널 함수를 정의한다.
$$
K(x^{(i)},x^{(j)}) = \phi(x^{(i)})^T\phi(x^{(j)})
$$
> 가장 널리 사용되는 커널 중 하나는 **방사 기저 함수(Radial Basis Function, RBF)**이다. **가우시안 커널**이라고도 한다.
$$
K(x^{(i)},x^{(j)}) = exp(-\frac{\|x^{(i)}-x^{(j)}\|^2}{2\sigma^2}) = exp(-\gamma\|x^{(i)}-x^{(j)}\|^2)
$$
여기서 $\gamma = \frac{1}{2\sigma^2}$이고 이는 최적화 대상 파라미터가 아니다.

간단히 요약하면, 커널이란 용어를 샘플 간의 유사도 함수로 해석할 수 있도록 만드는 것이다. 실제 사용은 어떻게 하나 살펴보자.
```python
# kernel을 Radial Basis Function으로 선택해 커널 기법을 이용해 커널 svm을 사용한다.
# gamma는 위에서 사용한 파라미터로, 가우시안 구의 크기를 제한하는 매개변수이다.
svm = SVC(kernel='rbf', random_state=1, gamma=0.10, C=10.0)
svm.fit(X_xor, y_xor)
plot_decision_regions(X_xor, y_xor,
                      classifier=svm)

plt.legend(loc='upper left')
plt.tight_layout()
# plt.savefig('images/03_14.png', dpi=300)
plt.show()
```
![](https://velog.velcdn.com/images/jinsoolve/post/bd053514-a9f9-4e50-bb8a-c479de87b796/image.png)
## $\gamma$ 변수
$\gamma$를 크게 하면 결정경계는 샘플에 가까워지고 구불구불해진다.
### $\gamma$를 작게 했을 때
![](https://velog.velcdn.com/images/jinsoolve/post/0cb747b7-8f79-447e-ab75-a1e6719c35fc/image.png)
### $\gamma$를 크게 했을 떄
![](https://velog.velcdn.com/images/jinsoolve/post/7e54d6f7-0861-4e91-a19f-6ed54b6ecf9b/image.png)
# 결정 트리(Decision Tree) 학습
![](https://i.imgur.com/Xa5kfft.png)

결정 트리는 훈련데이터에 있는 특성을 기반으로 샘플의 클래스 레이블을 추정할 수 있는 일련의 질문을 학습한다.
이때 트리를 구성하는 방식은 **정보 이득**이 최대가 되는 방향으로 구성한다.
## 정보 이득을 어떻게 확인할까?
불순도 지표를 이용하여 확인한다. 불순도가 낮을 수록 정보 이득이 커진다. 대표적인 불순도 지표는 다음과 같다.
### 지니 불순도
$$
I_H(t) = -\sum_{i=1}^cp(i|t)log_2p(i|t)
$$
### 엔트로피
$$
I_G(t) = 1 - \sum_{i=1}^cp(i|t)^2
$$
### 분류오차
$$
I_E = 1 - max{p(i|t)}
$$
![](https://velog.velcdn.com/images/jinsoolve/post/8141719e-40e2-4358-b372-9bacb7364004/image.png)
이런 식으로 결정 트리가 완성된다.
# 랜덤 포레스트로 여러 개의 결정 트리 연결
1. n 개의 부트스트랩 샘플을 뽑는다.
2. 부트스트랩 샘플에서 결정 트리를 학습한다.
	a. 중복을 허용하지 않고 랜덤하게 d개의 특성을 선택한다.
    b. 정보 이득과 같은 목적함수를 기준으로 최선의 분할을 만드는 특성을 사용해서 노드를 분할한다.
3. 1~2를 k번 반복한다.
4. 각 트리의 예측을 모아 다수결 투표로 클래스 레이블을 할당한다.

랜덤 포레스트는 결정 트리만큼 해석이 쉽지는 않지만 하이퍼파라미터 튜닝에 많은 노력을 기울일 필요가 없다.
# 정리
모델 해석이 중요할 때는 결정트리가 사용하기 좋다.
로지스틱 회귀는 확률적 경사 하강법을 사용한 온라인 학습 뿐만 아니라 특정 이벤트 확률 에측에도 사용 가능