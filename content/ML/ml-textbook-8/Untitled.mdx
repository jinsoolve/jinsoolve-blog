---
slug: ml-textbook-8
title: "머신러닝 교과서 7장: 앙상블 학습"
description: 머신러닝 교과서 7장의 앙상블 학습에 대해서 알아보자.
thumbnail: 
categories:
  - ML
tags:
  - 앙상블
createdAt: 2025/01/09
updatedAt: 
featured: false
---
- 다수결 투표 기반 모델(보팅) -> 최빈값을 선택
- 랜덤하게 복원추출하여(배깅) 과대적합 감소
- 앞선 모델의 오차를 학습하여 강력 모델 구축(부스팅)
# 7.1 앙상블 학습
앙상블 모델은 거의 항상 단일 모델보다 정확도가 높다.
# 7.2 다수결 투표를 사용한 분류 앙상블
모델 여러 개의 예측값을 가중치를 곱해서 앙상블 결과를 얻는다. 각각의 모델의 하이퍼파라미터를 GridSearch로 찾아 전체 앙상블 모델의 예측결과가 어떠한지 찾을 수 있다.
각각의 모델을 합칠 때 파이프라인을 잘 이용하자. 정규화나 표준화가 필요한 모델 같은 경우 파이프라인으로 전처리 과정을 합쳐준다.
# 7.3 배깅: 부트스트랩 샘플링을 통한 분류 앙상블
Bagging은 Bootstrap Aggregating 의 줄임말로, bootstrap(랜덤하게 복원추출)하여 각 분류기에 전달하여 결과를 앙상블하는 것을 말한다.
대표적으로는 결정트리를 bagging하게 샘플링하는 랜덤포레스트가 있다.

랜덤 포레스트는 분산을 최적화시키기에는 좋지만, 편향을 잡는 데에는 약하다. 모델이 너무 단순하기 때문이다. 이것이 배깅을 수행할때 편향이 낮은 모델, 예를 들어 가지치기하지 않은 결정트리를 분류기로 사용하여 앙상블을 만드는 이유다.
# 7.4 약한 학습기를 이용한 에이다부스트
adaboost(Adaptive Boosting)은 부스팅 앙상블 모델이다.

부스팅이란 앙상블 모델에서 하나의 모델을 훈련할 때 이전 모델의 결과를 참고해서 결과를 좋게 나오도록 하는 기법이다. 
여기서 사용하는 분류기들은 단순한 모델(약한 분류기, 예를 들면 깊지 않은 결정트리)를 사용한다.

Adaboost는 이전 분류기에서 제대로 분류하지 못한 샘플들에 대해 가중치를 높이고 잘 분류한 샘플들에는 가중치를 낮춰서 결국에는 모두 제대로 분류할 수 있도록 만든다.

AdaBoost는 잘못 분류된 점을 이용하여 약분류기 학습의 가중치를 결정하였다면, GBM은 Gradient Descent 기법을 이용하여 손실함수를 최소화하는 방향으로 학습을 합니다.

---

GBM의 매개변수에 대해서 간단하게 설명하겠다.
- n_estimators: 약한 학습기 수
- subsample: 해당 비율만큼 훈련 데이터에서 랜덤 샘플링(확률적 그래디언트가 된다)
- early stopping: 약한 학습기가 많다고 무조건 좋은 것이 아니라 감소하다가 어느 순간부터 증가하기 때문에 중간에 훈련을 멈출 수 있도록 하는 값

---

GBM에는 다양한 모델들이 있다.

- 일반적인 GradientBoostingClassifer
- XGBoost (eXtreme Gradient Boost)
	일반적인 GBM을 업그레이드 시킨 모델
    병렬처리, 효율적인 특징 중요도 계산등의 최적화가 되어 있음. (여러모로 전처리나 정규화, early stopping 등을 미리 제공함)
- HistGradientBoostingClassifier
	입력 특성을 256개의 구간으로 나누어 노드를 분할에 사용.
    일반적으로 샘플 개수가 1만 개보다 많은 경우 XGBoost보다 Hist가 더 효과적이다.
- LightGBM
	히스토그램 기반의 학습방법을 사용하여 데이터 분할을 최적화하고 속도 향상 시킴.
    HistGBM이 LGBM의 영향을 많이 받았다고 함.
