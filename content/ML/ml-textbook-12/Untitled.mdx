---
slug: ml-textbook-12
title: 12장 다층 인공 신경망을 밑바닥부터 구현
description: 딥러닝은 인공 신경망을 효과적으로 학습시키기 위한 머신러닝의 하위분야이다. 아래 내용을 소개하겠다.- 다층 신경망 개념- 역전파 알고리즘- 이미지 분류를 위한 다층 신경망 훈련
thumbnail: 
categories:
  - ML
tags: 
createdAt: 2025/01/09
updatedAt: 
featured: true
---
딥러닝은 인공 신경망을 효과적으로 학습시키기 위한 머신러닝의 하위분야이다. 아래 내용을 소개하겠다.
- 다층 신경망 개념
- 역전파 알고리즘
- 이미지 분류를 위한 다층 신경망 훈련

# 12.1 인공 신경망으로 복잡한 함수 모델링
인공 신경망은 뇌가 어떻게 복잡한 문제를 푸는지에 대한 가설과 모델을 기반으로 만들어졌다. 초기 연구는 이미 1940년대에 완료되었으나 역전파 알고리즘이 재발견되기 전까지는 관심을 받지 못 했다.

## 단일층 신경망
다층 신경망을 보기 전에 단일층에 대해 설명하겠다.
은닉층이 없는 신경망, 즉 입력층과 출력층만 존재하는 신경망을 뜻한다. 경사 하강법 최적화를 이용하여 모델을 학습 시킨다.

확률적 경사 하강법(stochastic gradient descent) 최적화를 사용할 수도 있다.
- 온라인 학습: 하나의 훈련 샘플
- 미니 배치 학습: 적은 수의 훈련 샘플

을 이용하여 일반적인 경사하강법에 비해 더 자주 가중치 업데이트하기 때문에 훈련이 빠르다.
또한 들쭉날쭉한 학습 특성이 다수 볼록함수를 가진 비선형 활성화 함수를 활용한 다층 신경망을 훈련시킬 때 큰 장점이 된다.
확률적 경사하강법에서 생기는 잡음이 지역 최솟값(전체 최솟값은 아닌데 지역에서 최솟값인 경우)를 탈출하는데 도움이 된다.

## 다층 신경망
여러 개의 단일 뉴런을 연결하여 만든 다층 피드포워드 신경망을 뜻한다. 다층 퍼셉트론(MultiLayer Perceptron, MLP) 혹은 심층 인공 신경망(deep artificial neural network)이라고도 한다. 
단일 신경망과 다른 점은 입력층과 출력층 사이에 여러 개의 은닉층이 존재한다.

<sub>
참고로, 퍼셉트론은 -1과 1로 출력하는 단순한 이진분류 모델이고 오분류된 샘플들에 대해 가충지를 조절한다.
반면에, 아달린은 연속 적인 실수값 출력을 생성하고 경사하강법을 이용하여 최적화 시킨다.
둘 다 분류를 하는 단순한 모델이다.
</sub>

은닉층이 많아질수록 역전파로 계산하는 오차 그레디언트가 점점 작아지는 그레이디언트 소실 문제가 나타난다. 이런 문제를 해결하기 위해 개발된 기법들이 딥러닝이 되었다.

### 정방향 계산 및 역전파 최적화
정방향 계산의 순서는 다음과 같다.
1. 신경망의 흐름대로 전파시켜 값을 출력한다.
2. 오차 값을 계산한다.
3. 가중치에 대한 도함수를 찾아 오차를 역전파하여 모델을 최적화시킨다.

여기서 사용하는 활성화 함수는 미분 가능해야 한다.
복잡한 문제를 해결하기 위해서는 활성함수를 비선형으로 사용해야 한다. (시그모이드 활성화 같은 함수)

MLP(다층 퍼셉트론)는 대표적인 피드포워드 인공 신경망이다.
여기서 피드포워드란 각 층에서 입력을 순환시키지 않고 다음 층으로 전달한다는 의미이다.

---
# 12.2 손글씨 숫자 분류
신경망을 이용해서 손글씨 숫자 분류 모델을 구현해 보자.

1. 데이터를 받아와서 savez나 pickle 같은 함수로 파이썬이 빠르게 읽을 수 있는 포맷으로 저장한다.
2. 신경망을 구현한다.
	1. sigmoid: 활성화 함수
    2. forward: 정방향 전파하여 out layer 반환
    3. compute_cost: 비용 함수 J 계산
    4. predict: 클래스 레이블에 대한 예측값 반환
    5. fit: 위 함수들을 이용해서 모델 훈련 및 평가
    
신경망 훈련에서는 훈련 정확도와 검증 정확도를 반드시 비교해야 한다. 모델의 최적화 여부나 과대적합 여부를 확인하는데 필수다.
아래 그림을 보면 에포크가 늘어날 수록 training과 validation 데이터들의 정확도를 확인할 수 있다.
![](https://velog.velcdn.com/images/jinsoolve/post/62af6607-294c-4c2d-bb67-c19a09de4837/image.png)

일반적으로 신경망이 머신러닝 모델들에 비해 비용이 많이 든다. 
따라서 어떤 조건이 되면 일찍 중지하고 다른 하이퍼파라미터 설정을 하는 것이 좋다.

## 과대적합
모델의 과대적합을 줄이는 방법은 규제 강도를 높이는 방법이 있다. 규제 파라미터의 값을 증가시키는 것.
혹은 드롭아웃(dropout)기법을 사용하는 것이 있다.

더 좋은 성능을 낼 수 있는 방법으로는
- skip-connection
- 학습률 스케줄러
- 인셉션 v3구조 처럼 신경망의 앞쪽 층에 손실함수 연결

이 있다.

---
# 12.3 인공 신경망 훈련
신경망 훈련의 핵심인 역전파 알고리즘에 대해서 설명하겠다.
역전파 알고리즘은 다층 신경망에서 복잡한 비용함수의 편미분을 효율적으로 계산하기 위한 방법이라 말할 수 있다.

일반적인 신경망의 비용함수 곡면은 볼록 함수가 아니거나 파라미터에 대해 매끄럽지 않다. 고차원 비용함수의 곡면에는 전역 최솟값을 찾기 위해 넘어야 할 지역 최솟값이 많다.

역방향은 오른쪽에서 왼쪽으로 진행하다보니 행렬-벡터 곱셈이 되어서 왼쪽에서 오른쪽으로 진행(행렬-행렬 곱셈)보다 훨씬 계산비용이 저렴하다.

## 역전파 과정
1. 출력층의 오차항(출력값-실제값) 계산
2. $\frac{\sigma J(W)}{\sigma W}$를 계산한다.
3. 은닉층의 오차항 계산
4. 2-3과정을 입력층의 J의 편미분을 계산할 때까지 반복한다.

그림으로 보면 쉽게 이해할 수 있다.
![](https://velog.velcdn.com/images/jinsoolve/post/42ff8c13-7fb0-43e5-9f8e-470cb7988417/image.png)

이렇게 구한 모든 층의 비용함수의 도함수(기울기) 값을 이용해서 매개변수 W를 업데이트 시킨다.
![](https://velog.velcdn.com/images/jinsoolve/post/54ac5810-f131-48c2-894b-3df7d3b1c6ee/image.png)


---
# 12.4 신경망의 수렴
미니 배치 방식을 사용하면 확률적이지만 매우 정확한 솔루션을 만들고 기본 경사하강법보다 훨씬 빠르게 수렴한다.
미니 배치 방식은 n개의 훈련샘플에서 k개의 부분집합에서 기울기를 계산한다.

다층 신경망은 아달린, 로지스틱 회귀, SVM 같은 알고리즘보다 훨씬 훈련시키기 어렵다.
최적화시켜야할 가중치가 훨씬 많고, 손실함수의 표면이 거칠어서 최적화 알고리즘이 지역 최솟값에 갇히기 쉽다.