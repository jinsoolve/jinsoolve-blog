<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[jinsoolve blog's RSS Feed]]></title><description><![CDATA[머신러닝과 알고리즘을 공부하는 김진수 입니다.]]></description><link>https://jinsoolve.netlify.app</link><generator>GatsbyJS</generator><lastBuildDate>Mon, 28 Apr 2025 04:54:24 GMT</lastBuildDate><item><title><![CDATA[Word2Vec과 GloVe의 차이는 무엇일까?]]></title><link>https://jinsoolve.netlify.app/posts/diff-between-word2vec-and-glove</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/diff-between-word2vec-and-glove</guid><pubDate>Sat, 26 Apr 2025 15:00:00 GMT</pubDate><content:encoded>그래서 Word2Vec과 GloVe의 차이는 대체 뭘까? 

개인적으로 매우 헷갈리는 topic이라서 내 개인 이해를 정리해 보았다.

먼저 standard한 Word2Vec과 GloVe라 가정하자.
# Word2Vec
Word2Vec은
1. **Stochastic GD** → 매번마다 모델 업데이트
2. **Negative Sampling** → 특정 중심단어에 대해서 틀린 주변단어 k개를 뽑고 올바른 주변단어가 알맞다고 모델이 예측하도록 한다.
3. **skip gram** → 중심단어를 보고 주변단어를 예측
   그 반대) **CBOW(Continuous Bag of Words)** → &apos;여러 주변단어를 보고 중심단어를 예측&apos; 하는 방식도 존재
을 사용한다고 가정
# GloVe
Glove는
1. corpus를 읽고 각 단어에 대해서 window 안에서 동시 등장 횟수(co-occurrence)를 센다. → 이를 토대로 **co-occurrence matrix** 를 생성
2. **word vector의 내적값이 co-occurrence의 log 값과 비슷해지도록 학습**.
   손실함수를 단어벡터(혹은 편향값)의 관점으로 편미분해서 해당 방향으로 각 단어벡터(혹은 편향값)으로 업데이트

# Word2Vec과 GloVe의 차이?
간단히 요약하자면,
- Word2Vec은 중심단어가 주어졌을 때, 알맞은 주변단어가 나올 확률이 최대가 되도록 학습하는 모델이다. 
- GloVe는 전체 Corpus에서 특정 단어 2개가 몇 번 같이 나왔는지를 맞춘다.

예를 들면,
- Word2Vec은 
  &quot;A랑 B가 친구인가? (예/아니오)&quot;를 맞추는 문제 → **Binary 문제를 해결하면서 학습(얘가 주변에 오는 게 맞아?)**
- GloVe는
  &quot;A랑 B가 총 몇 번 같이 놀았어? (숫자)&quot;를 맞추는 문제 → **Regression 문제를 해결하면서 학습(얘가 얘 주변에 몇 번이나 나왔어?)**
</content:encoded></item><item><title><![CDATA[Cross Entropy와 Softmax]]></title><link>https://jinsoolve.netlify.app/posts/cross-entropy-and-softmax</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/cross-entropy-and-softmax</guid><pubDate>Sat, 26 Apr 2025 15:00:00 GMT</pubDate><content:encoded>Cross-entropy와 Softmax가 자꾸 헷갈려서 정리를 한 번 해보자. 
+) 추가로 sigmoid와 tanh 도 보자.



# Softmax
$$
\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}
$$
$z_{i}$는 i번째 클래스의 점수이다.

이 점수들을 Softmax를 이용해서 0 ~ 1 사이의 확률값으로 만들어 버린다.

# Cross-entropy
예측한 확률 분포와 정답 분포가 얼마나 다른지를 측정하는 지표이다.

$$
\text{Cross-entropy} = -\sum_{i} p_i \log q_i
$$
$p_{i}$는 정답(라벨)이고, $q_{i}$는 모델이 예측한 (Softmax 결과) 확률이다.

식을 살펴보면, $-\log{q_{i}}$가 있는 걸 확인할 수 있다. 이걸 그래프로 보자.
![](https://i.imgur.com/2B50z1v.png)

모델이 예측한 결과인 $q_{i}$는 0 ~ 1 사이의 값임을 기억하자. 즉 $-\log{q_{i}}$는 양수가 되고, $q_{i}$가 0에 가깝다면 $\infty$가 되고, 1에 가깝다면 0이 된다.

만약 정답인 $p_{i}$가 1인데 $q_{i}$가 0이라면 값이 $\infty$가 되어서 값이 커지게 된다. 이걸 이용해서 cross-entropy를 손실함수에 사용한다.

# Sigmoid와 tanh
시그모이드와 tanh 모두 비선형성을 추가하기 위해 주로 hidden state 값을 계산할 때 사용한다. (softmax는 output을 출력할 때 사용한다.)
![](https://i.imgur.com/egcQZB5.png)


</content:encoded></item><item><title><![CDATA[RNN]]></title><link>https://jinsoolve.netlify.app/posts/intro-to-nlp-7</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/intro-to-nlp-7</guid><pubDate>Sat, 26 Apr 2025 15:00:00 GMT</pubDate><content:encoded># Language Model(LM)이란?
언어 모델이라는 건, 사실 **다음에 올 단어를 확률로 예측**하는 것이다.

이러한 언어 모델들을 어떻게 발전시켜왔는 지 살펴보자.

# n-gram Language Model
이미 [이전 포스트](https://www.jinsoolve.com/posts/intro-to-nlp-2/)에서 자세히 살펴보았던 내용이다.
**이전 n-1개의 단어들을 보고 다음 단어를 예측하는 언어모델**이다.

이제 count함수로 확률을 계산하게 된다.
![](https://i.imgur.com/6xsdYwp.png)
참고로, 저 순서의 조합대로 있어야 세는 거다.
그러다보니 n이 커질수록 해당 조합이 없을 가능성이 점점 높아진다.

그러다보니 다음과 같은 문제가 생긴다.
1. Sparsity Problem
   특정 조합이 아에 없어서 말이 되는 말임에도 불구하고 확률이 0이 되어비리는 문제. 
	해결방안은 다음과 같다.
	1. smoothing 기법
	   대충 0이 안 되게 확률에 약간의 값들을 모두 일정하게 더해주거나(additive), n이 서로 다른 n-gram 모델들 가중 평균으로 합쳐서 확률을 계산해주거나(interpolation), 기존 확률에서 약간의 값을 빼내서 새롭게 관측된 애들한테 나눠주는 방식(discounting)이 있다.
	2. backoff
	   계산할 때 n을 줄여서 계산해 버린다. 그럼 해당 순서의 조합이 있을 가능성이 높아지기 때문.
2. Storage Problem
   모든 n-gram들의 순열에 대해서 전부 저장해줘야 하기 때문에 저장 공간이 많이 필요하게 된다.


## Perplextiy(ppl)
혼잡도 인데 cross-entropy loss를 exponential 해버린 것과 동일한 의미이다.
$$
\text{Perplexity} = \left( \prod_{t=1}^{T} \frac{1}{P(x^{(t+1)}|x^{(t)}, x^{(t-1)}, \dots)} \right)^{\frac{1}{T}}
$$
$$
\text{Perplexity} = \exp\left( - \frac{1}{T} \sum_{t=1}^{T} \log P(x^{(t+1)} | x^{(t)}, \dots) \right)
$$
loss와 마찬가지로 ppl이 적을수록 좋다.

# Fixed-window Neural Language Model
n-gram 언어모델이 sparsity(말이 되는 말을 0으로 만들어 버림)와 storage(n-gram 순열들 모두 저장해야 함) 문제가 있었기 때문에 신경망 모델로 해결하고자 했다.

fixed-window 크기만큼의 단어를 확인해서 학습하는 방식이다.

![](https://i.imgur.com/IGmwjeZ.png)
각 단어에 대해서 신경망을 통해 학습한다. (사실, binary logistic regression unit 들을 여러 개 쌓아올린 것과 같다.)

## Improvement
W와 U에 각 단어의 의미를 일반화시켜 저장하기 때문에 sparsity 문제도 해결되고, 마찬가지의 이유로 n-gram을 저장할 필요가 없으니 저장 메모리 문제도 해결된다.

## Remaining Problem
윈도우가 너무 작아도 문제이고 그렇다고 키우면 W가 너무 커진다. 그래서 긴 문맥은 이 fixed window로는 감당이 안 되어 버린다.
그리고 결정적으로 각 단어가 위치마다 다른 가중치를 사용해서 같은 단어인데 위치에 따라 다른 의미라고 받아들여진다. → No symmetry

# Recurrent Neural Network (RNN)
아이디어: **같은 파라미터 W를 반복해서 적용시키면 해결되지 않을까?** 라는 아이디어로 해결하고자 함.
그래서 나온 것이 RNN이다.

![](https://i.imgur.com/PlK76Ov.png)
매번 같은 $W_{h}$, $W_{e}$를 사용하고 이전 은닉층의 값을 이용해 현재 은닉층의 값을 계산하는 방식이다.
비선형성을 위해 각 은닉층의 결과에 $\sigma$를 추가해 준다. 그리고 마지막 결과로는 $\text{softmax}$를 해서 확률로 결과를 내보내는 형식이다.

## Advantage
긴 문장도 처리가 가능해진다.
같은 파라미터를 반복해서 사용하므로 메모리가 절약된다.
또한 같은 파라미터를 사용해서 symmetry 를 가질 수 있게 되었다.

## Disadvantage
그러나 매번 모든 step을 다 계산해줘야 하므로 느리다.
또한 너무 많은 step들이 지나면 해당 정보를 잃어버려서 기억할 수 없게 된다. 
→ 해당 부분들은 뒤에서 좀 더 자세히 다뤄보자.

## RNN의 손실함수 계산
![](https://i.imgur.com/sMGtpfp.png)
$h^t$ 즉, t번 째 은닉층의 손실값에 대한 Gradient ($W_h$로 편미분한 Gradient이다) 를 계산하려면 $h^0$ ~ $h^{t-1}$r과 현재 층 $h^t$의 gradient를 모두 더해줘야 한다.
왜냐하면 $W_{h}$를 반복해서 사용하기 떄문에 $W_{h}$가 매 은닉층마다 영향을 끼치기 때문에 모든 은닉층의 Gradient들을 더해줘야 한다.

근데 바로 여기서 Vanishing Gradient와 Exploding Gradient 문제가 발생한다.

## Problems with RNNs: Vanshing and Exploding Gradients
### Vanishing Gradient
![](https://i.imgur.com/e5FB2Tb.png)
$h^4$에서 Gradient를 계산할 때, $\frac{\sigma J^4}{\sigma h^4}$ 와 $\frac{\sigma J^4}{\sigma h^1}$ 을 모두 더해줘야 하는데(물론 2,3 도 더해줘야 하는데 생략하자), $\frac{\sigma J^4}{\sigma h^4}$에 비해 $\frac{\sigma J^4}{\sigma h^1}$은 $\frac{\sigma h^2}{\sigma h^1} \times \frac{\sigma h^3}{\sigma h^2} \times  \frac{\sigma h^4}{\sigma h^3}$ 을 더 곱해줘야 하는데 이 값들이 만약 1보다 작다면 당연히 값이 줄어들 것이다.
즉, 최근 은닉층에 비해 옛날 은닉층의 값이 작아져서 그만큼 예전에 했던 말을 기억하지 못 하게 된다.
그러다보니 이 vanishing gradient 문제로 인해 오래 전 말을 기억하지 못하게 된다.

### Exploding Gradient
![](https://i.imgur.com/unNXWMa.png)
반대로 1보다 큰 값을 여러 번 곱해서 값이 너무 커지게 되면, 업데이트를 할 때 한 번에 너무 큰 step을 가게 된다.

![](https://i.imgur.com/4zGdy0b.png)
이런 식으로 목표를 자꾸 지나쳐 버리게 된다는 의미다.

그래도 이건 Gradient Clipping으로 어느 정도 완화가 가능하다.

#### Gradient Clipping
![](https://i.imgur.com/dSWMK0j.png)
Gradient가 threshold보다 커지면 threshold로 줄이는 방식으로 해결이 가능하다

### 하지만 그럼 Vanishing Gradient는 어떻게 해결해야 하는 걸까?
- 아이디어1: RNN에다가 분리된 **메모리셀**을 저장한 후에 이걸 **더해서** 이전 껄 기억해 볼 수 있지 않을까? → 이게 **LSTM**의 아이디어이다.
- 아이디어2: 더 나아가서 **직접 연결**을 만들어서 정보를 바로 넘기면 어떨까? → **Attention**, **Residual Connection** 등

</content:encoded></item><item><title><![CDATA[Fancy RNN]]></title><link>https://jinsoolve.netlify.app/posts/intro-to-nlp-8</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/intro-to-nlp-8</guid><pubDate>Sat, 26 Apr 2025 15:00:00 GMT</pubDate><content:encoded>[이전 포스트](https://www.jinsoolve.com/posts/intro-to-nlp-7/)에서 RNN에서 Vanishing Gradient로 인해 장기 의존성 문제가 있다는 사실을 이야기했다.

이런 Vanishing Gradient를 해결하기 위해 크게 2가지 아이디어를 마지막에 언급했었다.
1. 메모리셀에 저장해서 이걸 전달하는 방식 → LSTM
2. 직접 연결해서 정보를 넘겨주는 방식 → ResNet(Skip Connection), Attention 등...

한 번 알아보자.

# Long Short-Term Memory RNNs (LSTMs)
기존 hidden state $h^t$가 단기 메모리라면, cell state $c^t$를 만들어서 여기에 장기 메모리를 저장해 놓는 방식이다.

이 cell state에서는 정보를 읽고, 지우고, 쓸 수 있다. 이러한 동작은 3개의 각각의 Gate들에 의해서 제어된다.

## LSTM의 구조
$h^t$가 n 길이의 벡터로 정보를 저장하고 있다면, 이 gate들도 마찬가지로 n 길이의 벡터를 저장하고 있다.
각 값은 0 ~ 1의 값으로 1은 open, 0은 close를 의미한다.
![](https://i.imgur.com/NlsXhgQ.png)

이걸 예시를 통해서 이해해보자. chatgpt의 설명이 마음에 들어서 갖고 와 보았다.

&lt;Callout type=&quot;&quot;&gt;
### ✏️ 비유: “비밀 일기장”
우리는 매일매일 비밀 일기장을 쓰는 사람이야.
그런데 일기를 쓸 때 3가지 질문을 먼저 해.
#### 🛑 1. Forget Gate = “지우기 질문”
“오늘 쓸 때, 지난 일기 중에서 어떤 걸 지워야 할까?”
(예를 들어, 어제 짜증났던 일은 잊어버리고 싶어!)
→ Cell state에서 과거 기억을 일부 지워줘.
#### 🖋️ 2. Input Gate = “새 기록 질문”
“오늘 무슨 새 일을 추가해서 일기에 쓸까?”
(오늘 좋은 일은 기록하고 싶지!)
→ New cell content를 얼마나 받아들일지 결정해.

#### 📚 3. New Cell Content = “새로 쓸 내용 초안”
“오늘 있었던 일들을 일단 초안으로 적어봤어.”
(다 쓸 필요는 없고, 좋은 것만 고를 수도 있어.)
→ 오늘의 새 사건들 목록. 아직 확정은 아님.

#### 📓 4. Cell State = “진짜 일기장”
“이제 지울 건 지우고, 새로 쓸 건 골라서
진짜로 일기장에 남긴 기록!”
→ 과거 기억 + 새 내용이 합쳐진 업데이트된 메모리.

#### 📢 5. Output Gate = “말하기 질문”
“일기장을 다 보관해도,
오늘 다른 사람한테 보여줄 내용은 뭘까?”**
(다 보여주는 건 아니고 일부만 얘기할 수도 있어.)
→ 이걸 통해 만든 게 바로 Hidden state야.
(= 외부에 드러내는, “내가 기억하고 있는 것처럼 보이는” 정보)



&lt;/Callout&gt;




시각적으로 표현하면 아래와 같다.
![](https://i.imgur.com/7PNH0Ox.png)


이런 식으로 LSTM은 cell을 통해 이전 정보를 넘겨주기 때문에 오래 전의 단어도 기억을 잘 할 수 있게 된다.

## vanishing/exploding gradient 문제는 RNN만의 문제가 아니다
모델이 깊기만 하다면(feed-forward나 CNN 같이) 깊은 신경망들은 모두 이런 장기 의존증 문제를 갖고 있다.

이걸 해결하기 위핸 다른 방법들도 있다.

### Skip-Connection 
ResNet의 skip connection이 대표적이다.
![](https://i.imgur.com/nv0WpAg.png)
변화 없이 정보 그대로를 다음으로 넘겨준다.

### DenseNet
![](https://i.imgur.com/xvJiEnP.png)
모든 레이어를 이후 모든 레이어에 직접 연결하는 방식

### Highway Connection
![](https://i.imgur.com/fvPu45w.png)
Skip Connection과 유사하지만 어떤 비율로 정보(Identity)를 통과시킬지를 gate가 결정한다.
(LSTM의 영향을 받았다.)


# Bidirectional and Multi-layer RNN
근데 생각해보면 문맥이라는 건 왼쪽에만 있는 것이 아니라, 오른쪽에도 있다.
즉,오른쪽의 문맥 또한 확인해야 하는 것이다.

이를 바탕으로 다음과 같은 모델이 나왔다.

## Bidirectionial RNNs
![](https://i.imgur.com/LTiHX3y.png)
이름 그대로 양방향으로 확인하여 오른쪽 문맥까지 확인하는 것이다.

총 2개의 RNN을 만들어서 분리해서 관리한다.
![](https://i.imgur.com/tf0k2Fh.png)
하나는 정방향, 다른 하나는 양방향으로 설정해서 이 2개를 concat하는 방식이다.

### 한계
그러나 그 다음 단어를 생성해야 하는 Language Model의 특성 상, 오른쪽 문맥을 미리 확인할 수가 없다.

그러나, 양방향은 그 자체만으로도 강력하기 때문에 만약 전체 corpus를 접근할 수  있는 경우라면 매우 잘 사용할 수 있다.
예를 들어, BERT(Bidirectional Encoder Representations from Transformers)는 transformer기반이고, 양방향으로 확인하면서 문장 이해력을 높였다.

## Multi-layer RNNs
이번에는 여러 개의 층을 쌓아서 RNN의 능력을 높여보자.

![](https://i.imgur.com/vufuFms.png)
낮은 층은 low-level feature를 계산하고, 높은 층은 high-level feature를 계산한다.

너무 깊게 쌓으면 skip-connection이나 dense-connection 같은 것이 필요하다.

# Machine Translation (기계 번역)
## Neural Machine Translation (NMT) : Seq2Seq
기계 번역에서 획기적 발전 중 하나가 Seq2Seq 모델이다.

![](https://i.imgur.com/ev80vCW.png)
이런 식으로 encoder-decoder 모델이다

입력 언어 문장을 하나의 벡터로 압축한 후, 해당 벡터로 decoder에 넣어서 출력 언어 문장으로 내보내는 구조이다.
seq2seq는 conditional language model 이라 말할 수 있는데, 원래 문장을 조건으로 디코더가 다음 단어를 예측하기 때문이다.


![](https://i.imgur.com/zWJHvdY.png)

이런 NMT는 말 그대로 번역 쌍 데이터를 많이 확보해서 훈련해야 한다.

![](https://i.imgur.com/ApJSsfn.png)
이런 식으로 손실함수를 평균 내서 계산한다.

### 한계
하지만 이런 방식은 아무래도 하나의 벡터로 압축하기 때문에 이런 병목 현상에서 데이터의 유실이 일어날 수 있다.

또한 RNN의 특징인 장기 의존증 문제 또한 있다.

그리고 Multi layer RNN의 구조상, 이전 정보를 받아야 하므로 병렬적으로 수행하지 못 하기 때문에 시간이 오래 걸린다.</content:encoded></item><item><title><![CDATA[Attention]]></title><link>https://jinsoolve.netlify.app/posts/intro-to-nlp-9</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/intro-to-nlp-9</guid><pubDate>Sat, 26 Apr 2025 15:00:00 GMT</pubDate><content:encoded>기존 RNN의 병목 현상을 해결하기 위해 Attention이 등장했다.

# Attention
Decoder에서 한 단어를 예상할 때, 해당 단어와 특별히 관련되어 있는 Encoder의 특정 단어를 Direct Connection으로 연결하는 방식이라 할 수 있다.

먼저 basic한 아이디어는 병목 현상을 완화하기 위해 마지막 vector를 쓰는 게 아니라 평균 값을 내서 전달하면 좀 낫지 않겠느냐 라는 아이디어이다.
![](https://i.imgur.com/TW1oOZ9.png)

이걸 바탕으로 attention은 **weighted averaging**을 사용한다.
![](https://i.imgur.com/f7M4S53.png)
attention은 다음과 같이 이뤄진다.
1. query와 key 간의 유사도 점수를 계산하고, 이 점수를 softmax를 이용해서 확률로 만든다.
2. 각 확률을 value들에 곱해서 합한다 (가중 평균)
3. 이 결과가 output이 된다.

lookup table은 특정 쿼리에 알맞는 key를 골라서 해당 output을 그대로 출력한다.

![](https://i.imgur.com/eJsOrXJ.png)
이런 식으로 유사도 점수를 계산할 때는 
1. 맞추고자 하는 decoder의 한 단어의 hidden state와 encoder의 모든 hidden state들을 내적해서 attention score를 구한다.
2. 해당 attention score를 softmax를 해서 확률 분포를 얻는다.
3. 해당 확률 분포를 이용해서 가중평균한 결과를 얻는다. 
4. 그리고 해당 attention output과 decoder 의 한 단어의 hidden state를 concat 해준다.

위 내용에 대한 슬라이드이다.
![](https://i.imgur.com/MaIzjoI.png)


## Attention의 장점

병렬 계산이 가능하고, 모든 단어를 한 번에 볼 수 있어서 멀리 떨어진 단어 간 관계도 쉽게 배운다.

어텐션은 NMT의 성능을 끌어올렸고, 병목현상도 해결하고, 좀 더 사람처럼 번역해주고, gradient 소실 문제도 해결하고, 심지어는 어디에 attention 되어 있는 지를 알려주면서 해석 가능성도 주었다.

다만 하나 단점이 있다면 cost가 $(\text{sequence length})^2$ 이라서 시퀀스가 길어지면 느려진다.

## Attention의 일반화

어텐션을 좀 더 general하게 생각해보면 value들과 query가 주어졌을 때 query에 가장 연관이 있는 값을 산출해 내는 매커니즘이라 볼 수 있다.
즉, 값들 중 필요한 것만 뽑아서 요약하는 것이라고도 볼 수 있다.

포인터처럼 필요한 정보를 가리키고 메모리 조회처럼 필요한 걸 가져오는 범용 방버이라 할 수 있을 것이다.
</content:encoded></item><item><title><![CDATA[Transformers]]></title><link>https://jinsoolve.netlify.app/posts/intro-to-nlp-10</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/intro-to-nlp-10</guid><pubDate>Sat, 26 Apr 2025 15:00:00 GMT</pubDate><content:encoded>Seq2Seq에서 RNN을 아예 빼버리고 attention으로 구성해보면 어떨까? → Transformer의 구조

# Self-Attention
![](https://i.imgur.com/7Q1srIQ.png)

전에 seq2seq에서 decoder의 query와 encoder의 key/value를 보았다면
이젠 스스로의 query에서 스스로의 key/value를 보는 구조라고 생각하면 된다.

## 순서 문제
근데 여기서 문제가 이렇게 되면 글의 순서를 모른다는 점이다.
따라서 단어 벡터에다가 위치 정보를 더해 준다.
![](https://i.imgur.com/5HZaAow.png)

Position Representation은 여러 가지 방식이 있다.

### Sinusoidal Position Representations
![](https://i.imgur.com/XC8zJQG.png)
sin, cos 함수를 이용해서 주기적으로 반복해서 위치 정보를 만든다.
여기서 주기적이기 때문에, 값이 커져도 주기성으로 인해 해당 값에 대해서도 예상이 가능하다.(extrapolation)

하지만, 학습이 불가능하다보니 실제로 extrapolation이 잘 안 된다.

### Learned absolute position represtations
말 그대로 p 벡터를 학습 가능한 파라미터로써, 학습시켜서 값을 얻겠다는 생각이다.

학습 가능하다보니, 유연하다. 하지만 주기성이 없어서 extrapolate할 수 없다. 범위를 나가버리면 정의할 수 없다.

### RoPE (Rotary Position Embedding)

![](https://i.imgur.com/kbtvX7Q.png)
i와 j의 차이에만 집중하고 i, j 각각의 값은 신경 쓰지 않겠다는 방식이다. 

![](https://i.imgur.com/DHIi8HN.png)
내적은 회전시켜도 값이 그대로인 것을 이용해서 위치를 회전을 통해서 반영시키는 방식이다.

## 비선형성이 없는 문제
![](https://i.imgur.com/r8iy0cn.png)
그냥 Feed-forward network(은닉층이 1개인 신경망, MLP(Multi-Layer Perceptron)) 을 넣어서 해결한다.

## 미래 못 본다...
언어 모델은 오른쪽 Context를 실제로 보지 못 하는데 이걸 어떻게 반영할 것이냐의 문제.
![](https://i.imgur.com/XCxxGjZ.png)
attention score를 $-\infty$로 설정하면 exp 했을 때 0이 되므로 해당 단어를 무시하게 된다.
이런 식으로 오른쪽 context의 단어를 무시하게 만든다.

## 개선된 최종 self-attention block
![](https://i.imgur.com/ja3v6Vw.png)

# Transformer
그래서 transformer는 어떻게 생겼는지를 보자.

## Transformer Decoder
먼저 transformer의 decoder를 한 번 보자.
![](https://i.imgur.com/j9Vymds.png)
오른쪽이 transformer decoder, 왼쪽이 개선된 self-attention이다.
약 3가지 정도가 다르다.
1. multi-head attention을 썼다.
2. Residual connection을 썼다
3. Normalization를 해 줬다.

### Multi-headed attention
head가 여러 개라는 것은 글을 여러 개의 관점으로 바라본다. 즉, 각자 집중해서 보는 게 다르다는 의미다.

![](https://i.imgur.com/EWnN27H.png)

이떄 각 head는 독립적으로 계산하고 계산 효율을 위해 기존 d 차원을 나눠써 쓰기만 하다보니 결과적으로 cost는 크게 변하지 않는다.

![](https://i.imgur.com/VrHYyhO.png)
이런 식으로 덩어리만 나눈 것이 된다.

이때 d가 커지면 dot product가 너무 켜지게 될 수 있다.
![](https://i.imgur.com/5DYWsD8.png)
dot product는 softmax의 입력인데, softmax 그래프 특징 상 입력값이 너무 크면 gradient가 0에 수렴한다. 따라서 업데이트가 잘 안 된다.
다르게 보면 결과가 마치 one-hot vector처럼 되어버려서 업데이트가 안 된다고도 생각해 볼 수 있다.

이걸 해결하기 위해 $\sqrt{ \frac{d}{h} }$로 나눠준다.

### Residual Connections
![](https://i.imgur.com/AFGBvob.png)
이전 입력을 더해주면서 다음과 같은 효과를 지닌다.
![](https://i.imgur.com/9MyjU3g.png)

### Layer Normalization
정규화의 효과는 워낙 유명하다.
데이터를 안정화시켜 주기 때문에(너무 크거나 작은 걸 막아줌) 학습이 잘 된다.

### Transformer Decoder의 구조
![](https://i.imgur.com/9Q6I50T.png)
결론적으로 이런 식의 구조를 Transformer Decorder는 갖는다.

## Transformer Encoder
![](https://i.imgur.com/TwiiDbA.png)
Transformer Encoder는 사실 Transformer Decoder에서 masking 여부만 하지 않는 차이다.

이때 Decoder에서는 앞뒤 문맥을 모두 확인하고 싶을 때, Encoder에게 정보를 참고해서 생성하게 된다.

### Transformer Encoder-Decoder

![](https://i.imgur.com/KfH0oxe.png)

이런 식으로 Encoder는 bi-directional 하게 양방향의 context를 모두 알고 있고, Decoder는 uni-directional하게 생성하고 있다. 이때 decoder는 미래를 알 수 없어서 오른쪽 context를 모르고 있지만 encoder는 알고 있다. 따라서 encoder의 출력을 보고 decoder가 필요한 정보를 attend해서 갖고 온다.
위 decoder가 encoder에게서부터 갖고 오는 것을 Cross-Attention이라 한다.

### Cross-attention
![](https://i.imgur.com/Ov5Gol4.png)

쉽게 말하면 디코더의 query가 encoder의 key / value를 차고해서 decoder 값을 출력하는 방식이라는 의미다.

## Transformer의 한계
### Self-Attention의 계산량
- **Self-attention**의 계산량이 **sequence 길이의 제곱(𝑂(n²))** 에 비례한다는 문제.
- RNN은 **선형(𝑂(n))** 이라서 긴 문장 처리에 유리한데, Transformer는 길어질수록 계산량이 급증한다.
![](https://i.imgur.com/WKdOmCx.png)

- Attention을 할 때는 **모든 토큰쌍** (query-key)을 계산해야 해서,
    - 계산량이 𝑂(n²d) (n: 시퀀스 길이, d: hidden 차원)이다.
- 예를 들면 n=30이면 900번 계산이지만,
    - n=512, 혹은 n=50,000이 되면 계산량이 엄청나게 커진다.
### Pre-Norm vs Post-Norm
![](https://i.imgur.com/6bSUqDt.png)

- **Layer Normalization**을 **Self-Attention 앞에 넣느냐(Pre-Norm)**, **뒤에 넣느냐(Post-Norm)** 에 따라 학습 안정성 차이가 난다.

### RNN의 부활
![](https://i.imgur.com/5iZuXfd.jpeg)

- 긴 문맥(long context)이 필요할 때는 다시 RNN류 모델(RWKV, Mamba 등)이 뜨고 있다.
- 왜냐하면 **RNN은 𝑂(n)** 만의 연산량으로 가능하기 때문.

## Transformer를 개선하기 위한 노력들...
### Quadratic cost를 꼭 줄여야 할까?
- 실제 대규모 Transformer는 여전히 **quadratic self-attention**을 쓴다.
- 연산량 최적화 기법 (예: FlashAttention)이 등장했지만,
- “Attention을 근본적으로 바꾸는 것”은 scaling up에 큰 도움은 못 준 경우가 많았다. 
	- 즉, transformer 모델을 가볍게 하면 소규모에서는 괜찮은데 진짜 큰 모델에서는 원래 Transformer 구조가 더 잘 작동한다는 거다.
### Transformer 구조 변경이 효과 있을까?

- 결론: **대부분 구조 변경은 성능을 “의미 있게” 올려주지 못했다**.    
- Transformer 기본 구조 자체가 이미 매우 강력하다.

## 정리
### ✅ 장점 (Advantages)

1. **긴 거리(long-range) 의존성을 쉽게 잡는다**
    
    - Self-attention은 **모든 단어 쌍** 사이에 연결을 만든다.
        
    - 예를 들어, 첫 번째 단어랑 100번째 단어 사이 관계도 바로 연결할 수 있어.
        
    - RNN처럼 순서대로 거쳐야 하지 않으니까 **멀리 떨어진 관계**도 **바로 포착 가능**.
        
    
2. **병렬화가 쉽다 (Easier to parallelize)**
    
    - RNN은 시퀀스를 **순서대로** 처리해야 해서 한 번에 여러 작업을 못했어.
        
    - Transformer는 **모든 단어를 동시에** 처리할 수 있어서,
        
        → GPU 같은 병렬 장비에서 **엄청 빠르게 학습**할 수 있어.
        
    

---

### ❗ 단점 (Drawbacks)

1. **위치 정보(Positional Information)를 충분히 담을 수 있나?**
    
    - 기본 self-attention은 **순서**를 고려하지 않고 “집합(set)“처럼 데이터를 본다.
        
    - 그래서 **positional encoding**(위치 정보를 넣는 방법)이 꼭 필요하다.
        
    - → 만약 positional encoding이 약하면 **문장의 순서**를 모델이 잘 이해 못할 수도 있음.
        
    
2. **Self-attention은 계산량이 Quadratic (𝑂(n²))이다**
    
    - 모든 단어 쌍을 비교해야 해서, 문장 길이(n)가 길어지면 **n²만큼 계산량이 늘어남**.
        
    - → 문장이 길어지면 **매우 느려진다**.
        
    - 예를 들어, n이 10배 길어지면 연산량은 100배 늘어나는 거야.</content:encoded></item><item><title><![CDATA[Word2Vec]]></title><link>https://jinsoolve.netlify.app/posts/intro-to-nlp-4</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/intro-to-nlp-4</guid><pubDate>Fri, 25 Apr 2025 15:00:00 GMT</pubDate><content:encoded>저번에 [word vector](https://www.jinsoolve.com/posts/intro-to-nlp-3/)에 대해서 알아봤는데, 이번에는 word vector의 프레임워크인 Word2Vec에 대해서 좀 더 알아보자.

# Word2Vec의 계산 방식

먼저 Word2Vec이 어떤 식으로 동작하는 지 가볍게 살펴보자.

![](https://i.imgur.com/MZ8EIAo.png)
각 단어당 2개의 벡터가 있었는데 outside(context)와 center에서의 벡터가 각각 있었음을 기억하자.
위와 같은 방식으로 $v_{4}$의 단어에 대해서 결과를 얻으려면 위와 같이 계산하는 방식이다.

이때 **&quot;Bag of words&quot; (BOW)** 라는 말을 쓰는데,  여기서 BOW란, 단어의 순서나 위치에 상관없이 주변에 존재하는지의 여부만 확인해서 어떤 단어가 주변에 있는지를 확인한다.
Word2Vec은 이전 포스트에서도 봤다시피 순서에 신경쓰지 않았던 것을 확인할 수 있다.


# Word2Vec의 다양한 구현 방식

Word2Vec에서는 2가지 방식을 선택할 수 있다.
1. **Skip-grams (SG)**
	중심 단어를 보고 주변 단어들을 예측
	(이게 우리가 이전 포스트에서 사용했던 방식이다.)
2. **Continuous Bag of Words (CBOW)**
	BOW 단어들(즉, 주변에 있는 단어들)을 보고 중심 단어를 예측

또한 Loss Function에 대해서 다양한 방식들이 있다.

1. **Naive Softmax**
	Naive Softmax 방식은 간단하긴 하지만 수식($P(o|c) = \frac{\exp(u_{o}^T v_{c})}{\sum_{w \in V} \exp(u_{w}^T v_{c})}$)을 보면, 분모를 계산하는 과정에서 모든 단어들에 대해서 계산을 해야 하기 때문에 너무 오래 걸린다. 
2. **Negative Sampling**
	따라서 실제 standard word2vec은 skip-gram과 negative sampling 방식을 채택한다.
	해당 방식을 바로 뒤에서 좀 더 자세히 다루겠다.
3. etc...

## Negative Sampling
Negative Sampling 방식은 거짓 랜덤 샘플 k개를 생성해서, 모델을 진짜 단어가 올 확률을 높이고, 가짜 단어가 올 확률을 낮추는 방향으로 모델을 학습시킨다.

손실함수는 다음과 같다.
$$
J_{\text{neg-sample}}(\mathbf{u}_o, \mathbf{v}_c, U) = -\log \sigma(\mathbf{u}_o^T \mathbf{v}c) - \sum_{k \in \{K\text{ sampled indices}\}} \log \sigma(-\mathbf{u}_k^T \mathbf{v}_c)
$$

### 거짓 샘플은 어떻게 생성하는 걸까?
$$
P(w) = \frac{U(w)^{3/4}}{Z}
$$
$U(w)$는 unigram distribution. 즉, 단어 등장 확률 분포이다. $Z$는 정규화 상수로 정규 분포로 만들어준다는 의미다.
$U(w)$에 $\frac{3}{4}$ 제곱을 함으로써, 기존 단어집에서 자주 등장하는 단어는 덜 뽑히고 덜 등장하는 단어는 약간 더 자주 뽑히도록 샘플링한다.


### Negative Sampling 방식으로 Gradient Descent 하기
그래서 위 방식대로 Stochastic GD를 한다면, 한 번에 하나의 윈도우만 보고 gradient를 계산하는데 한 윈도우 안에는 $2m + 1 + 2km$ (m: 윈도우 크기, k: negative 샘플 수) 개의 단어 정도만 있게 된다.

따라서 전체 단어 $2|V| \times  d$에 비해 업데이트해야 하는 단어 수가 매우 적어진다. 즉, 매우 Sparse하게 된다.

이 Sparse 함을 이용해서 우리는 계산을 좀 더 효율적으로 해볼 수 있을 것이다!
아래 같은 방식들이 있을 수 있다.
- sparse matrix 
  활성화하고 싶은 값에만 활성화하고 나머지는 0으로 해서 특정 행만 업데이트 시킨다.
- hash 테이블


# 그냥 같이 등장한 횟수만 쓰면 되지 않을까?
word2vec을 쓰려면 코퍼스를 계속 반복하면서 학습해야 word vector를 얻을 수 있다.
굳이 word2vec을 써야 하는 걸까?

그냥 같은 문장에서 얼마나 자주 같이 등장했는지의 횟수만 쓰면 안 되는 걸까?

즉, co-occurence counts를 쓰면 되지 않을까?
![](https://i.imgur.com/1xtL0wc.png)
이런 식으로 같이 등장한 횟수를 세는 것이다.
근데 이 행렬은 보다시피 sparse matrix이기 때문에, 이를 적은 수의 dense vector로 쪼개보자.

선대의 꽃(?)인 **Singular Value Decomposition (SVD)** 를 쓰자. (~~또 당신입니까...~~)
$$
X = U \sum V^{T}
$$
SVD를 이용해서 쪼개버린다면 우리는 적은 수의 dense vector만 저장하면 된다.

## 하지만 그냥 raw count 데이터를 쓰면 안 된다...
그냥 쓰면 효과가 적다.
우리는 the, he, has 같은 쓸데(?)없지만 자주 등장하는 단어들(function word)이 있음을 상기해보자. 이런 단어들은 자주는 등장하지만 의미에 큰 영향을 주지 않는다. 따라서 이런 단어들을 제거해줄 필요가 있다.
- frequency에 log를 취하기
- 특정 값 t에 대해서, t보다 크면 다 t로 만들어버리기 = $min(X,t)$
- 그냥 function word들 전부 무시해버리기

이런 식으로 데이터를 정제해준 후에 SVD를 하는 것이 좋다.

## GloVe
위 논문에서는 단어의 의미를 **벡터의 차이**로 표현하자는 제안을 한다.
그래서 위에서 구한 co-occurence probability의 비율을 벡터 공간의 선형 구조로 표현하고자 한다.
→ Log-bilinear model로 벡터 차이 값을 표현한다.

![](https://i.imgur.com/xha8lcM.png)
이런 식으로 표현할 수 있다.

![](https://i.imgur.com/3ymulNi.png)
이렇게 loss 함수를 설정하면 Word2Vec과 비교해서 빠르게 훈련이 가능하고 거대한 코퍼스에 대해서도 가능하다.
# 하나의 단어에는 여러 개의 의미가 있는데...
하지만 하나의 단어에는 여러 개의 의미가 있다.

예를 들어, &quot;pike&quot; 라는 단어에는 다음과 같은 뜻들이 존재한다.
![](https://i.imgur.com/1CFSysP.png)

이런 걸 어떻게 구분할 수 있을까?

우리는 위 뜻들의 pike들이 벡터로 표현돼서 결국 하나의 벡터로 합쳐지게 된다. 그걸 수식으로 하면 다음과 같다.

$$
v_{\text{pike}} = a_1 v_{\text{pike}1} + a_2 v{\text{pike}2} + a_3 v{\text{pike}_3} + \dots
$$
근데 놀랍게도! 이 단어들은 서로 구분이 된다.

그 이유는 대다수의 경우, 하나의 문장에는 하나의 의미로써 쓰이지.. 같은 단어임에도 서로 다른 의미로 해당 단어를 쓰는 문장은 거의 없다. 
그러다보니 저 $a_{1}, a_{2} , \dots$ 들이 sparse 하게 되어서 자동으로 분리가 되어버린다.


# Named Entity Recognition(NER)
![](https://i.imgur.com/LjeYqC1.png)

근데 이름 같은 고유명사 같은 경우는 이게 사람 이름인지 지명 이름인지 헷갈릴 때가 존재할 수 있다.
이건 어떻게 해결해야  할까?

SSimple NER에서는 수작업으로 데이터들을 라벨링해서 supervised learning을 했다고 한다.
binary logistic classifier로 학습했다. (물론 실제로는 multi-class softmax를 쓴다.)
yes or no로 학습.

전통적인 softmax 분류기는 선형 구분 밖에 못하지만, 신경망으로는 비선형적인 분류 또한 가능하다
![](https://i.imgur.com/2r0Qs7l.png)

</content:encoded></item><item><title><![CDATA[Batch Normalization을 하는 이유는 뭘까?]]></title><link>https://jinsoolve.netlify.app/posts/why-do-batch-normalization</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/why-do-batch-normalization</guid><pubDate>Thu, 24 Apr 2025 15:00:00 GMT</pubDate><content:encoded>
- 각 batch 데이터마다 분포가 다르게 되면 모델의 학습이 어렵다.
- 즉, 전에는 0 ~ 20 데이터가 들어와서 그거대로 학습했는데, 이번 batch에는 2000 ~ 4000 데이터가 들어와버리면 앞 batch에서의 영향을 거의 못 받아 버린다.
- 마치 늘 새로운 걸 배우는 것처럼 되어버린다.

근데 여기서 의문인 건, 어쨌든 같은 데이터셋에 있던 데이터들이니깐, 20하고 4000은 그 만큼 큰 차이로 받아들어야 하는 거 아닌가 하는 의문이 들었다.

- BatchNorm은 20과 4000을 동일한 의미로 만드는 게 아니라 조건을 일정하게 만들어주는 것.
- batch 내의 평균과 분산을 맞춰주되, 스케일 &amp; 시프트 파라미터 ($\gamma$, $\beta$)로 원래 차이를 표현 가능하게 한다고 한다.</content:encoded></item><item><title><![CDATA[Word Vectors]]></title><link>https://jinsoolve.netlify.app/posts/intro-to-nlp-3</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/intro-to-nlp-3</guid><pubDate>Thu, 24 Apr 2025 15:00:00 GMT</pubDate><content:encoded>
이번 포스트에서는 Word Vector에 대해서 알아보자.

먼저 아래와 같은 고민을 해보자.
우리는 **어떻게 컴퓨터에게 단어의 뜻을 이해시킬 수 있을까?**

# 어떤 노력들이 있었을까...
## WordNet?
![](https://i.imgur.com/6GU7crk.png)

예전에 WordNet라는 단어들 간의 관계에 대한 데이터셋이 있었는데, 같은 의미이거나 상위적인 의미이거나 이런 걸 표현했다.
근데 이런 WordNet은 미묘한 뉘앙스를 이해하지 못하거나, 정적 데이터이다 보니 계속해서 변화하는 자연어의 의미를 반영하지 못 했다. 또한 주관적이기도 하고, 인간 자체 노동이 필요했다. 
이런 방식이다보니, 정확한 단어의 유사도를 얻기 어려웠다.

## Representing words as discrete symbols?
예전 NLP에서 단어를 컴퓨터에게 이해시킬 때 one-hot vector 방식을 채택해서 discrete한 값을 갖게 되었다.
![](https://i.imgur.com/c1Hq3o5.png)
매우 유사한 단어지만, 그 유사성이나 관계를 제대로 표현하지 못 했다.
## Distributional Semantics
&gt; &quot;You shall know a word by the company it keeps&quot; (J. R. Firth 1957: 11)

그러다가 **&quot;결국 단어는 그 문맥으로 이해할 수 있는 거 아니야?&quot;** 라는 생각을 누군가가 하게 된다.
즉, 비슷한 단어들과 함께 등장하는 2개의 단어가 있다면 그 두 단어는 서로 유사하다 라는 말로 이해할 수 있을 것이다.
위 아이디어에서 **Word Vector** 가 등장하게 된다.

# Word Vector
사실, Word Vector는 말 그대로 **단어를 실수값들로 이루어진 벡터로 표현**한 것 뿐이다.
![](https://i.imgur.com/zuUWcz6.png)

이처럼 비슷한 단어라면 그 값 또한 비슷할 거다. distributed하게(=분산해서) 표현하다보니 이런 유사성을 표현할 수도 있었다.
![](https://i.imgur.com/uFDzqX6.png)
비슷한 단어라면 비슷하게 뭉쳐 있을 것이다.

# Word2Vec
그럼 이 word vector들을 어떻게 구할 수 있을까?
Word2Vec은 이 word vector값이 무엇인지 구하게 해주는 Framework(즉, skeleton code 같은 것. 미리 짜여진 무언가)이다.
![](https://i.imgur.com/tJ5Dpvy.png)
이런 Center word에 대해서 앞뒤로 문맥을 살펴보면서 어떤 단어가 나타나는 지를 보고 학습하여 word vector를 구하는 방식이다.

여기서 $P(w_{t+j} | w_{t})$는 $w_t$의 앞뒤로 (window size 안쪽으로) $w_{t+j}$가 나올 확률을 의미한다.
모델이 학습하게 될 많은 자연어들, 문장들에서 어떤 단어 A 앞뒤로 어떤 단어 B가 자주 등장할수록 모델은 A  앞뒤로 단어 B가 나오는게 자연스럽다를 학습하게 될 것이다. 가장 자연스럽게 되도록 단어 A에 대한 word vector 값이 정해지게 된다.
## $P(w_{t+j},t_{j})$는 어떻게 구하나?
그럼 이때 $P(w_{t+j},t_{j})$는 어떻게 구하는 지 살펴보자.

먼저 우리는 각 단어의 word vector들을 작은 랜덤값으로 초기화한다.
이제 이걸 토대로 다음과 같이 계산한다.

$$
P(o|c) = \frac{\exp(u_{o}^T v_{c})}{\sum_{w \in V} \exp(u_{w}^T v_{c})}
$$
여기서 $v_{w}$ := `단어 w가 중심 단어에 있을 때의 word vector`이고,  $u_{w}$ := `단어 w가 context word에 있을 때의 word vector`이다.

이런 식으로 벡터곱을 해주고, softmax를 해주면 단어 c의 주변에 단어 o가 오게 될 확률을 구할 수 있다.

### 여기서 $u_{o}^T \cdot v_{c}$는 뭘 의미하는 걸까?
말 그대로 벡터끼리의 내적(dot product)이다.
![](https://i.imgur.com/C2bONmV.png)

$$
\vec{u} \cdot \vec{v} = \sum_{i=1}^{n} u_i v_i = \|u\| \cdot \|v\| \cdot \cos(\theta)
$$

단어 c의 앞뒤로 o가 나오는게 자연스럽다면 이 값이 당연히 커질 것이다.

## Word2Vec의 Loss 함수
그럼 이제 어떻게 Loss 함수를 정해야 하는 지를 보자.

![](https://i.imgur.com/nyBQaAM.png)
Likelihood. 쉽게 말하면, &apos;얼마나 그럴 듯 한 지&apos;를 수치로 표현한 것이다.
어떤 문장이 주어졌을 때, 각 중심 단어에 대해서 앞뒤 문맥이 자연스러운지를 표현한 확률을 모두 곱하면 그게 해당 문장이 그럴듯한지에 대한 지표가 된다.

이제 이걸 negative log likelihood를 취해서 손실함수로 만들자.
![](https://i.imgur.com/7bVB2jT.png)
이 손실함수를 최소화시키도록 학습하면 된다.

## Word2Vec의 프로세스 정리
$\theta$는 모든 word vector를 하나에 모은 거대한 벡터라 하자.
단어가 총 $|V|$ 개 라면, 각 단어마다 word vector가 2개($v_{w}$, $u_{w}$) 이므로 총 $2|V|$가 된다.
그리고 각 word vector의 차원은 d. 즉, d개의 실수들을 가진 벡터로 표현한다고 하자.

1. 모든 word vector들을 작은 랜덤값으로 초기화
2. $P(w_{t+j},t_{j})$를 구하고 이 값들을 이용해서 $J(\theta)$도 구해준다.
3. $J(\theta)$가 최소가 되는 방향으로 Gradient Descent를 해준다. → Optimization
4. 최적화가 완료되면 모든 word vector들이 잘 완성되어 있을 것이다.

## Optimization
그런데 손실함수 계산할 때 이 단어들을 모두 학습시키면 너무 오래 걸린다. 이 window들이 너무나도 많기 때문이다. 
그래서 Stochastic GD나 mini-batch GD를 사용한다.

예시를 통해서 이해해보자.

&gt; the cat sits on the mat

위와 같은 문장이 있다고 하자. 이때 window 크기 m = 2 이라 하자. 그럼 다음과 같은 18개의 window들이 생길 수 있다.

```text
(The → cat)
(The → sits)
(cat → The)
(cat → sits)
(cat → on)
(sits → The)
(sits → cat)
(sits → on)
(sits → the)
(on → cat)
(on → sits)
(on → the)
(on → mat)
(the → sits)
(the → on)
(the → mat)
(mat → on)
(mat → the)
```

### Batch GD
1. 모든 18개의 윈도우 쌍에 대한 loss들을 계산
2. loss 미분해서 각 윈도우 쌍 마다의 gradient를 계산
3. 모든 Gradient들의 평균을 계산
4. 평균 Gradient로 모든 단어벡터에 대해서 업데이트

→ 1 epoch에 대해서 1번 업데이트
&lt;Callout type=&quot;info&quot;&gt;
1 epoch란?
전체 훈련 데이터를 다 사용해서 학습하는 과정
&lt;/Callout&gt;
### Stochastic GD
다음 같이 이루어짐.
Step 1:
  (The → cat) → loss 계산 → 업데이트
Step 2:
  (The → sits) → loss 계산 → 업데이트
Step 3:
  (cat → The) → loss 계산 → 업데이트
...
Step 18:
  (mat → the) → loss 계산 → 업데이트

→ 1 epoch에 대해서 18번 업데이트

### Mini-batch GD
batch size = 4라 가정하자.

1. (The → cat), (The → sits), (cat → The), (cat → sits) → 평균 loss 계산 → gradient 계산 → 업데이트
2. (cat → on), (sits → The), (sits → cat), (sits → on) → 평균 loss 계산 → 업데이트
3. ...
4. ...
5. ...

위와 같이 업데이트 된다. → 1 epoch에 대해서 5번 업데이트

&gt; 참고로, loss들을 평균내서 미분해서 gradient를 얻어낸 것과 
&gt; loss들을 각각 미분해서 gradient를 얻어낸 후 gradient를 평균낸 것은 수학적으로 동일하다고 한다.

</content:encoded></item><item><title><![CDATA[백준 28129 - 2022 APC가 어려웠다고요?]]></title><link>https://jinsoolve.netlify.app/posts/boj-28129</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/boj-28129</guid><pubDate>Thu, 27 Mar 2025 15:00:00 GMT</pubDate><content:encoded>위 포스트는 [백준 28129 - 2022 APC가 어려웠다고요?](https://www.acmicpc.net/problem/28129)의 풀이입니다.

# 핵심 아이디어
`dp[i][j] := i번째 수가 j가 되는 경우의 수`

$$
dp[i][j] = \sum_{k=max(j-k,a[i-1])}^{min(j+k,b[i-1])} dp[i-1][k]
$$

위와 같은 dp 식을 누적합으로 $O(N^2)$으로 해결할 수 있다.

# 코드
```cpp
#include &lt;bits/stdc++.h&gt;

#define endl &quot;\n&quot;
#define all(v) (v).begin(), (v).end()
#define all1(v) (v).begin()+1, (v).end()
#define For(i, a, b) for(int i=(a); i&lt;(b); i++)
#define FOR(i, a, b) for(int i=(a); i&lt;=(b); i++)
#define Bor(i, a, b) for(int i=(a)-1; i&gt;=(b); i--)
#define BOR(i, a, b) for(int i=(a); i&gt;=(b); i--)
#define ft first
#define sd second

using namespace std;
using ll = long long;
using lll = __int128_t;
using ulll = __uint128_t;
using ull = unsigned long long;
using ld = long double;
using pii = pair&lt;int, int&gt;;
using pll = pair&lt;ll, ll&gt;;
using ti3 = tuple&lt;int, int, int&gt;;
using tl3 = tuple&lt;ll, ll, ll&gt;;

template&lt;typename T&gt; using ve = vector&lt;T&gt;;
template&lt;typename T&gt; using vve = vector&lt;vector&lt;T&gt;&gt;;

template&lt;class T&gt; bool ckmin(T&amp; a, const T&amp; b) { return b &lt; a ? a = b, 1 : 0; }
template&lt;class T&gt; bool ckmax(T&amp; a, const T&amp; b) { return a &lt; b ? a = b, 1 : 0; }

const int INF = 987654321;
const int INF0 = numeric_limits&lt;int&gt;::max();
const ll LNF = 987654321987654321;
const ll LNF0 = numeric_limits&lt;ll&gt;::max();

const int mxn = 3001;
const ll mod = 1e9+7;

void solve() {
    int n, k; cin &gt;&gt; n &gt;&gt; k;
    ve&lt;int&gt; a(n+1), b(n+1);
    for(int i=1; i&lt;=n; i++) cin &gt;&gt; a[i] &gt;&gt; b[i];

    vve&lt;ll&gt; dp(n+1, ve&lt;ll&gt;(mxn, 0));
    vve&lt;ll&gt; psum(n+1, ve&lt;ll&gt;(mxn, 0));
    for(int i=1; i&lt;=n; i++) {
        if(i == 1) {
            for(int j=a[i]; j&lt;=b[i]; j++) {
                dp[i][j] = 1;
            }
        }
        else {
            for(int j=a[i]; j&lt;=b[i]; j++) {
                int l = max(j-k,a[i-1]), r = min(j+k,b[i-1]);
                dp[i][j] = (psum[i-1][r] - psum[i-1][l-1] + mod) % mod;
            }
        }

        for(int j=1; j&lt;mxn; j++) {
            psum[i][j] = (psum[i][j-1] + dp[i][j]) % mod;
        }
    }
    cout &lt;&lt; psum[n][mxn-1] &lt;&lt; endl;
}

int main(void) {
    ios_base::sync_with_stdio(false);
    cin.tie(nullptr);
    cout.tie(nullptr);

    int TC=1;
//    cin &gt;&gt; TC;
    FOR(tc, 1, TC) {
//        cout &lt;&lt; &quot;Case #&quot; &lt;&lt; tc &lt;&lt; &quot;: &quot;;
        solve();
    }


    return 0;
}
```</content:encoded></item><item><title><![CDATA[Quantized Side Tuning 논문 리뷰]]></title><description><![CDATA[Quantized Side Tuning: Fast and Memory-Efficient Tuning ofQuantized Large Language Models 논문에 대한 리뷰를 작성한 글입니다.]]></description><link>https://jinsoolve.netlify.app/posts/Quantized Side Tuning</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/Quantized Side Tuning</guid><pubDate>Mon, 10 Mar 2025 15:00:00 GMT</pubDate><content:encoded>위 포스트는 [Quantized Side Tuning: Fast and Memory-Efficient Tuning ofQuantized Large Language Models](https://arxiv.org/abs/2401.07159) 논문에 대한 리뷰를 작성한 글입니다.

&lt;Callout type=&quot;danger&quot;&gt;
이 글은 공부를 하면서 작성한 글이므로 잘못된 부분이 있을 수 있습니다.
&lt;/Callout&gt;

위 논문을 주제로 세미나 발표를 했는데, 해당 [발표 자료](https://docs.google.com/presentation/d/1wjYZsqvrNqP0CigTNErqj3WX-j9ssWQXwN5DXN4TIM0/edit?usp=sharing)를 첨부합니다.
# Abstract

# 1 Introduction


# 2 Related Work

## 2.1 Parameter-Efficient Finetuning
LLM을 특정 도메인에 최적화시키기 위해 우리는 finetuning을 한다.
하지만 LLM 크기가 점차 커지면서, 전체 모델을 finetuning하는 건 비싸졌고 이를 줄이기 위해 **PERT(Parmeter-Efficient Fine-Tuning)**을 사용하고자 했다.

PERT는 **전체 가중치를 조정하지 않고 일부 파라미터만 조정하여 특정 작업에 맞게 Finetuning 하는 방법**을 의미한다.

&lt;Callout type=&quot;info&quot;&gt;
PERT에는 여러 종류가 있다.

- 입력 테스트 또는 어텐션 앞에 학습 가능한 프롬프트 임베딩을 추가하는 방식
- 특정 작업에 특화된 새로운 파라미터를 Transformer 층 내부에 삽입하는 방식
- low-rank decomposition을 활용하여 LLM 가중치에 삽입할 수 있는 학습 가능한 파라키터를 구성하는 방식(LoRA)
- 등등...
&lt;/Callout&gt;

이런 PERT로 학습 가능한 파라미터 수를 최소화하면서 모델 성능을 최적화할 수 있게 되었다.
그러나 **학습 가능한 파라미터 수를 줄이는 것이 반드시 메모리 점유율 감소로 이어지는 것은 아니다.**

## 2.2 Memory-Efficient Training and Finetuning

### 가역 신경망

기존 신경망은 역전파를 수행하기 위해서 순전파 과정에서 나온 모든 중간 결과(Activation 값)을 저장해야 한다. 이때의 메모리 사용량이 높기 때문에 이를 해결하기 위해 가역 신경망이라는 개념이 나왔다.

![](https://i.imgur.com/IDABPHd.png)

**가역 신경망(Reversible Neural Networks)** 은 순전파와 역전파가 서로 반대로 수행할 수 있도록 설계된 모델로, 순전파에서의 출력을 사용하여 역전파 과정에서 입력을 복원할 수 있는 신경망이다.

위 그림은 가역 신경망의 대표적인 예시인 RevNet(ResNet을 기반으로 한 신경망)이다.
Adder-Split 구조로, 네트워크의 입력 X를 두 개의 부분으로 나누고 서로를 활용하여 출력을 계산하는 방식이다.

이런 식으로 하면 입력을 다시 복원할 수 있으므로 역전파를 수행할 때 추가 메모리를 사용할 필요 없이 효율적으로 학습이 가능하다.

### 네트워크 압축

또 다른 방법으로는 **네트워크 압축(Network Compression)** 이라는 것이 있다.
이는 원래 LLM을 더 작은 형태로 축소하여 훈련 및 추론 과정 모두에서 계산 효율성을 높인다.
예시로는 Network Pruning 이나 Knowledge Distillation이 있다.
&lt;Callout type=&quot;info&quot;&gt;
- Network Pruning: 개별 파라미터의 중요도를 평가한 후 중요도가 낮은 파라미터를 제거해서 모델 간소화
- Knowle Distillation: 교사-학생 모델. 학생 네트워크가 특정 데이터셋에서 교사 네트워크의 출력 분포를 근사하도록 훈련하는 방식
&lt;/Callout&gt;

PERT는 소수의 학습 가능한 파라미터만 업데이트해서 기존 LLM 능력은 유지하면서 특정 작업에 대한 성능을 극대화시키지만 네트워크 압축은 모델 크기를 줄이고 추론 속도를 높이지만, 원래 모델의 복잡한 표현력을 일부 잃을 가능성이 있다.

### QLoRA
QLoRA는 LLM을 4비트로 양자화 한 후, LoRA를 추가하여 양자화된 LLM을 Finetuning 하는 방식이다.

&lt;Callout type=&quot;&quot;&gt;
여기서, 양자화란?
양자화라는 단어 때문에 어렵게 느껴지는 데 사실 별 거 없다.
16비트(혹은 32비트) 수를 4비트로 표현하기 위해 압축하는 것이다. 
특정 범위의 수를 예를 들어 0 ~ 9의 수는 0으로, 10~19의 수는 1로 이런 식으로 압축하는 방식이다.
좀 나이브하게 설명했는데, 자세한 방법은 나중에 설명하겠다.
&lt;/Callout&gt;

QLoRA는 full finetuning 과 비교하여 가중치와 Optimizer의 메모리 점유율을 크게 줄이면서도 유사한 성능을 유지할 수 있다.
하지만 QLoRA는 중간 활성화 값의 메모리 점유율을 고려하지 않기 때문에, 큰 배치 크기로 LLM을 finetuning하는 경우 훈련 시간이 길어지는 문제가 발생.

### Side Network
Side Network를 도입하여 Side Network의 특징 상 역전파 과정이 불필요 하므로 증간 활성화 값과 관련된 메모리 점유율을 줄이는 역할을 한다.

하지만 Side Network를 도입하더라도 LLM 자체의 모델 크기(즉, 가중치의 메모리 점유율)는 여전히 계산 비용 문제를 야기한다. 이러한 방법들은 30억개 미만의 파리미터를 가진 모델에만 적용될 수 있으며, 더 큰 모델을 finetuning하는데에는 실패한다.

&lt;Callout type=&quot;&quot;&gt;
Side Network란?
Finetuning의 일종으로, 기존 LLM은 그대로 두고 작은 side network를 추가하여 해당 network에 대해서만 학습 시키는 방식.
메모리 사용량을 줄이면서도 성능 유지 가능.
&lt;/Callout&gt;

&lt;Callout type=&quot;&quot;&gt;
Side Network(Side Tuning)이 안 되는 이유?
1. 대형 모델에서는 큰 효과가 없기 때문
	- 작은 모델은 원래 표현력이 부족하기 때문에 side network의 영향이 커서 side network를 학습한 것만으로도 충분히 학습이 가능함.
	- 그러나 대형 모델의 경우, 기존 모델의 표현력이 너무 강력해서, side network로는 역부족이다.
2. 대형 모델은 더 많은 데이터를 필요로 하는데, side network만으로는 역부족
	- 대형 모델은 이미 거대한 데이터로 학습됨.
	- side network가 새로운 데이터로 아무리 학습해도  기존 모델이 업데이트가 되지 않기 때문에 한계가 있음
3. 중간 활성화 값 메모리 문제
	- 대형 모델은 층의 개수가 많고, 활성화 값의 크기도 크다.
	- Side Network만 학습하기 때문에 기존 모델의 활성화 값도 저장해야 한다. 따라서 메모리 사용량 증가.
	- 애초에 메모리를 절약하려고 만든 방법인데 큰 모델에서는 오히려 메모리 사용량이 증가할 수도 있음
&lt;/Callout&gt;

# 3 Quantized Side Tuning

## 3.1 4-bit Quantization
아까 위에서 잠깐 언급한 4비트 양자화의 방법에 대해서 다시 한 번 설명해 보겠다.
$$
X^{4bit} = \text{round} \left( \frac{M_{4bit}}{\text{Absmax}(X^{16bit})} X^{16bit} \right)
\\[5pt]
= \text{round} \left( c^{16bit} \cdot X^{16bit} \right)
$$
위 공식은 쉽게 말하면 16비트의 범위의 수를 4비트의 범위 수로 근사화시키는 것이다. 
(이때 $\text{Absmax}(X^{16bit})$ 는 X의 값들 중 절댓값이 가장 큰 애의 값이고, $M_{4bit}$은 4비트에서 가질 수 있는 최대값이다.) 
참고로 여기서 $c^{16bit}$는 양자화 상수이다. 우리는 이 양자화 상수를 가역하면 된다.

반대로 역양자화(De-quantization)은 다음과 같다.
$$
\text{dequant}(c_{16bit}, X_{4bit}) = \frac{X_{4bit}}{c_{16bit}} = X_{16bit}
$$

위와 같은 공식을 통해 16비트 수를 4비트 수로 나타낼 수 있다.

하지만 저런 식으로 양자화를 시켜버리면 당연히 복구 했을 때 왜곡되는 경우가 나타날 것이다.
특히 다른 값들에 비해 매우 큰 값이 포함되어 있을 때(즉, 이상치(outlier)문제일 때) 왜곡이 심하다. 다음과 같은 예시를 살펴보자.
$X_{16bit} = [-5, -3, -1, 1, 3, 5, 100]$ 이런 경우 4비트로 양자화하면 $X_{4bit} = \text{round} (7 \times X_{\text{norm}}) = [0, 0, 0, 0, 0, 0, 7]$ 이런 식으로 되어버리므로 왜곡이 생긴다.

이를 해결하기 위해, 입력 텐서($X$)를 개별 블록으로 나누어 묶고 각 블록을 독립적으로 양자화하는 방식을 사용한다. ($X \in \mathbb{R}^{b \times h}$)
예를 들어, 각 블록이 B개의 요소를 포함한다고 하자. X를 1차원 배열($b \times  h$)로 변환한 후에 B개씩 묶어보면 총 n개($= \frac{b \times h}{B}$)가 나온다. 이렇게 블록들을 만들고 각 블록들에 대해 독립적으로 양자화한다면 왜곡을 완화시킬 수 있다.
블록 크기를 작게 할수록 양자화 오차가 줄어들지만, 블록크기가 너무 작으면 각 블록마다 양자화 상수를 저장해야 하므로 메모리 사용량이 증가할 수도 있기 때문에 적절한 블록 크기를 설정하는 것이 중요하다.

그럼에도 불구하고 양자화 상수가 16비트이고 블록이 늘어날 수록 양자화 상수 또한 많이 저장해야 하므로 메모리 오버헤드가 발생한다. 따라서 해당 논문에서는 양자화 상수의 메모리 점유율을 줄이기 위해 기존과 동일한 양자화 전략을 적용하여 **양자화 상수 자체를 다시 양자화** 하는 전략을 취했다.
위 논문에서는 8비트 수를 이용하여 양자화 상수를 양자화한다. 따라서 순전파(Forward Pass)는 다음과 같이 정의된다.

$$
Y_{16bit} = \text{dequant}(\text{dequant}(c_{16bit}^{2}, c_{8bit}^{1}), W_{4bit}) X_{16bit}
$$
즉, 8비트인 \[양자화 상수의 양자화 상수\]를 이용해 \[양자화 상수\]를 복원시킨 후에, 얻은 양자화 상수로 원래 16비트 수를 복원시킨다.

이처럼 4비트 양자화는 가중치의 메모리 점유율을 크게 줄여서 LLM의 저장 및 배포를 용이하게 만든다. 또한 저정밀 소수연산은 GPU 같은 최신 가속기에서 더 빠르게 실행될 수 있어서 좋다.

그럼에도 불구하고, **양자화 과정에서 고정밀 데이터 타입에서 저정밀 데이터 타입으로 변환할 때 정보 손실이 발생**하며, 이는 정확도 저하의 원인이 될 수 있다.

## 3.2 Side Tuning
이제 **LLM 훈련 과정에서의 메모리 점유율(memory footprint)을 분석한 후**, 양자화 과정에서 발생하는 정보 손실을 줄이고 정확도 감소를 최소화하는 **측면 네트워크(side network)의 신경망 아키텍처**를 소개하겠다.

### 훈련 단계에서의 메모리 점유율

주어진 LLM이 N개의 층을 가지고 있다고 할 때, $y_i = f_i(W_i, x_i)$는 LLM의 i번째 Transformer 층을 나타낸다.
• 여기서, $x_i$는 i번째 층의 입력이며, 이전 층의 출력값과 동일하다. 즉, $x_i = y_{i-1}$이다.

LLM의 훈련 과정에서 필요한 메모리는 **세 가지 주요 요소**로 구성된다.

1. $M_1$ **- LLM의 가중치(Weights)** $\{W_i\}_{i=1}^{N}$
	• 모델이 학습해야 하는 기본적인 파라미터.
2. $M_2$ **- 옵티마이저 상태(Optimizer State)**
	• **Adam 옵티마이저**(Kingma &amp; Ba, 2014)를 사용할 경우, **훈련 가능한 파라미터 크기의 3배**의 메모리를 필요로 한다.
		• (1) 그래디언트(gradient) 저장
		• (2) 1차 모멘트(moment) 저장
		• (3) 2차 모멘트(moment) 저장
3. $M_3$ **- 중간 활성화 값(Intermediate Activations)** $\{y{\prime}i\}{i=1}^{N}$
	• **모델의 깊이(depth), 너비(width), 배치 크기(batch size), 시퀀스 길이(sequence length)** 등의 훈련 설정에 따라 메모리 점유율이 결정됨.

### QLoRA의 한계
• QLoRA는 LLM의 가중치($M_1$) 및 옵티마이저 상태($M_2$)의 메모리 점유율을 줄이는 데 성공했지만, 중간 활성화 값($M_3$)을 줄이는 데 실패했다.
• 따라서 큰 배치 크기(batch size) 또는 긴 시퀀스 길이(sequence length) 를 사용하는 경우, QLoRA의 메모리 점유율이 급격히 증가한다.
• 반대로, 배치 크기를 줄이면 메모리 사용량은 감소하지만, 훈련 시간이 길어지는 단점이 발생한다.

### 기존 연구의 한계
• Sung et al. (2022)는 중간 활성화 값($M_3$)의 메모리 점유율만 줄이는 방법을 제안했지만,
• 30억(3B) 개 이상의 파라미터를 가진 모델을 미세 조정하는 데 어려움을 겪는다.</content:encoded></item><item><title><![CDATA[n-gram Language Models]]></title><link>https://jinsoolve.netlify.app/posts/intro-to-nlp-2</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/intro-to-nlp-2</guid><pubDate>Sun, 09 Mar 2025 15:00:00 GMT</pubDate><content:encoded>**언어 모델**이란, 결국에는 **그 다음으로 어떤 단어가 오는 것이 가장 자연스러운지를 확률로 보고 가장 높은 확률의 단어를 선택해서 문장을 구성**하는 방식이다.

위 포스트에서는 n-gram 모델이 무엇이고 어떤 원리인지를 살펴볼 것이다.
# n-gram 언어모델이란?
**이전의 n-1개의 단어들을 살펴보고 그 다음 n번째로 어떤 단어가 올 확률이 가장 높은 지를 계산해서 문장을 구성하는 방식의 언어모델**이다.
$$
P(w_n | w_1, w_2, …, w_{n-1}) \approx P(w_n | w_{n-N+1}, …, w_{n-1})
$$
모든 단어를 살펴보는 것이 아니라, 최근 n-1개의 단어를 보고 그 다음 n번째 단어를 추정하는 방식.

# Sampling(샘플링)
샘플링이란, **전체 데이터 집단에서 일부 데이터를 선택**하는 과정이다.

위에서 n-gram 방식을 통해서 그 다음에 올 단어들의 확률을 구했다고 가정하자. 그럼 해당 단어들을 어떻게 샘플링 할 것인가에 대해서 얘기해보자.
## Top-k
1 ~ k 등 까지의 단어들을 뽑는 샘플링
## Top-p
상위 누적확률 p 만큼까지의 단어들을 뽑는 샘플링

# Evaluating a language model
**Extrinsic Evaluation**은 실제로 언어모델을 실제상황에 적용하여서 평가하는 방식이다.
그러나 이 방식은 비싸고 시간이 많이 걸린다.
따라서 우리는 모델을 평가하기 위해 주로 **Intrinsic Evaluation**을 사용한다. Train과 Test Dataset을 나누어서 이를 적용해본다.
## Perplexity(ppl)
사전적 의미로는 **혼란도**라는 의미이다. 언어 모델의 평가지표 중 하나로써, 원하는 문장과 생성한 문장이 일치하지 않으면 혼란도는 높을 것이다. 쉽게 생각해보면, 모델의 손실함수와 동일한 의미를 갖는다고 볼 수 있다.
![](https://i.imgur.com/frP0S1z.png)
위와 같이 Cross-entropy 형태로 나타낸다.

&lt;Callout type=&quot;info&quot;&gt;
Cross-Entroy(교차 엔트로피)란, 두 개의 확률 분포 간의 차이를 측정하는 손실함수이다.
특히 분류 문제에서 모델의 예측이 정답과 얼마나 다른지를 평가하는 데 사용된다.
$$
H(P, Q) = - \sum_{i} P(x_i) \log Q(x_i)
$$
&lt;/Callout&gt;

당연히 모델이 정확해질수록(혹은 훈련을 더 많이 할수록) ppl 수치는 낮아질 것이다.

# Smoothing
Smoothing은 **훈련데이터에 없는 단어나 구성이 등장했을 때, 확률을 0으로 두지 않게 하기 위해 적절한 값을 부여하는 기법**이다.

확률을 0으로 설정해버리면, 해당 단어나 조합이 절대 발생하지 않는다고 판단하여 실제로 있는 말임에도 불구하고 언어모델이 해당 말을 생성하지 않는 문제가 발생한다. 이를 해결하기 위해 smoothing 기법을 사용한다.

Smoothing을 하는 방식에도 여러 가지가 있다. 총 3가지 방식이 있다.
- Additive: 약간의 값을 모든 확률에 더하는 방식
- Interpolation: 몇 개의 n-gram들을 결합하는 방식
- Discounting: 관측된 확률에서 약간의 양을 빼내서 새롭게 관측된 애들한테 나눠주는 방식

## Laplace Smoothing (Additive)
확률에 특정 값을 균일하게 더해서 해결하는 방식이다. 흔히, Add-k(혹은 Add-alpha) 기법으로 알려져 있다.
![](https://i.imgur.com/qVSPqnZ.png)

## Linear Interpolation
여러 개의 n-gram의 확률 값을 가중 평균을 이용해서 합쳐서 최종 확률을 계산하는 방식이다.
![](https://i.imgur.com/mZNmTTC.png)

위 그림은 trigram, bigram, unigram의 확률 값을 가중평균을 이용해서 합친 것이다. 위와 같이 처리해주면 확률이 0이 되는 문제점을 해결할 수 있다.

이때 가중평균의 $\lambda$값은 하이퍼 파리미터처럼 valid 데이터셋을 통해서 추정해내자.
## Absolute Discounting
Additive 기법과 반대의 느낌이다.
기존의 단어들과 구성들의 확률값에서 약간의 값을 뺀 후, 새로운 단어나 구성이 들어온다면 거기로 나눠줘서 해결하는 방식이다.
![](https://i.imgur.com/7zNdBpI.jpeg)

위 그림처럼 기존 확률값들에서 0.5씩 빼준 다음, 빼서 얻은 값을 새로운 단어나 구성이 들어온다면 거기로 나눠준다.</content:encoded></item><item><title><![CDATA[백준 1787 - 문자열의 주기 예측]]></title><link>https://jinsoolve.netlify.app/posts/boj-1787</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/boj-1787</guid><pubDate>Tue, 04 Mar 2025 15:00:00 GMT</pubDate><content:encoded>위 포스트는 [백준 1787 - 문자열의 주기 예측](https://www.acmicpc.net/problem/1787) 의 해설입니다.

# 핵심 아이디어
결국 부분 문자열에서 가장 짧으면서 일치하는 Prefix와 Suffix를 찾으면 된다. (해당 길이를 전체 부분문자열 길이에서 뺀 것이 정답이다.)
KMP 알고리즘의 Pi 배열을 이용해보자.

`pi[i] := 0~i 부분 문자열에서 prefix와 suffix가 일치하면서 가장 긴 길이`
이때, 우리는 가장 짧은 prefix와 suffix를 찾는데 이를 재귀적으로 해결할 수 있다.

$$
[pre] \dots [suf]
$$
이렇게 있을 때 pre와 suf는 일치한다. 따라서 pre의 pi 배열 길이만큼이 suf의 뒤에서 pi 배열 길이만큼에 일치한다는 사실을 이용해서 재귀적으로 해결할 수 있다.

따라서 재귀적으로 들어가면서 가장 작은 값을 체크해주면 된다.
# 코드
```cpp
#include &lt;bits/stdc++.h&gt;

#define endl &quot;\n&quot;
#define all(v) (v).begin(), (v).end()
#define all1(v) (v).begin()+1, (v).end()
#define For(i, a, b) for(int i=(a); i&lt;(b); i++)
#define FOR(i, a, b) for(int i=(a); i&lt;=(b); i++)
#define Bor(i, a, b) for(int i=(a)-1; i&gt;=(b); i--)
#define BOR(i, a, b) for(int i=(a); i&gt;=(b); i--)
#define ft first
#define sd second

using namespace std;
using ll = long long;
using lll = __int128_t;
using ulll = __uint128_t;
using ull = unsigned long long;
using ld = long double;
using pii = pair&lt;int, int&gt;;
using pll = pair&lt;ll, ll&gt;;
using ti3 = tuple&lt;int, int, int&gt;;
using tl3 = tuple&lt;ll, ll, ll&gt;;

template&lt;typename T&gt; using ve = vector&lt;T&gt;;
template&lt;typename T&gt; using vve = vector&lt;vector&lt;T&gt;&gt;;

template&lt;class T&gt; bool ckmin(T&amp; a, const T&amp; b) { return b &lt; a ? a = b, 1 : 0; }
template&lt;class T&gt; bool ckmax(T&amp; a, const T&amp; b) { return a &lt; b ? a = b, 1 : 0; }

const int INF = 987654321;
const int INF0 = numeric_limits&lt;int&gt;::max();
const ll LNF = 987654321987654321;
const ll LNF0 = numeric_limits&lt;ll&gt;::max();

int n;
string s;
ve&lt;int&gt; dp, pi;

vector&lt;int&gt; getPi(string p){
    int m = (int)p.size(), j=0;
    vector&lt;int&gt; ret(m, 0);
    for(int i=1; i&lt;m; i++){
        while(j &gt; 0 &amp;&amp; p[i] != p[j]) j = ret[j-1];
        if(p[i] == p[j]) ret[i] = ++j;
    }
    return ret;
}
int sol(int i) {
    if(i&lt;0) return INF;
    int &amp;ret = dp[i];
    if(ret != -1) return ret;
    if(!pi[i]) return ret = INF;
    return ret = min(pi[i], sol(pi[i]-1));
}

void solve() {
    cin &gt;&gt; n &gt;&gt; s;
    pi = getPi(s);
    dp = ve&lt;int&gt;(n, -1);

    ll ans = 0;
    for(int i=0; i&lt;n; i++) {
        ll res = sol(i);
        if(res != INF) ans += i+1 - res;
    }
    cout &lt;&lt; ans &lt;&lt; endl;
}

int main(void) {
    ios_base::sync_with_stdio(false);
    cin.tie(nullptr);
    cout.tie(nullptr);

    int TC=1;
//    cin &gt;&gt; TC;
    FOR(tc, 1, TC) {
//        cout &lt;&lt; &quot;Case #&quot; &lt;&lt; tc &lt;&lt; &quot;: &quot;;
        solve();
    }


    return 0;
}
```

# 참고
https://justicehui.github.io/poi/2020/05/09/BOJ1787/</content:encoded></item><item><title><![CDATA[백준 1055 - 끝이없음]]></title><link>https://jinsoolve.netlify.app/posts/boj-1055</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/boj-1055</guid><pubDate>Tue, 04 Mar 2025 15:00:00 GMT</pubDate><content:encoded>위 포스트는 [백준 1055 - 끝이없음](https://www.acmicpc.net/problem/1055)의 해설입니다.
# 핵심 아이디어

문자열이 재귀적으로 반복하는 것을 알 수 있다.

이때 min과 max의 차이가 최대 100개 정도임을 알 수 있고, 우리는 i번째 문자가 주어졌을 때 해당 i 번째 문자가 무엇인지를 알아차리는 함수를 작성하고 이를 100번만 반복하면 원하는 정답을 얻을 수 있다.

i번째 문자가 무엇인지를 알아차리는 것은 이분탐색을 이용해서 찾을 것이다.

예를 들어, `abc`와 `$x$y$z$`가 주어졌을 때, 2번 연산한다고 하면 첫번째 $는 `abcxabcyabczabc`로 변화할 것이다.

연산을 해보면 다음과 같은 수식으로 정리할 수 있다.

k번 연산했다고 가정했을 때, 다음과 같은 전체 문자열 길이를 얻을 수 있다.

$$
n \times \$^{k-1} + m \times \frac{\$^{k-1}-1}{\$-1}
$$

이때, n은 처음 입력 문자열의 길이, \$는 문자열 S의 \$의 개수, m은 문자열 S에서 \$가 아닌 문자의 개수가 된다.

위 수식을 이용해서 이분 탐색을 하여 i번째 문자가 무엇인지를 찾아내는 것이다.
이분 탐색을 해서 k번째 연산을 했을 때 다음 \$로 이동해야 할 때가 온다.
(예를 들어, `$x$y$z$`을 k번 연산했더니 i가 첫번째 \$안에 포함되지 않은 경우가 생긴다.)
이때 이동하면서 \$가 아닌 문자 번호인지를 체크하고, 만약 \$ 영역에 포함된다면 다시 재귀적으로 해결해주고, 그렇지 않다면 해당 문자열을 반환해준다.


# 코드
```cpp
#include &lt;bits/stdc++.h&gt;

#define endl &quot;\n&quot;
#define all(v) (v).begin(), (v).end()
#define all1(v) (v).begin()+1, (v).end()
#define For(i, a, b) for(int i=(a); i&lt;(b); i++)
#define FOR(i, a, b) for(int i=(a); i&lt;=(b); i++)
#define Bor(i, a, b) for(int i=(a)-1; i&gt;=(b); i--)
#define BOR(i, a, b) for(int i=(a); i&gt;=(b); i--)
#define ft first
#define sd second

using namespace std;
using ll = long long;
using lll = __int128_t;
using ulll = __uint128_t;
using ull = unsigned long long;
using ld = long double;
using pii = pair&lt;int, int&gt;;
using pll = pair&lt;ll, ll&gt;;
using ti3 = tuple&lt;int, int, int&gt;;
using tl3 = tuple&lt;ll, ll, ll&gt;;

template&lt;typename T&gt; using ve = vector&lt;T&gt;;
template&lt;typename T&gt; using vve = vector&lt;vector&lt;T&gt;&gt;;

template&lt;class T&gt; bool ckmin(T&amp; a, const T&amp; b) { return b &lt; a ? a = b, 1 : 0; }
template&lt;class T&gt; bool ckmax(T&amp; a, const T&amp; b) { return a &lt; b ? a = b, 1 : 0; }

const int INF = 987654321;
const int INF0 = numeric_limits&lt;int&gt;::max();
const ll LNF = 987654321987654321;
const ll LNF0 = numeric_limits&lt;ll&gt;::max();

const ll mxn = 1e9+99;
string p, s;
ve&lt;string&gt; notDollar;
ll op, Min, Max;
// a^b mod c
ll pow(ll a, ll b) {
    ll res = 1;
    while(b) {
        if(a &gt; mxn) return LNF;
        if(b%2) res = (res * a);
        a = (a * a);
        b &gt;&gt;= 1;
    }
    return res;
}

ll n, dollar, m;
ll f(ll k) {
    if(--k == 0) return p.length();
    if(dollar == 1) return n + m*k;
    ll dk = pow(dollar, k);
    if(dk == LNF) return LNF;

    return n*dk + m*(dk-1)/(dollar-1);
}

void solve() {
    cin &gt;&gt; p &gt;&gt; s &gt;&gt; op &gt;&gt; Min &gt;&gt; Max;
    Min--; Max--;
    string str;
    int _=0;
    for(auto c : s) {
        dollar += (c == &apos;$&apos;);
        m += (c != &apos;$&apos;);
        if(c != &apos;$&apos;) str += c;
        else if(_ != 0) {
            notDollar.emplace_back(str);
            str = &quot;&quot;;
        }
        _++;
    }
    if(!str.empty()) notDollar.emplace_back(str);
    n = p.length();

    string ans;
    ll opop = f(op+1);
    for(ll i=Min; i&lt;=Max; i++) {
        if(i &gt;= opop) {
            ans += &apos;-&apos;;
            continue;
        }
        ll target = i;
        while(target &gt;= (int)p.length()) {
            ll lo=1, hi=1e9;
            while(lo &lt; hi) {
                ll mid = lo + (hi-lo)/2 + (hi-lo)%2;
                ll res = f(mid);
                if(res &lt;= target) lo = mid;
                else hi = mid-1;
            }
            ll res = f(lo);
            target -= res;
            for(auto only : notDollar) {
                if(target &lt; only.length()) {
                    ans += only[target];
                    target = -1;
                    break;
                }
                target -= only.length();
                if(target &lt; res) break;
                else target -= res;
            }
        }
        if(target != -1) ans += p[target];
    }
    cout &lt;&lt; ans &lt;&lt; endl;

}

int main(void) {
    ios_base::sync_with_stdio(false);
    cin.tie(nullptr);
    cout.tie(nullptr);

    int TC=1;
//    cin &gt;&gt; TC;
    FOR(tc, 1, TC) {
//        cout &lt;&lt; &quot;Case #&quot; &lt;&lt; tc &lt;&lt; &quot;: &quot;;
        solve();
    }


    return 0;
}
```
# 후기
솔직히 아이디어 자체는 그리 까다롭지 않다고 생각하지만, 구현적인 아이디어가 상당히 까다롭고 반례가 잘 생길 수 밖에 없는 문제라고 생각한다.
필자는 https://testcase.ac/problems/1055 여기의 도움을 받아서 해결했다.
여기서 찾은 반례는 알고보니 pow 연산 시, a가 너무 커지면 안 돼서 막아놓았는데 이걸 $a \times a$ 전에 했어야 했는데 그러지 않아서 생겼던 문제였다.</content:encoded></item><item><title><![CDATA[백준 8872 - 빌라봉]]></title><link>https://jinsoolve.netlify.app/posts/boj-8872</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/boj-8872</guid><pubDate>Mon, 03 Mar 2025 15:00:00 GMT</pubDate><content:encoded>위 포스트는 [백준 8872 - 빌라봉](https://www.acmicpc.net/problem/8872) 문제의 해설입니다.

# 핵심 아이디어
위 문제에는 여러 개의 트리가 존재한다.
임의의 2개의 트리를 서로 이을 때 최대 시간이 최소가 되게 하기 위해서는 각 트리의 지름에서 중간점을 찾아, 그 중간점끼리 연결해야 최대시간이 최소가 될 것이다.

또한 각 트리끼리의 이동 시간의 최대시간을 최소로 만들려면 트리의 지름이 가장 큰 애와 나머지 트리를 서로 연결해야 할 것이다.

따라서 정답은 결국 3개 중 하나가 된다. (지름이 가장 긴 순서대로 번호가 매겨졌을 때라 가정하자.)
1. 1번 트리의 지름
2.  1번 트리 - L - 2번 트리
	$(1번 트리의 반지름) + L + (2번 트리의 반지름)$
3. 2번 트리 - L - L - 3번 트리
   $(2번 트리의 반지름) + 2L + (3번 트리의 반지름)$

# 코드
```cpp
#include &lt;bits/stdc++.h&gt;

#define endl &quot;\n&quot;
#define all(v) (v).begin(), (v).end()
#define all1(v) (v).begin()+1, (v).end()
#define For(i, a, b) for(int i=(a); i&lt;(b); i++)
#define FOR(i, a, b) for(int i=(a); i&lt;=(b); i++)
#define Bor(i, a, b) for(int i=(a)-1; i&gt;=(b); i--)
#define BOR(i, a, b) for(int i=(a); i&gt;=(b); i--)
#define ft first
#define sd second

using namespace std;
using ll = long long;
using lll = __int128_t;
using ulll = __uint128_t;
using ull = unsigned long long;
using ld = long double;
using pii = pair&lt;int, int&gt;;
using pll = pair&lt;ll, ll&gt;;
using ti3 = tuple&lt;int, int, int&gt;;
using tl3 = tuple&lt;ll, ll, ll&gt;;

template&lt;typename T&gt; using ve = vector&lt;T&gt;;
template&lt;typename T&gt; using vve = vector&lt;vector&lt;T&gt;&gt;;

template&lt;class T&gt; bool ckmin(T&amp; a, const T&amp; b) { return b &lt; a ? a = b, 1 : 0; }
template&lt;class T&gt; bool ckmax(T&amp; a, const T&amp; b) { return a &lt; b ? a = b, 1 : 0; }

const int INF = 987654321;
const int INF0 = numeric_limits&lt;int&gt;::max();
const ll LNF = 987654321987654321;
const ll LNF0 = numeric_limits&lt;ll&gt;::max();

ll n, m, l;
vve&lt;pll&gt; g;
ve&lt;bool&gt; vis;

ll U, D;
void dfs(ll p, ll u, ll d) {
    vis[u] = true;
    if(D &lt; d) {
        U = u;
        D = d;
    }
    for(auto [v,w] : g[u]) {
        if(v == p) continue;
        dfs(u, v, d+w);
    }
}
ve&lt;pll&gt; route;
bool path(ll p, ll u, ll dst) {
    if(u == dst) {
        return true;
    }
    for(auto [v,w] : g[u]) {
        if(v == p) continue;
        route.emplace_back(v, route.back().sd + w);
        if(path(u,v,dst)) return true;
        route.pop_back();
    }
    return false;
}
bool cmp(ll a, ll b) { return a &gt; b; }

void solve() {
    cin &gt;&gt; n &gt;&gt; m &gt;&gt; l;
    g = vve&lt;pll&gt;(n+1);
    vis = ve&lt;bool&gt;(n+1, false);
    while(m--) {
        ll a, b, t; cin &gt;&gt; a &gt;&gt; b &gt;&gt; t;
        g[a].emplace_back(b,t);
        g[b].emplace_back(a,t);
    }

    ll ans = 0;
    ve&lt;ll&gt; maxs;
    for(int i=0; i&lt;n; i++) {
        if(vis[i]) continue;

        D = -1;
        dfs(i,i,0);
        ll A = U;

        D = -1;
        dfs(A,A,0);
        ll B = U;
        ll AB_dist = D;

        ckmax(ans, AB_dist);

        route.clear();
        route.emplace_back(A,0);
        path(A,A,B);

        int j=0;
        while(j &lt; route.size()-1) {
            if(max(route[j].sd, AB_dist-route[j].sd) &lt; max(route[j+1].sd, AB_dist-route[j+1].sd)) break;
            j++;
        }
        maxs.emplace_back(max(route[j].sd, AB_dist-route[j].sd));
    }
    sort(all(maxs), cmp);
    if(maxs.size() &gt;= 2) ckmax(ans, maxs[0] + maxs[1] + l);
    if(maxs.size() &gt; 2) ckmax(ans, maxs[1] + maxs[2] + l*2);

    cout &lt;&lt; ans &lt;&lt; endl;
}

int main(void) {
    ios_base::sync_with_stdio(false);
    cin.tie(nullptr);
    cout.tie(nullptr);

    int TC=1;
//    cin &gt;&gt; TC;
    FOR(tc, 1, TC) {
//        cout &lt;&lt; &quot;Case #&quot; &lt;&lt; tc &lt;&lt; &quot;: &quot;;
        solve();
    }


    return 0;
}
```
</content:encoded></item><item><title><![CDATA[백준 6569 - 몬드리안의 꿈]]></title><link>https://jinsoolve.netlify.app/posts/boj-6569</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/boj-6569</guid><pubDate>Thu, 27 Feb 2025 15:00:00 GMT</pubDate><content:encoded>위 포스트는 [백준 6569 - 몬드리안의 꿈](https://www.acmicpc.net/problem/6569)에 대한 해설입니다.


# 풀이

| ... | ... | ... | ... |  ...  | ... | ... | ... |
| :-: | :-: | :-: | :-: | :---: | :-: | :-: | :-: |
| 채워짐 | 채워짐 | 채워짐 | 채워짐 |  채워짐  | 채워짐 | 채워짐 | 채워짐 |
| 채워짐 | 채워짐 | 채워짐 | 채워짐 |  w-1  | ... | ... | ... |
| ... |  2  |  1  |  0  | (h,w) |     |     |     |


위와 같이 큰 직사각형이 존재한다고 하자.
`dp[h][w][bit] := bit상태일 때, (h,w)부터 끝까지 큰 직사각형을 2x1 직사각형으로 채우는 경우의 수`라 하자. (이때 (0,0) ~ (h-1,w-1) 영역은 모두 채워져 있음을 가정한다.)

여기서 bit는 w개의 bit로 이루어져 있는데, 이는  &lt;u&gt;(w-1) (w-2) ... (2) (1) (0)&lt;/u&gt; 을 의미한다. 
각 비트는 채워진 여부를 의미한다. 0이면 비어있고 1이면 채워져 있다.


이때 다음과 같이 풀면 된다.

1. (h-1,w)가 빈 경우,
   이 경우 (h,w)에서 세로로 세워진 2x1 직사각형을 놓치 않으면 (h-1,w)을 채울 방법이 앞으로 더 이상 없으므로 무조건 채워주어야 하므로 무조건 세로로 직사각형을 놓는다.
2. Else
	아래 1,2 경우를 더 해준다.
	1. 현재 칸에 아무런 직사각형을 놓치 않는다.
	2. (h,w-1)가 빈 경우, 눕혀진 직사각형 1x2 직사각형을 놓는다.
3. 끝까지 도달한 경우 마지막 행이 채워졌는지의 여부를 갖고 있는 bit가 모두 1로 이루어져 있는지 확인하여 모두 1이라면 1을, 아니라면 0을 반환해준다.
# 코드
```cpp
#include &lt;bits/stdc++.h&gt;

#define endl &quot;\n&quot;
#define all(v) (v).begin(), (v).end()
#define all1(v) (v).begin()+1, (v).end()
#define For(i, a, b) for(int i=(a); i&lt;(b); i++)
#define FOR(i, a, b) for(int i=(a); i&lt;=(b); i++)
#define Bor(i, a, b) for(int i=(a)-1; i&gt;=(b); i--)
#define BOR(i, a, b) for(int i=(a); i&gt;=(b); i--)
#define ft first
#define sd second

using namespace std;
using ll = long long;
using lll = __int128_t;
using ulll = __uint128_t;
using ull = unsigned long long;
using ld = long double;
using pii = pair&lt;int, int&gt;;
using pll = pair&lt;ll, ll&gt;;
using ti3 = tuple&lt;int, int, int&gt;;
using tl3 = tuple&lt;ll, ll, ll&gt;;

template&lt;typename T&gt; using ve = vector&lt;T&gt;;
template&lt;typename T&gt; using vve = vector&lt;vector&lt;T&gt;&gt;;

template&lt;class T&gt; bool ckmin(T&amp; a, const T&amp; b) { return b &lt; a ? a = b, 1 : 0; }
template&lt;class T&gt; bool ckmax(T&amp; a, const T&amp; b) { return a &lt; b ? a = b, 1 : 0; }

const int INF = 987654321;
const int INF0 = numeric_limits&lt;int&gt;::max();
const ll LNF = 987654321987654321;
const ll LNF0 = numeric_limits&lt;ll&gt;::max();

int H, W;
ll dp[11][11][1&lt;&lt;11];

ll sol(int h, int w, int bit) {
    if(h==H) return (bit == ((1&lt;&lt;W)-1));
    ll &amp;ret = dp[h][w][bit];
    if(ret != -1) return ret;
    int nh = h, nw = w+1;
    if(nw == W) nh++, nw=0;
    if(h!=0 and (bit &amp; (1&lt;&lt;(W-1))) == 0) return ret = sol(nh,nw,(bit*2+1)%(1&lt;&lt;W));
    ret = sol(nh,nw,(bit*2)%(1&lt;&lt;W));
    if(w!=0 and (bit &amp; 1) == 0) ret += sol(nh,nw,((bit+1)*2+1)%(1&lt;&lt;W));
    return ret;
}

int main(void) {
    ios_base::sync_with_stdio(false);
    cin.tie(nullptr);
    cout.tie(nullptr);

    while(true) {
        cin &gt;&gt; H &gt;&gt; W;
        if(H==0 and W==0) break;
        memset(dp, -1, sizeof dp);
        cout &lt;&lt; sol(0,0,0) &lt;&lt; endl;
    }


    return 0;
}
```</content:encoded></item><item><title><![CDATA[백준 24979 - COW Operations]]></title><link>https://jinsoolve.netlify.app/posts/boj-24979</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/boj-24979</guid><pubDate>Thu, 27 Feb 2025 15:00:00 GMT</pubDate><content:encoded>위 포스트는 [백준 24979 - COW Operations](https://www.acmicpc.net/problem/24979)에 대한 해설입니다.

# 핵심 아이디어

## 아이디어1
주어진 Operation을 해보면 아래와 같은 변환이 가능하다는 것을 알 수 있다.
- OW, WO ↔ C
- WC. CW ↔ O
- CO, OC ↔ W

양쪽으로 자유롭게 왔다갔다할 수 있다.

## 아이디어2
문자의 순서를 바꿀 수 있다. 
- CO ↔ OC
- WC ↔ CW
- WO ↔ OW

주어진 Operation을 해보면 문자의 순서를 바꿀 수 있다는 사실을 알 수 있다.

## 아이디어3
C 하나를 남겨야 하므로 O나 W 둘 중 하나를 아예 제거해보자.
W를 제거한다고 가정하면 아이디어1에 의해 W를 전부 CO나 OC로 바꿀 수 있다.
결과적으로 C와 O로 이루어진 문자열을 얻을 수 있다.

# 풀이
아이디어2와 아이디어3에 의해서 문자열을 CC...CCOO...O 꼴로 만들 수 있다.
이때 C가 홀수개이고 O가 짝수개라면 우리는 C를 만들 수 있다는 것을 알 수 있다.

C와 O의 개수는 각각 누적합을 이용해서 세어준다.


# 코드
```cpp
#include &lt;bits/stdc++.h&gt;

#define endl &quot;\n&quot;
#define all(v) (v).begin(), (v).end()
#define all1(v) (v).begin()+1, (v).end()
#define For(i, a, b) for(int i=(a); i&lt;(b); i++)
#define FOR(i, a, b) for(int i=(a); i&lt;=(b); i++)
#define Bor(i, a, b) for(int i=(a)-1; i&gt;=(b); i--)
#define BOR(i, a, b) for(int i=(a); i&gt;=(b); i--)
#define ft first
#define sd second

using namespace std;
using ll = long long;
using lll = __int128_t;
using ulll = __uint128_t;
using ull = unsigned long long;
using ld = long double;
using pii = pair&lt;int, int&gt;;
using pll = pair&lt;ll, ll&gt;;
using ti3 = tuple&lt;int, int, int&gt;;
using tl3 = tuple&lt;ll, ll, ll&gt;;

template&lt;typename T&gt; using ve = vector&lt;T&gt;;
template&lt;typename T&gt; using vve = vector&lt;vector&lt;T&gt;&gt;;

template&lt;class T&gt; bool ckmin(T&amp; a, const T&amp; b) { return b &lt; a ? a = b, 1 : 0; }
template&lt;class T&gt; bool ckmax(T&amp; a, const T&amp; b) { return a &lt; b ? a = b, 1 : 0; }

const int INF = 987654321;
const int INF0 = numeric_limits&lt;int&gt;::max();
const ll LNF = 987654321987654321;
const ll LNF0 = numeric_limits&lt;ll&gt;::max();

void solve() {
    string s; cin &gt;&gt; s;
    ve&lt;ll&gt; csum(s.length()+1, 0), osum(s.length()+1, 0);
    for(int i=1; i&lt;=s.length(); i++) {
        char c = s[i-1];
        csum[i] = csum[i-1] + (c == &apos;O&apos; ? 0:1);
        osum[i] = osum[i-1] + (c == &apos;C&apos; ? 0:1);
    }

    int q; cin &gt;&gt; q;
    string ans;
    while(q--) {
        int l, r; cin &gt;&gt; l &gt;&gt; r;
        ll cval = csum[r] - csum[l-1];
        ll oval = osum[r] - osum[l-1];
        ans += ((cval%2 == 1 and oval%2 == 0) ? &quot;Y&quot; : &quot;N&quot;);
    }
    cout &lt;&lt; ans &lt;&lt; endl;
}

int main(void) {
    ios_base::sync_with_stdio(false);
    cin.tie(nullptr);
    cout.tie(nullptr);
    
    int TC=1;
//    cin &gt;&gt; TC;
    FOR(tc, 1, TC) {
//        cout &lt;&lt; &quot;Case #&quot; &lt;&lt; tc &lt;&lt; &quot;: &quot;;
        solve();
    }


    return 0;
}
```

# 참고
https://dong-gas.tistory.com/76</content:encoded></item><item><title><![CDATA[백준 1646 - 피이보나치 트리]]></title><link>https://jinsoolve.netlify.app/posts/boj-1646</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/boj-1646</guid><pubDate>Tue, 25 Feb 2025 15:00:00 GMT</pubDate><content:encoded>위 포스트는 [백준 1646 - 피이보나치 트리](https://www.acmicpc.net/problem/1646) 문제에 대한 해설입니다.

# 관찰
## 관찰1
n번 피이보나치 트리의 루트의 왼쪽 서브트리는 **n-2번 째 피이보나치 트리**이고, 오른쪽 서브트리는 **n-1번 째 피이보나치 트리**이다.

그리고 번호는 pre-order로 정해지므로 
- root가 1번이면,
- 2번 부터 Size(n-2 피이보나치 트리)+1번까지 n-2번 째 피이보나치 트리가 차지하고
- n-1번째 피이보나치 서브트리가 Size(n-2 피이보나치 트리)+2 번부터를 차지한다.

## 관찰2
n번째 피이보나치 트리에서 시작점 from과 도착점 to를 찾아서 이동 경로를 찾아야 하는데 이걸 위에서 재귀적으로 찾아들어간다고 생각해보자.

총 3가지 경우가 생길 수 있다.
1. from과 to 둘 중 하나가 현재 트리의 root일 때,
2. from과 to가 서로 같은 서브트리(왼쪽 혹은 오른쪽)에 속할 때
3. from과 to가 다른 서브트리에 속할 때

위 3가지 경우에 대해서 재귀적으로 해결해주면 된다.

## 관찰3
**n번째 피이보아치 트리에서 root에서 dst 번호의 노드까지의 이동경로를 구하는 재귀함수**를 구하면 매우 편하다.

먼저 dst번호가 1이라면 현재 위치이므로 **\&quot;\&quot;** 을 return 해준다.

관찰1에서 말한 것처럼 dst 노드가 왼쪽 서브트리인지 오른쪽 서브트리인지 알기 위해서는 pre-order 번호로 판단할 수 있다.
- 왼쪽 서브트리라면 n-2번째 피이보나치 트리에서 dst-1번 노드를 찾으면 된다.
- 반대로 오른쪽 서브트리라면, n-1번째 피이보나치 트리에서 dst - (Size(n-2번 피이보나치 트리)+1) 번 노드를 찾으면 된다.

이를 코드로 정리해보면 다음과 같다.

```cpp
string go(int n, ll dst) {
    if(dst == 1) return &quot;&quot;;
    if(dst &lt;= fibo(n-2)+1) return &quot;L&quot; + go(n-2, dst-1);
    else return &quot;R&quot; + go(n-1, dst - (fibo(n-2)+1));
}
```


# 풀이
관찰2에서 말한 것처럼, 재귀적으로 들어가면서 해결하면 된다.

1. from과 to 둘 중 하나가 현재 트리의 root일 때,
	1. from이 현재 root일 때, go(n,to)을 그냥 반환하면 된다.
	2. to가 현재 root일 때, go(n,from)에서 해당 길이만큼 올라오기만 하면 되므로 해당 길이의 &quot;U&quot;를 반복시킨 문자열을 반환.
2. from과 to가 서로 같은 서브트리(왼쪽 혹은 오른쪽)에 속할 때, 
   속해 있는 서브트리 쪽으로 재귀적으로 이동한다.
3. from과 to가 다른 서브트리에 속할 때
	1. go(n,from) 길이만큼의 &quot;U&quot;를 반복시키고
	2. go(n,to)을 그 뒤에다가 붙여주면 그것이 정답이 된다.


# 코드
위 내용을 코드로 작성해보면 아래와 같다.

```cpp
#include &lt;bits/stdc++.h&gt;

#define endl &quot;\n&quot;
#define all(v) (v).begin(), (v).end()
#define all1(v) (v).begin()+1, (v).end()
#define For(i, a, b) for(int i=(a); i&lt;(b); i++)
#define FOR(i, a, b) for(int i=(a); i&lt;=(b); i++)
#define Bor(i, a, b) for(int i=(a)-1; i&gt;=(b); i--)
#define BOR(i, a, b) for(int i=(a); i&gt;=(b); i--)
#define ft first
#define sd second

using namespace std;
using ll = long long;
using lll = __int128_t;
using ulll = __uint128_t;
using ull = unsigned long long;
using ld = long double;
using pii = pair&lt;int, int&gt;;
using pll = pair&lt;ll, ll&gt;;
using ti3 = tuple&lt;int, int, int&gt;;
using tl3 = tuple&lt;ll, ll, ll&gt;;

template&lt;typename T&gt; using ve = vector&lt;T&gt;;
template&lt;typename T&gt; using vve = vector&lt;vector&lt;T&gt;&gt;;

template&lt;class T&gt; bool ckmin(T&amp; a, const T&amp; b) { return b &lt; a ? a = b, 1 : 0; }
template&lt;class T&gt; bool ckmax(T&amp; a, const T&amp; b) { return a &lt; b ? a = b, 1 : 0; }

const int INF = 987654321;
const int INF0 = numeric_limits&lt;int&gt;::max();
const ll LNF = 987654321987654321;
const ll LNF0 = numeric_limits&lt;ll&gt;::max();

ll fibo_cache[51];

ll fibo(int n) {
    if(n==0 or n==1) return 1;
    ll &amp;ret = fibo_cache[n];
    if(ret != -1) return ret;
    return ret = 1 + fibo(n-2) + fibo(n-1);
}
string go(int n, ll dst) {
    if(dst == 1) return &quot;&quot;;
    if(dst &lt;= fibo(n-2)+1) return &quot;L&quot; + go(n-2, dst-1);
    else return &quot;R&quot; + go(n-1, dst - (fibo(n-2)+1));
}
string sol(int n, ll from, ll to) {
    // from, to 둘 중 하나가 root일 때
    if(from == 1) return go(n,to);
    if(to == 1) {
        string res = go(n, from);
        return string(res.length(), &apos;U&apos;);
    }

    // from, to가 같은 쪽일 때
    if(from &lt;= fibo(n-2)+1 and to &lt;= fibo(n-2)+1) {
        return sol(n-2, from-1, to-1);
    }
    if(from &gt; fibo(n-2)+1 and to &gt; fibo(n-2)+1) {
        return sol(n-1, from - (fibo(n-2)+1), to - (fibo(n-2)+1));
    }

    // from, to가 다른 쪽일 때
    string res = go(n,from);
    string from_to_root = string(res.length(), &apos;U&apos;);
    string root_to_to = go(n,to);
    return from_to_root + root_to_to;
}

void solve() {
    int n; cin &gt;&gt; n;
    ll from, to; cin &gt;&gt; from &gt;&gt; to;
    cout &lt;&lt; sol(n,from,to) &lt;&lt; endl;
}

int main(void) {
    ios_base::sync_with_stdio(false);
    cin.tie(nullptr);
    cout.tie(nullptr);

    memset(fibo_cache, -1, sizeof fibo_cache);

    int TC=1;
//    cin &gt;&gt; TC;
    FOR(tc, 1, TC) {
//        cout &lt;&lt; &quot;Case #&quot; &lt;&lt; tc &lt;&lt; &quot;: &quot;;
        solve();
    }


    return 0;
}
```</content:encoded></item><item><title><![CDATA[삼격형의 두 변의 길이와 사잇각을 알 때, 나머지 한 변의 길이를 구하는 공식]]></title><link>https://jinsoolve.netlify.app/posts/formula-for-the-third-side-of-a-triangle-given-two-sides-and-the-included-angle</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/formula-for-the-third-side-of-a-triangle-given-two-sides-and-the-included-angle</guid><pubDate>Wed, 19 Feb 2025 15:00:00 GMT</pubDate><content:encoded>
![](https://i.imgur.com/SJCydEl.png)


위와 같은 삼각형이 존재할 때, 변의 길이 b, c와 그 사잇각 $\alpha$를 알고 있다고 가정하자. 이때 a의 길이를 구하는 공식은 다음과 같다.

$$
a = \sqrt{ b^2 + c^2 - 2bc\cos \alpha }
$$

# 증명
위와 같이 되는 이유는 아래 그림과 같은 삼각형이 존재한다고 가정할 떄, 선분 AC의 길이는 다음과 같다고 할 수 있다.
![](https://i.imgur.com/aBKANhX.png)

$$
\\[5pt]
\sqrt{ (c \times \sin B)^2 + (a - c \cos B)^2 } \\[5pt]
= \sqrt{ (c \sin B)^2 + a^2 -2ac \cos B + (c \cos B)^2 } \\[5pt]
= \sqrt{ a^2 + c^2(\sin^2 B + \cos^2 B) - 2ac \cos B } \\[5pt]
= \sqrt{ a^2 + c^2 - 2ac \cos B }
$$
위와 같이 유도할 수 있다.</content:encoded></item><item><title><![CDATA[백준 1155 - 변형 하노이]]></title><link>https://jinsoolve.netlify.app/posts/boj-1155</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/boj-1155</guid><pubDate>Tue, 18 Feb 2025 15:00:00 GMT</pubDate><content:encoded>위 포스트는 [백준 1155 - 변형 하노이](https://www.acmicpc.net/problem/1155) 문제의 해설입니다.

# 핵심 아이디어
## 아이디어 1
A번 폴: n번 디스크
B번 폴: 1 ~ n-1 번 디스크
C번 폴: 빔

위 경우 **n번 디스크는 우선순위와 상관없이 무조건 C번 폴로만 이동 가능**하다.

## 아이디어 2
문제의 조건에 있는 _&quot;동일한 디스크를 연속으로 두 번 옮길 수 없다.&quot;_ 라는 말로 인해서,
1 ~ n-1 번 디스크 덩어리를 A번 폴에서 B번 폴로 옮긴 다음 또 옮기는 시도를 할 수 없다.

왜냐하면 1 ~ n-1 번 디스크 덩어리를 옮기려면 어떻게 움직이든 가장 마지막으로 움직이는 디스크는 1번 디스크이다. 그런데 해당 디스크 덩어리를 다시 이동시키려면 가장 먼저 움직여야 하는 디스크는 1번 디스크이다. 따라서 **어떤 디스크 덩어리를 이미 옮겼을 때 연속으로 또 옮기는 시도는 할 수 없다.**

# 풀이
우리는 이 문제를 재귀적으로 해결할 수 있다.
n개의 디스크를 옮긴다고 할 때 n번 디스크와 1 ~ n-1 번 디스크 덩어리를 나눠서 생각하자.
이때, 1 ~ n-1 번 디스크 덩어리는 재귀적으로 해결한다고 가정하자.
## 해설
### n 개의 디스크를 옮기는 법
폴이 from, via, to 가 있다고 가정하자.
또한 1 ~ n 번 디스크 덩어리를 from에서 to로 옮기고 싶다고 하자.

이때 1 ~ n-1 번 디스크 덩어리와 n번 디스크로 나눠서 생각하자. 위 n개의 디스크 덩어리를 from에서 to로 옮기는 방법은 2가지만 존재한다. 
**1 ~ n-1 번 디스크 덩어리를 [from → via 로 먼저 보내는 방법]과, [from → to 로 보내는 방법] 이렇게 2가지만 존재**한다.

#### CASE 1: from → via
먼저, 가장 간단한 방법이다.
1. 1 ~ n-1 번 디스크 덩어리: from → via 
2. n 번 디스크: from → to (by 핵심 아이디어1)
3. 1 ~ n-1 번 디스크 덩어리: via → to

#### CASE 2: from → to
그 다음 방법이다.
1. 1 ~ n-1 번 디스크 덩어리: from → to
2. n 번 디스크: from → via (by 핵심 아이디어1)
3. 1 ~ n-1 번 디스크 덩어리: to → from
4. n 번 디스크: via → to
5. 1 ~ n-1 번 디스크 덩어리: from → to



위 CASE 1, CASE 2 모두 1 ~ n-1 번 디스크 덩어리와 n 번 디스크가 서로 번갈아가면서 옮기고 있는 모습을 확인할 수 있다.(by 핵심아이디어2)

### 우선순위는 그럼 어떻게 적용할까?

그리고 우선순위 같은 경우, 재귀적으로 생각해보면 여러 개의 디스크를 합친 덩어리들은 재귀적으로 해결이 된다고 가정하면, 결국 1개의 디스크를 옮길 때만 적용한다고 생각하면 편하다.
한 개를 옮길 때 문제에서 주어진 우선순위에 따라서&quot;만&quot; 옮길 수 있다.

`hanoi[n][from][to] := from에 위치한 n개의 디스크들을 규칙에 알맞게 to로 옮길 때 필요한 횟수`

그러면 n = 1일 때, hanoi\[1\]\[from\]\[to\] 에서 각 from은 오로지 1개의 to로만 이동할 수 있다.
예를 들어서 from → A번 폴, from → B번 폴 2개 다 존재할 수 없다는 의미이다. 둘 중 우선순위가 낮은 폴로는 갈 수 없다. 
이를 이용해서 초기값을 정한 후 위 hanoi 배열을 채워나가고 n개의 디스크를 옮겼을 때의 결과를 내보낸다.

이떄 **옮겨야 할 디스크의 개수와 이동 우선 순위가 주어진다면 이동 순서와 횟수는 결정**되기 때문에 hanoi[n][0][1]과 hanoi[n][0][2] 중 0이 아닌 값이 정답이 된다. (물론 0,1,2 번 폴이 존재한다고 가정했을 때의 이야기다.)


## 코드

```cpp
#include &lt;bits/stdc++.h&gt;

#define endl &quot;\n&quot;
#define all(v) (v).begin(), (v).end()
#define all1(v) (v).begin()+1, (v).end()
#define For(i, a, b) for(int i=(a); i&lt;(b); i++)
#define FOR(i, a, b) for(int i=(a); i&lt;=(b); i++)
#define Bor(i, a, b) for(int i=(a)-1; i&gt;=(b); i--)
#define BOR(i, a, b) for(int i=(a); i&gt;=(b); i--)
#define ft first
#define sd second

using namespace std;
using ll = long long;
using lll = __int128_t;
using ulll = __uint128_t;
using ull = unsigned long long;
using ld = long double;
using pii = pair&lt;int, int&gt;;
using pll = pair&lt;ll, ll&gt;;
using ti3 = tuple&lt;int, int, int&gt;;
using tl3 = tuple&lt;ll, ll, ll&gt;;

template&lt;typename T&gt; using ve = vector&lt;T&gt;;
template&lt;typename T&gt; using vve = vector&lt;vector&lt;T&gt;&gt;;

template&lt;class T&gt; bool ckmin(T&amp; a, const T&amp; b) { return b &lt; a ? a = b, 1 : 0; }
template&lt;class T&gt; bool ckmax(T&amp; a, const T&amp; b) { return a &lt; b ? a = b, 1 : 0; }

const int INF = 987654321;
const int INF0 = numeric_limits&lt;int&gt;::max();
const ll LNF = 987654321987654321;
const ll LNF0 = numeric_limits&lt;ll&gt;::max();

void solve() {
    int n;
    ll hanoi[31][3][3]; memset(hanoi, 0, sizeof hanoi);

    cin &gt;&gt; n;
    for(int i=0; i&lt;6; i++) {
        string order; cin &gt;&gt; order;
        int f = order.front()-&apos;A&apos;, t = order.back()-&apos;A&apos;;
        if(!hanoi[1][f][0] and !hanoi[1][f][1] and !hanoi[1][f][2]) hanoi[1][f][t] = 1;
    }

    for(int i=2; i&lt;=n; i++) {
        for(int f=0; f&lt;3; f++) {
            for(int t=0; t&lt;3; t++) {
                int v = 3-f-t;
                if(hanoi[i-1][f][v] and hanoi[i-1][v][t]) hanoi[i][f][t] = hanoi[i-1][f][v] + hanoi[i-1][v][t] + 1;
                else if(hanoi[i-1][f][t] and hanoi[i-1][t][f]) hanoi[i][f][t] = hanoi[i-1][f][t]*2LL + hanoi[i-1][t][f] + 2;
            }
        }
    }

    cout &lt;&lt; (hanoi[n][0][1] ? hanoi[n][0][1] : hanoi[n][0][2]) &lt;&lt; endl;
}

int main(void) {
    ios_base::sync_with_stdio(false);
    cin.tie(nullptr);
    cout.tie(nullptr);

    int TC=1;
//    cin &gt;&gt; TC;
    FOR(tc, 1, TC) {
//        cout &lt;&lt; &quot;Case #&quot; &lt;&lt; tc &lt;&lt; &quot;: &quot;;
        solve();
    }


    return 0;
}
```

# 참고
https://velog.io/@frog_slayer/BOJ-1155-변형-하노이</content:encoded></item><item><title><![CDATA[백준 25315 - N수매화검법]]></title><link>https://jinsoolve.netlify.app/posts/boj-25315</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/boj-25315</guid><pubDate>Mon, 17 Feb 2025 15:00:00 GMT</pubDate><content:encoded>[백준 25315번 - N수매화검법](https://www.acmicpc.net/problem/25315) 문제의 해설입니다.

# 핵심 아이디어
서로 교차하는 베기 A와 베기 B가 있을 때, 두 베기가 사라지려면 딱 2가지 경우이다.
$W_{A} \times (m+1) + W_{B} \times (m)$ 아니면 $W_{A} \times (m) + W_{B} \times (m+1)$이다.
결국 $W_{A}$ 가 더해지느냐, 아니면 $W_{B}$가 더해지느냐의 차이다. 따라서 무조건 적은 값이 더해지는 게 이득이라는 사실을 알 수 있다.

즉, **모든 두 베기의 교차마다 두 베기 중 작은 가중치를 더해주면 되고, 모든 베기의 가중치를 전부 더해주면 그것이 정답**이다.


# 풀이
모든 베기가 $N(\leq 2500)$ 이므로 $N^2$ 만에 모든 베기의 쌍에 대해서 교차하는 지의 여부와 교차한다면 두 베기 중 가중치가 작은 걸 더해준다. 그리고 모든 베기의 가중치를 전부 더해준다.

선분 교차 판정은 CCW를 기준으로 판단한다. ([선분 교차 판정 참고](https://killerwhale0917.tistory.com/6))

## 코드
```cpp
#include &lt;bits/stdc++.h&gt;

#define endl &quot;\n&quot;
#define all(v) (v).begin(), (v).end()
#define all1(v) (v).begin()+1, (v).end()
#define For(i, a, b) for(int i=(a); i&lt;(b); i++)
#define FOR(i, a, b) for(int i=(a); i&lt;=(b); i++)
#define Bor(i, a, b) for(int i=(a)-1; i&gt;=(b); i--)
#define BOR(i, a, b) for(int i=(a); i&gt;=(b); i--)
#define ft first
#define sd second

using namespace std;
using ll = long long;
using lll = __int128_t;
using ulll = __uint128_t;
using ull = unsigned long long;
using ld = long double;
using pii = pair&lt;int, int&gt;;
using pll = pair&lt;ll, ll&gt;;
using ti3 = tuple&lt;int, int, int&gt;;
using tl3 = tuple&lt;ll, ll, ll&gt;;

template&lt;typename T&gt; using ve = vector&lt;T&gt;;
template&lt;typename T&gt; using vve = vector&lt;vector&lt;T&gt;&gt;;

template&lt;class T&gt; bool ckmin(T&amp; a, const T&amp; b) { return b &lt; a ? a = b, 1 : 0; }
template&lt;class T&gt; bool ckmax(T&amp; a, const T&amp; b) { return a &lt; b ? a = b, 1 : 0; }

const int INF = 987654321;
const int INF0 = numeric_limits&lt;int&gt;::max();
const ll LNF = 987654321987654321;
const ll LNF0 = numeric_limits&lt;ll&gt;::max();

struct Point {
    ll x, y;

    Point() {}
    Point(ll _x, ll _y) : x(_x), y(_y) {}
    ll cross(Point other) {
        return x*other.y - y*other.x;
    }
    ll crossSign(Point other) {
        ll res = this-&gt;cross(other);
        if(res &gt; 0) return 1;
        else if(res &lt; 0) return -1;
        return 0;
    }
    ll dist(Point other) {
        return pow(x-other.x,2) + pow(y-other.y,2);
    }
    Point operator-(Point other) const {
        return Point(x-other.x, y-other.y);
    }
    bool operator&lt;(Point other) const {
        if(x == other.x) return y &lt; other.y;
        return x &lt; other.x;
    }
    void print() {
        cout &lt;&lt; x &lt;&lt; &apos; &apos; &lt;&lt; y &lt;&lt; &apos; &apos;;
    }
};

bool isIntersect(Point p1, Point p2, Point p3, Point p4) {
    ll p1p2 = (p2-p1).crossSign(p3-p2) * (p2-p1).crossSign(p4-p2); // 선분 p1p2 기준
    ll p3p4 = (p4-p3).crossSign(p1-p4) * (p4-p3).crossSign(p2-p4); // 선분 p3p4 기준

    // 두 직선이 일직선 상에 존재
    if(p1p2 == 0 &amp;&amp; p3p4 == 0) {
        if(p2 &lt; p1) swap(p1,p2);
        if(p4 &lt; p3) swap(p3,p4);
        return p3 &lt; p2 &amp;&amp; p1 &lt; p4;
    }
    return p1p2 &lt;= 0 &amp;&amp; p3p4 &lt;= 0;
}

struct Cut {
    ll sx, sy, ex, ey, w;
};

int N;
ll ans = 0;

void solve() {
    cin &gt;&gt; N;
    ve&lt;Cut&gt; v(N);
    for(int i=0; i&lt;N; i++) {
        cin &gt;&gt; v[i].sx &gt;&gt; v[i].sy &gt;&gt; v[i].ex &gt;&gt; v[i].ey &gt;&gt; v[i].w;
        ans += v[i].w;
    }

    for(int i=0; i&lt;N; i++) {
        for(int j=i+1; j&lt;N; j++) {
            if(!isIntersect({v[i].sx,v[i].sy}, {v[i].ex,v[i].ey}, {v[j].sx,v[j].sy}, {v[j].ex,v[j].ey})) continue;
            ans += min(v[i].w, v[j].w);
        }
    }

    cout &lt;&lt; ans &lt;&lt; endl;
}

int main(void) {
    ios_base::sync_with_stdio(false);
    cin.tie(nullptr);
    cout.tie(nullptr);

    int TC=1;
//    cin &gt;&gt; TC;
    FOR(tc, 1, TC) {
//        cout &lt;&lt; &quot;Case #&quot; &lt;&lt; tc &lt;&lt; &quot;: &quot;;
        solve();
    }


    return 0;
}
```</content:encoded></item><item><title><![CDATA[c++에서 내림차순에 대한 lower_bound와 upper_bound하기]]></title><link>https://jinsoolve.netlify.app/posts/descending-order-binary-search</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/descending-order-binary-search</guid><pubDate>Sun, 09 Feb 2025 15:00:00 GMT</pubDate><content:encoded>내림차순 정렬이 되어있을 때의 lower_bound와 upper_bound 사용법

```cpp
lower_bound(v.begin(), v.end(), num, greater&lt;int&gt;());
```
- `lower_bound` := `num`과 같거나 작은 원소들 중 첫번째 원소
- `upper_bound` := `num`보다 작은 원소들 중 첫번째 원소


</content:encoded></item><item><title><![CDATA[PoisonedRAG 논문 리뷰]]></title><description><![CDATA[PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generationof Large Language Models 에 관한 논문 리뷰]]></description><link>https://jinsoolve.netlify.app/posts/PoisonedRAG</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/PoisonedRAG</guid><pubDate>Mon, 13 Jan 2025 15:00:00 GMT</pubDate><content:encoded>위 글은 [PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generationof Large Language Models](https://arxiv.org/pdf/2402.07867) 논문을 리뷰한 글이다.
&lt;Callout type=&quot;danger&quot;&gt;
이 글은 공부를 하면서 작성한 글이므로 잘못된 부분이 있을 수 있습니다.
&lt;/Callout&gt;

# Abstract
LLM은 정말 좋은 성능을 보였지만, 여전히 최신 지식 부족이나 Hallucination 같은 한계가 존재했는데 이를 해결하기 위해서 RAG(Retrieval Augmented Generation) 기법을 사용하였다.

RAG는 외부 지식 데이터베이스에서 검색한 결과를 기반으로 답변을 생성하는 기술이다. 지금까지 논문들은 이러한 RAG의 효율성 및 정확도를 높이기 위해 노력했지만 RAG의 보안적인 측면은 거의 탐구되지 않았고 위 논문은 RAG의 보안 측면을 탐구하는 논문이다.

RAG 시스템이 외부 지식 데이터베이스를 참고하여 답변을 한다는 점을 이용하여 이 지식 데이터베이스에 소수의 악성 텍스트를 삽입하여 공격자가 하는 특정 질문(target question)에 대해 LLM이 공격자가 원하는 특정 답변(target answer)를 생성하도록 유도할 수 있음을 발견하였다. 이러한 RAG 시스템에 대한 지식 손상 공격(Knowledge Corruption Attack)은 **PoisonedRAG**이다.

위 논문은 이러한 지식 손상 공격을 최적화 문제로 보고 공식화시킨다. 이 최적화 문제의 답은 **malicious text(악성 텍스트)** 의 집합이다. 

RAG 시스템에 대해서 Attacker가 알고 있느냐 모르냐로 white-box와 black-box의 경우로 나눠서 2가지 최적화 문제를 제시하고 이를 해결하였다.
결과적으로 5개의 악성텍스트를 제공했을 때 거대한 지식 데이터베이스를 기반으로 RAG를 한 LLM에 대해 90%의 attack success의 결과를 얻을 수 있었다.

또한 위 공격들에 대해 몇 가지 방어기법을 적용해보고 그 결과를 살펴보았을 때, PoisonedRAG를 막기에는 부족하다는 것을 보여줄 수 있었다.
# 1 Introduction
Abstract에서 설명한 바와 같이 LLM은 hallucination을 극복하고 특정 도메인에서의 지식 차이나 최신 지식을 얻기 위해서 RAG 기법을 사용한다.
![](https://i.imgur.com/vcv6DhR.png)
RAG는 총 3가지로 구분할 수 있는데,

- Knowledge Database
- Retriever
- LLM

먼저 Knowledge Database(위키피디아, 기사 등에서 갖고옴)에서 Retriver를 통해서 관련있는 정보를 추출한 후에 LLM에게 context로써 제공한다.
그러면 LLM은 제공된 정보를 가지고 답변을 생성하게 된다.

이러다 보니 결국 LLM은 Knowledge Database라는 새로운 attack surface를 제공하게 되었고 이를 통해 공격받을 수 있게 되었다.

현재까지의 RAG에 관련된 논문은 이 RAG의 정확도나 효율성을 올리기 위한 작업들이었지만 보안적인 취약점을 위한 논문은 존재하지 않았고 위 논문은 보안적 취약점을 이야기하는 바이다.

#### Knowledge database as a new and practical attack surface
Attacker는 이러한 Knowledge Database를 공격한다.
예를 들면, 위키피디아의 내용에 malicious texts(악성 텍스트)를 넣는 방식으로 진행한다.
혹은, 가짜 뉴스를 포스팅한다던가 악성 웹사이트를 호스팅하여 인터넷을 통한 정보 수집을 공격하는 방식으로 해볼 수 있다.

#### Threat model
PoisonedRAG에서는 Target Question을 넣으면 Target Answer를 LLM이 생성하게 되도록 하는 것이 목표다. 말 그대로 LLM이 말도 안 되는 말을 하도록 하는 것이다.
이렇게 되면 LLM을 다양한 분야에 사용하기 어려움이 많이 생긴다.

위 논문에서는 knowledge database의 텍스트에 접근할 수 없고, LLM에 접근하거나 질문할 수 없음을 가정한다. 공격자는 retriever를 알 수도 있고, 모를 수도 있다. 이에 따라 2개의 세팅, **white-box setting**과 **black-box setting**으로 나눈다.

쉽게 얘기하면 knowledge database나 LLM은 아예 못 건드리고 retriever를 건드리냐, 건드리지 않느냐로 나눈다.


#### Overview of PoisonedRAG
PoisonedRAG에 대해서 가볍게 살펴보자.

위 논문에서는 악성 텍스트를 구성하는 것을 최적화 문제라 생각하고 가설을 공식화하였다.
하지마 이 문제를 해결하기는 쉽지 않고 따라서 [heuristic](https://ko.wikipedia.org/wiki/휴리스틱_이론)(경험적으로 느슨하게 그럴 거라 대충 생각한다? 정도의 의미)하게 생각하였을 때 **retrieval condition**과 **generation condition**으로 나눠서 생각하자.

**retrieval condition**이란, target question이 들어왔을 때 retriever가 악성텍스트를 잘 뽑아내는 지의 여부이다.
**generation condition**은 이러한 악성텍스트가 LLM이 target answer을 생성하게 하는 지를 의미한다.

위 논문에서는 **이 2 조건을 각각 성공시킬 수 있는 2개의 sub-text으로 악성텍스트를 decompose**한다. 그리고 이 2개의 **sub-text**를 합쳐서 동시에 2개의 조건을 성공시킬 수 있도록 하는 것이 주요한 아이디어이다.
#### Evaluation of PoisonedRAG
위 논문에서는 Attack Success Rate (ASR)을 평가 지표로 사용하고, 여기서 ASR은 target question에 대해서 target answer가 나왔는 지의 여부를 사용한다.

다음과 같은 결과를 얻을 수 있었다고 한다.
1. PoisonedRAG는 적은 양의 악성 텍스트로도 높은 ASR을 달성할 수 있었다.
2. SOTA(State-of-the-art) baseline(기본 모델) 즉, 현재 최고 수준의 성능을 보이는 기존방법에 대해서도 우수한 성능을 보였다.
3.  Ablation Studies(특정 요소의 중요성을 평가하기 위해 그 요소를 제거하나 변경하면서 성능 미치는 영향을 분석하는 방법) 결과로, 다양한 하이퍼-파라미터 설정에서도 안정적이고 일관된 성능을 보여준다.

#### Defending against PoisonedRAG
몇 가지 defenses 기법에 대해서도, PoisonedRAG를 적용하였을 때 이를 막기에는 역부족이었음을 확인할 수 있었다.
(여기서 말하는 defense 기법에는 paraphrasing과 perplexity-based detection이 있었다.)

&lt;Callout type=&quot;&quot;&gt;
**Paraphrasing**이란, 같은 내용을 다른 단어로 바꿔서 말하는 것.
주로 문자의 표현을 다양화하건, 더 간단하고 명확하게 전달하고자 할 때 사용됨.
&lt;/Callout&gt;

&lt;Callout type=&quot;&quot;&gt;
**Perplexity-based detection**이란, (쉽게 말하면) 
문장을 듣고 &quot;이건 정말 사람이 쓴 말 같다&quot; 또는 &quot;이거 뭔가 이상한데?&quot;라고 판단하는 모델의 직감 같은 것.

텍스트가 얼마나 예측 가능한지(혹은 자연스러운지)를 측정하는데 사용.
&lt;/Callout&gt;

위와 같은 Defense 기법에도 PoisonedRAG는 여전히 효과가 굉장했다!

# 2 Background and Related Work
추후에 계속...

## 2.1 Background on RAG

## 2.2 Existing Attacks to LLMs

## 2.3 Existing Data Poisoning Attacks




# 3 Problem Formulation

## 3.1 Threat Model

## 3.2 Knowledge Corruption Attack to RAG





# 4 Design of PoisonedRAG

## 4.1 Deriving Two Necessary Conditions for an Effective Knowledge Corruption Attack

## 4.2 Crafting Malicious Texts to Achieve the Two Derived Conditions
### 4.2.1 Crafting $I$ to Achieve Generation Condition
### 4.2.2 Crafting $S$ to Achieve Retrieval Condition



# 5 Evaluation
## 5.1 Experimental Setup
## 5.2 Main Results
## 5.3 Ablation Study
### 5.3.1 Impact of Hyperparameters in RAG
### 5.3.2 Impact of Hyperparameters in PoisonedRAG



# 6 Evaluation for Real-world Applications
## 6.1 Advanced RAG Schemes
## 6.2 Wikipedia-based ChatBot
## 6.3 LLM Agent



# 7 Defenses
## 7.1 Paraphrasing
## 7.2 Perplexity-based Detection
## 7.3 Duplicate Text Filtering
## 7.4 Knowledge Expansion



# 8 Discussion and Limitation


# 9 Conclusion and Future Work


</content:encoded></item><item><title><![CDATA[세 점의 좌표가 주어졌을 때 삼각형의 면적 구하는 방법]]></title><link>https://jinsoolve.netlify.app/posts/how-to-calculate-triangle-area</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/how-to-calculate-triangle-area</guid><pubDate>Sun, 12 Jan 2025 15:00:00 GMT</pubDate><content:encoded>![](https://i.imgur.com/QghzIIz.png)

세 점의 좌표가 주어졌을 때 어떻게 빠르게 삼각형의 너비를 계산할 수 있을까?
세 점 $A(x_1, y_1), B(x_2, y_2), C(x_3, y_3)$가 주어졌다고 가정할 때 다음과 같은 수식을 세울 수 있다.

신발끈 공식을 생각해보면 쉽게 풀린다
$$
\frac{1}{2}
\left|
\begin{array}{c} x_1 \\ y_1 \end{array}
\begin{array}{c} x_2 \\ y_2 \end{array}
\begin{array}{c} x_3 \\ y_3 \end{array}
\begin{array}{c} x_1 \\ y_1 \end{array}
\right|\\
= 
\frac{1}{2} 
\left| 
(x_1 \cdot y_2 + x_2 \cdot y_3 + x_3 \cdot y_1) - 
(x_2 \cdot y_1 + x_3 \cdot y_2 + x_1 \cdot y_3)
\right|
$$

# 코드
```cpp 
ll triangle(Point &amp;p1, Point &amp;p2, Point &amp;p3) {
    ll res = abs(p1.x*p2.y + p2.x*p3.y + p3.x*p1.y - p2.x*p1.y - p3.x*p2.y - p1.x*p3.y);
//    if(res % 2 == 0) return (ld)((ll)(res/2));
//    return (ld)((ll)(res/2) + 0.5);
    return res;
}
```

# 참고
https://m.blog.naver.com/eandimath/221760895905

</content:encoded></item><item><title><![CDATA[C++에서 이진수의 비트 수를 세는 방법]]></title><link>https://jinsoolve.netlify.app/posts/count-bit-in-cpp</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/count-bit-in-cpp</guid><pubDate>Sun, 12 Jan 2025 15:00:00 GMT</pubDate><content:encoded>$101101$ 이런 이진수가 주어질 때 C++에서 1의 개수를 세려면 어떻게 해야 할까?

```cpp
// unsigned int
__builtin_popcount();

// long long
 __builtin_popcountll();
```

그러나 c++20 이후부터는 아래 함수로 가능하는 듯 하다.
```cpp
popcount();
```


# 참고
https://codingdog.tistory.com/entry/c-20부터-적용된-bit-popcount-함수에-대해서-알아봅시다</content:encoded></item><item><title><![CDATA[2025년 겨울방학 계획]]></title><description><![CDATA[2024년 2학기에 대한 회고와 2025년의 겨울방학을 어떻게 보낼 지에 관하여...]]></description><link>https://jinsoolve.netlify.app/posts/2025-winter-vacation-plan</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/2025-winter-vacation-plan</guid><pubDate>Sun, 12 Jan 2025 15:00:00 GMT</pubDate><content:encoded># 서론
벌써 겨울방학이 시작된지 거의 3주가 지나버렸다.  
최대한 일찍 작성하려고 했는데, 종강하고 나서는 전공 18학점을 수강해야 했던 2학기에 대한 보상으로 놀고, 미뤄두었던 블로그 개편 작업을 진행하고 학부연구생을 준비하면서 벌써 3주가 삭제되어 버렸다.
그러나 지난 주 블로그를 gatsby로 호스팅하는 작업을 마무리하고 학부연구생도 accept이 되면서 여러모로 많이 정리되어 이제부터 실력 향상을 위해서 달려볼 시간이 되었다.

사실 이 방학이라는 시즌을 손꼽아 기다렸는데, 개인적인 공부 및 성장을 하고 싶어서이다. 학기 중에는 학점을 이수하는 것만으로도 벅찼고, 스스로의 성장을 이뤄내기에는 방학만큼 좋은 시즌이 없다고 생각한다.

# 본론
공부해야 할 내용들은 사실 지난 여름방학 계획가 크게 달라지는 것이 없다.

1. 머신러닝
2. 알고리즘
3. 영어

## 머신러닝
먼저, 머신러닝이다.
머신러닝 같은 경우, 내가 정말 원하던 연구실에 들어갈 수 있어서 너무 좋았다.

사실 작년에도 해당 연구실에 지원을 했는데, 좀 더 구체적으로 본인이 원하는 연구주제를 갖고 왔으면 좋겠다라는 말씀과 더불어 나에게 도움이 될 만한 조언들을 많이 해주셨고, 비록 떨어지게 되었지만 해당 연구실에 대해 좋은 기억을 갖게 되었다. 그래서 작년 말 쯔음에 해당 연구실에 다시 지원하였고 교수님께 같이 연구해보자고 말을 들을 수 있었다.

사실 다시 지원할 당시에도 여전히 나만의 연구주제를 갖고 있지는 않은 상태였다. 지난 1년 동안 나 나름대로의 노력이나 공부를 해 보았지만, 모든 연구 주제가 나한테는 흥미롭고 재미있어 보여서 무엇 하나를 딱 정하기는 나에게는 어려웠다. 그래서 그냥 솔직하게 내 생각을 말씀드렸고, 연구를 너무 배우고 싶은데 혼자서 하기는 너무 어렵다. 우리 연구실에서 연구를 배우고 싶다는 의지를 표명했다. 
2번째로 지원한 것 때문인지 이런 말들 때문인지는 여전히 모르겠지만 감사하게도 기회를 주셨고 이런 기회가 주어진 것에 감사히 여기며 내가 할 수 있는 최선을 다 해 볼 생각이다.

그래서 다시 주제로 돌아와서 어떤 공부를 할 지 말해보면, 일단 **연구 주제에 관련한 기반 지식을 쌓고 연구를 해보는 것**이다. 사실 그냥 학부연구생 생활을 열심히 하는 것에 가깝다 허허.

그래도 간단하게 정리해보면 다음과 같다.
1. 논문 리뷰
2. 연구 미팅 참여 및 연구하기
3. 얻은 지식들 사소한 것이라도 정리해서 포스팅하기

RAG Attack 관련해서 연구를 하기로 하였고 관련해서 교수님께 관련 논문을 추천해주셔서 일단 해당 논문들을 읽고 리뷰하는 내용의 블로그 포스트를 올릴 예정이다.
그리고 연구 미팅 참여 및 연구를 참여할 예정인데, 이 부분은 좀 더 자세히 해보아야 알 수 있을 것 같다.

또한 논문을 리뷰하면서 혹은 연구를 진행하면서 얻은 지식들이 있다면 사소한 것이라도 정리해서 올릴 생각이다.
개인적으로 **공부한 것을 나만의 언어로 다시 정리하는 것**을 좋아하는 편이다. 정리하면서도 확실히 이해가 될 때가 많고 다시 보면 기억해 내기도 쉽기 때문이다. 
(사실 이걸 위해 블로그를 개편했다 ㅎㅎ. 블로그가 이쁠수록 애정을 갖고 더 열심히 포스팅할 거라 생각해서.. 실제로 효과가 꽤 좋은 것 같다.)

머신러닝 같은 경우 이런 식으로 차곡차곡 기본기를 쌓아갈 예정이다.
일단은 연구만으로도 벅차기 때문에 캐글 같은 컴피티션을 참가하는 것은 연구에 어느정도 적응을 하고 나서 생각해 볼 예정이다. (~~자고로 목표는 현실적으로...~~)

## 알고리즘
이번에는 알고리즘이다. 
늘 해오던 것이지만, 올해는 더욱 각별한 것 같다. 바로 ICPC가 이제는 단 한 번의 기회밖에 남지 않았기 때문이다. 지금까지 알고리즘을 관성적으로(?), 기계적으로 푼 느낌이라면 올해에는 좀 더 **목표성을 가지고 공부**를 할 계획이다. 

현재 나에게 있어서 목표는 다음과 같다.
1. 플레티넘 문제를 풀 수 있는 능력
2. 골드 이하 문제를 정확하고 신속하게 푸는 능력
3. 다이아 문제를 도전할 수 있는 능력

위 순서대로 나에게 가장 필요한 역량인 것 같다. &lt;sub&gt;(사실 1번을 하다보면 2번도 크게 도움이 될 거라 생각한다. 물론 따로 연습은 필요할 것이다.)&lt;/sub&gt; 현재 계획 중에 있는 것은 플레티넘 난이도의 문제 2 ~ 3개 정도를 1시간 ~ 1시간 30분 정도의 시간동안 집중해서 풀어보고 업솔빙을 해보는 것이다. 

사실 풀 줄 아는 문제를 많이 푸는 것은 도움은 되지만 가성비가 좋지는 않다고 생각한다. 내가 어렵다고 느끼는 것을 쉽다고 느낄 때까지 도전하고 반복하는 것이 실력 향상에 큰 도움이 될 것이라 생각한다.
위 스터디를 목표로는 매일, 최소한으로는 격일에 한 번은 하고 싶다. 약간의 강제성을 부여하기 위해 같이 할 팀원을 구해서 할 예정이다. (현재 조정 중에 있다.)

그리고 주기 적으로 라이브나 버추얼, 혹은 팀 연습을 통해 전체 문제셋에 대한 연습을 해 볼 생각이다. (아마 1~2주 마다 1,2번은 하게 될 것 같다.)

3번 같은 경우는 1번에 어느 정도 익숙해지고 나면 조금씩 난이도를 높여가면서 다5~플3 이런 식으로 문제를 섞어서 도전해 볼 생각이다. 일단은 1,2번에 집중하고 싶다.

## 영어
영어 같은 경우는 크게 2가지 목표다

1. 회화 능력
2. TOEIC 성적(혹은 TOEIC SPEAKING)

회화 같은 경우는 **꾸준히 연습하는 것** 만이 답 같다. **문장을 구성하는 능력부터 시작**해야 할 것 같다. 
블로그에 포스팅하는 글을 영어로 영작하는 연습을 해 볼 예정이다. 영어로 된 글이나 논문을 읽을 때도 해석할 때 사람들이 어떤 식으로 문장을 구성하는 지를 유심히 살펴보고 기억해볼 생각이다. 그리고 특히 영단어나 표현 같은 걸 잘 메모하고 암기할 생각이다.
물론 결국 회화니깐, 말하는 연습을 하는 것도 중요하다. 일단 매주 영어 회화 스터디를 하고 있어서 여기서 최대한 말하는 연습을 많이 해 볼 생각이다. 이 외에도 회화에 시간을 쓰고 싶지만 이번 방학 때 할 것이 많아서 우선순위로 조금 밀리는 느낌이다.

다음으로 TOEIC 성적이다. 여태껏 제대로 토익성적을 준비해 본 적이 없는데, 대학원 지원 혹은 취업을 고민하게 되면서 필요하다고 느꼈다. 토익은 특별한 것 없이 기출을 틈틈이 풀어보면서 감을 익힐 생각이다.


# 결론
![](https://i.imgur.com/lw8DQy3.png)

이렇게 적어보니 어떻게 보면 목표가 너무 거창해 보이기도 하고, 내가 꾸준히 해낼 수 있을까? 혹은 이제 방학이 겨우 7주 정도 남았는데 내가 정말 많이 성장할 수 있을까? 하는 의심, 걱정, 불안 등이 들기도 한다.
실제로 이번 겨울방학 동안 지금까지 그런 걱정과 스트레스를 많이 받아왔다.


그러나 어차피 자신을 의심해 보았자, 변하는 것은 없다.
중요한 것은 포기하지 않는 것이라 생각한다. 제대로 하는 실패는 자주할수록 가장 빨리 성공에 닿는 길이라 생각한다.

**벅차보이고 불가능해 보일지라도 일단 시작해서 하나씩 차근차근 성장해 나가는 기쁨을 알고 그 안의 즐거움을 발견해 내고자 한다면 어느 순간 목표에 도달해있는 내 모습을 발견할 수 있을 것이다.**
지금까지의 배운 경험으로는 위와 같은 마음가짐으로 내가 할 수 있는 최선을 다 했을 때, 목표 그 이상의 것을 얻을 수 있었고 그리하지 않을 지라도 후회없이 기분 좋은 실패를 통해 배워나갈 수 있었다.

지나고 보면, 이렇게 매일 노력하고 내가 하고자 하는 것을 위해 바쁘게 살아갈 때가 가장 행복하고 가장 많은 성장을 이뤄냈던 것 같다.

어쩌면 내 인생의 많은 선택과 변화가 일어날 2025년 한 해를 행복하고 뜻 깊게 보내고 싶다.


마지막으로 좋아하는 격언을 하나 남기면서 블로그 포스팅을 마치겠다. &lt;sub&gt;비록 이 말을 한 앙드레 말로가 원래 의도한 바와는 다르다는 얘기가 있지만, 그저 글귀 자체가 마음에 드니 상관없다.&lt;/sub&gt; 

&gt; 오랫동안 꿈을 그리는 사람은 마침내 그 꿈을 닮아 간다.

우리 모두 화이팅할 수 있었으면 좋겠다.</content:encoded></item><item><title><![CDATA[백준 24491 - Searching for Soulmates]]></title><link>https://jinsoolve.netlify.app/posts/boj-24491</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/boj-24491</guid><pubDate>Sun, 12 Jan 2025 15:00:00 GMT</pubDate><content:encoded># [문제](https://www.acmicpc.net/problem/24491)
각 테스트케이스마다 2개의 1 ~ $10^{18}$의 수 a, b가 주어질 때, a를 b로 만드는데 드는 최소의 연산 횟수를 구하는 문제이다.

이때 연산은 $\times 2$, $\div 2$(짝수일 때만), $+1$이 가능하다.

# 해결
## 핵심 아이디어
2개의 숫자를 2진수로 바꿔서 생각해보자.

예를 들어, $1010$을 $101001$으로 만들고 싶다고 할 때, prefix가 같으므로 이때의 최소 연산 횟수는 쉽게 구할 수 있고 고정되어 있다고 할 수 있다.
$$
1010 → 10100 → 101000 → 101001
$$
이 횟수를 잘 생각해보면 **prefix 길이를 뺀 나머지 길이**(여기서는 2), **prefix가 아닌 부분에서의 1비트 개수**(여기서는 1)을 합하면 쉽게 구할 수 있다.

그럼 예를 들어서  a와 b가 주어지고, b=$1011$ 인 상황이라고 하자.
a에서 $1011$, $101$, $10$, $1$을 만드는데 드는 최소 연산 횟수를 구하고 나면 각 수에서 원래의 b로 만드는 최소 횟수를 구하는 건 위에서 얘기한 것처럼 prefix가 같으므로 쉽게 구할 수 있다.

따라서 a를 위 수들로 만드는 횟수만 구하면 된다.
a가 만약 목표로 하는 수들보다 작다면  그냥 1로 더해서 만들어준다. 어차피 우리는 목표로 하는 수만 도달하면 저기부터는 알아서 최적의 루트로 b로 만들어주므로 그냥 만들어주기만 하면된다.
반면, a가 목표로 하는 수보다 큰 경우는 $\div 2$을 해줘서 구해준다. 물론 이때 홀수라면 $+1$을 해준다.

이런 식으로 모든 목표 수를 만들고 났을 때의 연산 횟수와 b로 증가시키는 횟수를 더해서 그 중 최솟값을 구하면 그것이 정답이 된다.

## 코드
```cpp
#include &lt;bits/stdc++.h&gt;

#define endl &quot;\n&quot;
#define all(v) (v).begin(), (v).end()
#define all1(v) (v).begin()+1, (v).end()
#define For(i, a, b) for(int i=(a); i&lt;(b); i++)
#define FOR(i, a, b) for(int i=(a); i&lt;=(b); i++)
#define Bor(i, a, b) for(int i=(a)-1; i&gt;=(b); i--)
#define BOR(i, a, b) for(int i=(a); i&gt;=(b); i--)
#define ft first
#define sd second

using namespace std;
using ll = long long;
using lll = __int128_t;
using ulll = __uint128_t;
using ull = unsigned long long;
using ld = long double;
using pii = pair&lt;int, int&gt;;
using pll = pair&lt;ll, ll&gt;;
using ti3 = tuple&lt;int, int, int&gt;;
using tl3 = tuple&lt;ll, ll, ll&gt;;

template&lt;typename T&gt; using ve = vector&lt;T&gt;;
template&lt;typename T&gt; using vve = vector&lt;vector&lt;T&gt;&gt;;

template&lt;class T&gt; bool ckmin(T&amp; a, const T&amp; b) { return b &lt; a ? a = b, 1 : 0; }
template&lt;class T&gt; bool ckmax(T&amp; a, const T&amp; b) { return a &lt; b ? a = b, 1 : 0; }

const int INF = 987654321;
const int INF0 = numeric_limits&lt;int&gt;::max();
const ll LNF = 987654321987654321;
const ll LNF0 = numeric_limits&lt;ll&gt;::max();

void solve() {
    ll a, b; cin &gt;&gt; a &gt;&gt; b;
    ll ans = LNF;
    ll psum=0;

    for(ll remove=0; (b&gt;&gt;remove)&gt;0; remove++) {
        ll prefix = b&gt;&gt;remove;
        while(a &gt; prefix) {
            if(a%2 == 1) {
                a++;
                psum++;
            }
            a/=2;
            psum++;
        }
        ll sum = psum;
        sum += prefix - a;
        sum += remove;
        sum += __builtin_popcountll(b &amp; ((1&lt;&lt;remove) - 1));

        ckmin(ans, sum);
    }
    cout &lt;&lt; ans &lt;&lt; endl;
}

int main(void) {
    ios_base::sync_with_stdio(false);
    cin.tie(nullptr);
    cout.tie(nullptr);

    int TC=1;
    cin &gt;&gt; TC;
    FOR(tc, 1, TC) {
//        cout &lt;&lt; &quot;Case #&quot; &lt;&lt; tc &lt;&lt; &quot;: &quot;;
        solve();
    }


    return 0;
}
```</content:encoded></item><item><title><![CDATA[백준 10321 - 요새 건설]]></title><link>https://jinsoolve.netlify.app/posts/boj-10321</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/boj-10321</guid><pubDate>Sun, 12 Jan 2025 15:00:00 GMT</pubDate><content:encoded># [백준 10321 - 요새 건설](https://www.acmicpc.net/problem/10321)

## 핵심 아이디어
껍질에 있는 점들을 대상으로 임의의 대각선에 대해서 해당 대각선에서 가장 먼 점 2개를 고르면 해당 대각선으로 만들 수 있는 가장 큰 영역이다. 이때 먼점 2개는 대각선을 기준으로 서로 다른 영역에 있음을 가정한다.

## 풀이
1. Graham Scan을 이용해서 Convex Hull을 구한다. 이때 Hull에 있는 점들은 반시계 방향 순서대로 정렬되어 있다.
2. Hull(껍질)에 있는 점들에 대해서 임의의 2개의 점을 고르고 2점을 이어서 만든 대각선에 대해 가장 먼 점 2개를 고른다. (이때 먼 점 2개는 대각선을 기준으로 서로 다른 영역에 있음을 의미함.)
	- 임의의 2개 점을 i, j라 하자. i를 고정시키고 j를 반시계 방향으로 증가시키면서 이동한다고 가정하자.
	- 각 영역의 가장 먼 점을 a, b라 하자.
	- j가 반시계 방향으로 이동하면 a와 b 모두 반 시계 방향으로 이동할 수 밖에 없다. 따라서 a,b의 이동횟수는 한 개의 i에 대해서 n번 씩이다.
	- 따라서 $O(n^2)$ 만에 모든 대각선들에 대한 영역을 검사할 수 있다.

## 코드
```cpp
#include &lt;bits/stdc++.h&gt;

#define endl &quot;\n&quot;
#define all(v) (v).begin(), (v).end()
#define all1(v) (v).begin()+1, (v).end()
#define For(i, a, b) for(int i=(a); i&lt;(b); i++)
#define FOR(i, a, b) for(int i=(a); i&lt;=(b); i++)
#define Bor(i, a, b) for(int i=(a)-1; i&gt;=(b); i--)
#define BOR(i, a, b) for(int i=(a); i&gt;=(b); i--)
#define ft first
#define sd second

using namespace std;
using ll = long long;
using lll = __int128_t;
using ulll = __uint128_t;
using ull = unsigned long long;
using ld = long double;
using pii = pair&lt;int, int&gt;;
using pll = pair&lt;ll, ll&gt;;
using ti3 = tuple&lt;int, int, int&gt;;
using tl3 = tuple&lt;ll, ll, ll&gt;;

template&lt;typename T&gt; using ve = vector&lt;T&gt;;
template&lt;typename T&gt; using vve = vector&lt;vector&lt;T&gt;&gt;;

template&lt;class T&gt; bool ckmin(T&amp; a, const T&amp; b) { return b &lt; a ? a = b, 1 : 0; }
template&lt;class T&gt; bool ckmax(T&amp; a, const T&amp; b) { return a &lt; b ? a = b, 1 : 0; }

const int INF = 987654321;
const int INF0 = numeric_limits&lt;int&gt;::max();
const ll LNF = 987654321987654321;
const ll LNF0 = numeric_limits&lt;ll&gt;::max();

struct Point {
    ll x, y;

    Point() {}
    Point(ll _x, ll _y) : x(_x), y(_y) {}
    ll cross(Point other) {
        return x*other.y - y*other.x;
    }
    ll crossSign(Point other) {
        ll res = this-&gt;cross(other);
        if(res &gt; 0) return 1;
        else if(res &lt; 0) return -1;
        return 0;
    }
    ll dist(Point other) {
        return pow(x-other.x,2) + pow(y-other.y,2);
    }
    Point operator-(Point other) const {
        return Point(x-other.x, y-other.y);
    }
    bool operator==(Point other) const {
        return x == other.x &amp;&amp; y == other.y;
    }
    bool operator&lt;(Point other) const {
        if(x == other.x) return y &lt; other.y;
        return x &lt; other.x;
    }
    void print() {
        cout &lt;&lt; x &lt;&lt; &apos; &apos; &lt;&lt; y &lt;&lt; &apos; &apos;;
    }
};

Point reference;

vector&lt;Point&gt; Graham_Scan(vector&lt;Point&gt; &amp;points) {
    sort(all(points)); points.erase(unique(all(points)), points.end());
    reference = points[0];
    auto cmp = [&amp;](Point a, Point b) {
        ll res = (a - reference).cross(b - reference);
        if(res != 0) return res &gt; 0;
        return reference.dist(a) &lt; reference.dist(b);
    };
    sort(points.begin()+1, points.end(), cmp);

    vector&lt;Point&gt; convex;
    for(Point p3 : points) {
        while(convex.size() &gt;= 2) {
            Point p2 = convex.back();
            Point p1 = convex[convex.size() - 2];
            ll ccw = (p2-p1).cross(p3-p2);
            if(ccw &gt; 0) break;
            convex.pop_back();
        }
        convex.emplace_back(p3);
    }
    return convex;
}

ll triangle(Point &amp;p1, Point &amp;p2, Point &amp;p3) {
    ll res = abs(p1.x*p2.y + p2.x*p3.y + p3.x*p1.y - p2.x*p1.y - p3.x*p2.y - p1.x*p3.y);
//    if(res % 2 == 0) return (ld)((ll)(res/2));
//    return (ld)((ll)(res/2) + 0.5);
    return res;
}

int n;
ve&lt;Point&gt; v;

void solve() {
    cin &gt;&gt; n;
    ve&lt;Point&gt;tmp(n);
    For(i,0,n) cin &gt;&gt; tmp[i].x &gt;&gt; tmp[i].y;
    v = Graham_Scan(tmp);

//    for(auto x:v) {
//        x.print();
//        cout &lt;&lt; endl;
//    }

    n = v.size();
   if(n == 3) {
        ll res = triangle(v[0], v[1], v[2]);
        if(res%2 == 0) cout &lt;&lt; res/2 &lt;&lt; endl;
        else cout &lt;&lt; res/2 &lt;&lt; &quot;.5\n&quot;;
        return;
    }


    ll ret = 0;
    For(i,0,n) {
        int a=(i+1)%n, b=(i+3)%n;
        For(j, i+2, n) {
            while(true) {
                int na = (a+1)%n;
                if(na == j) break;
                if(triangle(v[i], v[j], v[a]) &lt; triangle(v[i], v[j], v[na])) a = na;
                else break;
            }
            while(true) {
                int nb = (b+1)%n;
                if(nb == i) break;
                if(triangle(v[i],v[j],v[b]) &lt; triangle(v[i],v[j],v[nb])) b = nb;
                else break;
            }
            ckmax(ret, triangle(v[i],v[j],v[a]) + triangle(v[i],v[j],v[b]));
        }

    }

    if(ret%2 == 0) cout &lt;&lt; ret/2 &lt;&lt; endl;
    else cout &lt;&lt; ret/2 &lt;&lt; &quot;.5\n&quot;;
}

int main(void) {
    ios_base::sync_with_stdio(false);
    cin.tie(nullptr);
    cout.tie(nullptr);

    cout &lt;&lt; fixed &lt;&lt; setprecision(1);

    int TC=1;
    cin &gt;&gt; TC;
    FOR(tc, 1, TC) {
//        cout &lt;&lt; &quot;Case #&quot; &lt;&lt; tc &lt;&lt; &quot;: &quot;;
        solve();
    }


    return 0;
}
```



## 오답노트

1. 처음에는 한 점에 대해서 가장 먼 점을 구하면 해당 대각선으로 만든 영역 중 가장 큰 것이 해당 점을 기주으로 했을 때 최선이라 가정했으나, 이 가정을 증명할 수 없어서 틀렸다. 
   모든 대각선에 대해서 검사하고 a,b가 n번 씩 밖에 움직일 필요가 없음을 이용해야 했다.
2. 위 풀이대로 구현하였는데 틀렸습니다가 나왔다. ret을 -1로 초기화했는데, ret이 한번도 업데이트되지 않는 경우가 존재한 모양이다. (심지어 $n &lt; 3$ 일 때 0을 반환해주도록 했는데, 그 외의 경우도 있는 듯 하다.)
   


# 참고
- https://justicehui.github.io/icpc/2019/02/04/BOJ10321/</content:encoded></item><item><title><![CDATA[백준 33007 - Greatest of the Greatest Common Divisors]]></title><link>https://jinsoolve.netlify.app/posts/boj-33007</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/boj-33007</guid><pubDate>Fri, 10 Jan 2025 15:00:00 GMT</pubDate><content:encoded># [백준 33007 - Greatest of the Greatest Common Divisors](https://www.acmicpc.net/problem/33007)

## 핵심 아이디어
오프라인 쿼리 방식으로, r을 순차적으로 증가시키면서 **r을 새롭게 포함시킬 때 해당 인덱스 r과 어떤 인덱스 l을 추가하면 어떤 gcdVal을 얻을 수 있는 지를 미리 저장해 놓았다가 이를 포함**시킨다.
그리고 query의 오른쪽 끝이 r인 쿼리들에 대해서 정답을 업데이트 시켜준다.


## 풀이

1. 소인수 분해를 전처리를 해준다. 
   수의 개수와 수의 크기가 모두 최대 $10^5$ 이므로 총 $10^{7.5}$ 만에 모든 수의 소인수분해를 할 수 있다.
   `divs[x] := { x로 나뉘어지는 모든 수들의 인덱스 }` 형식으로 저장한다.
2. 위 전처리 결과를 이용해서 숫자 i를 소인수 분해로 갖는 수들의 목록을 얻을 수 있다.
   이때 이 수들에 대한 모든 쌍을 볼 필요 없이, 인접한 쌍 (l,r)끼리만 확인하면 된다. 왜냐하면 이 중 어느 2개의 수만 쿼리가 포함하면 숫자 i를 gcd로 가질 수 있기 때문이다. 
   따라서 가장 가까운 두 쌍에 대해서만 `pre[r].emplace_back(l, i)`  이런 식으로 저장해주면 된다.
3. 쿼리도 마찬가지로 `queries[r].emplace_back(l, i)` 이런 식으로 저장해준다. 여기서 i는 쿼리 번호이다. 오프라인 쿼리 방식으로 처리해줄 것이다.
	1. 이제 모든 r을 순차적으로 증가시키면서 세그먼트 트리를 업데이트 시켜준다.
	   `for(auto [l, gcdVal]: pre[r])` 이런 식으로 r을 포함시켰을 때, 어떤 l을 포함하면 어떤 gcdVal을 얻을 수 있는 지를 세그먼트 트리에 업데이트 시켜준다.
	2. 쿼리도 `for(auto [l, query_num]: queries[r])` 이런 식으로 쿼리의 범위 끝이 r인 쿼리들에 대한 답을 업데이트 해준다.

## 코드
```cpp
#include &lt;bits/stdc++.h&gt;  
  
#define endl &quot;\n&quot;  
#define all(v) (v).begin(), (v).end()  
#define all1(v) (v).begin()+1, (v).end()  
#define For(i, a, b) for(int i=(a); i&lt;(b); i++)  
#define FOR(i, a, b) for(int i=(a); i&lt;=(b); i++)  
#define Bor(i, a, b) for(int i=(a)-1; i&gt;=(b); i--)  
#define BOR(i, a, b) for(int i=(a); i&gt;=(b); i--)  
#define ft first  
#define sd second  
  
using namespace std;  
using ll = long long;  
using lll = __int128_t;  
using ulll = __uint128_t;  
using ull = unsigned long long;  
using ld = long double;  
using pii = pair&lt;int, int&gt;;  
using pll = pair&lt;ll, ll&gt;;  
using ti3 = tuple&lt;int, int, int&gt;;  
using tl3 = tuple&lt;ll, ll, ll&gt;;  
  
template&lt;typename T&gt; using ve = vector&lt;T&gt;;  
template&lt;typename T&gt; using vve = vector&lt;vector&lt;T&gt;&gt;;  
  
template&lt;class T&gt; bool ckmin(T&amp; a, const T&amp; b) { return b &lt; a ? a = b, 1 : 0; }  
template&lt;class T&gt; bool ckmax(T&amp; a, const T&amp; b) { return a &lt; b ? a = b, 1 : 0; }  
  
const int INF = 987654321;  
const int INF0 = numeric_limits&lt;int&gt;::max();  
const ll LNF = 987654321987654321;  
const ll LNF0 = numeric_limits&lt;ll&gt;::max();  
  
class Segment {  
public:  
    vector&lt;int&gt; tree; //tree[node] := a[start ~ end] 의 합  
  
    Segment() {}  
    Segment(int size) {  
        this-&gt;resize(size);  
    }  
    void resize(int size) {  
        size = (int) floor(log2(size)) + 2;  
        size = pow(2, size);  
        tree.resize(size, 1);  
    }  
    int query(int node, int start, int end, int left, int right) {  
        if(right &lt; start || end &lt; left) return 1;  
        if(left &lt;= start &amp;&amp; end &lt;= right) return tree[node];  
        return max(query(node * 2, start, (start + end) / 2, left, right),  
                   query(node * 2 + 1, (start + end) / 2 + 1, end, left, right));  
    }  
    void update(int node, int start, int end, int index, int value) {  
        if(index &lt; start || end &lt; index) return;  
        if(start == end) ckmax(tree[node], value);  
        else {  
            update(node * 2, start, (start + end) / 2, index, value);  
            update(node * 2 + 1, (start + end) / 2 + 1, end, index, value);  
            tree[node] = max(tree[2*node], tree[2*node+1]);  
        }  
    }  
};  
  
const int mxn = 1e5+1;  
int n, q;  
ve&lt;int&gt; a;  
vve&lt;int&gt; divs(mxn);  
vve&lt;pii&gt; pre, queries;  
ve&lt;int&gt; ans;  
  
ll gcd(ll a, ll b) {  
    if(b == 0) return a;  
    return gcd(b, a%b);  
}  
  
void solve() {  
    cin &gt;&gt; n;  
    a = ve&lt;int&gt;(n+1);  
    pre = vve&lt;pii&gt;(n+1);  
    queries = vve&lt;pii&gt;(n+1);  
    FOR(i,1,n) cin &gt;&gt; a[i];  
  
    // 모든 소인수들에 대해서 어떤 a[인덱스]를 나눌 수 있는지를 모두 저장. 즉, 미리 소인수분해를 해 놓는다.  
    FOR(i,1,n) {  
        for(int j=1; j*j&lt;=a[i]; j++) {  
            if(a[i]%j == 0) {  
                divs[j].emplace_back(i);  
                if(j*j != a[i]) divs[a[i]/j].emplace_back(i);  
            }  
        }  
    }  
  
    // 모든 i에 대하여 i로 나눠지는 l,r (여기서 l,r은 인접한 애들)을 저장해준다.  
    FOR(i,1,mxn) {  
        for(int j=1; j&lt;divs[i].size(); j++) {  
            int l=divs[i][j-1], r=divs[i][j];  
            if (gcd(a[l], a[r]) == i) pre[r].emplace_back(l,i);  
        }  
    }  
  
    cin &gt;&gt; q;  
    ans = ve&lt;int&gt;(q+1, 1);  
    FOR(i,1,q) {  
        int l, r; cin &gt;&gt; l &gt;&gt; r;  
        queries[r].emplace_back(l,i);  
    }  
  
  
    Segment seg(n);  
    // 1 ~ r 에 대한 수들만 처리하고, 그에 대한 쿼리만 순차대로 처리해준다.  
    FOR(r,1,n) {  
        // pre[r] 정보들을 세그먼트 트리에 업데이트시켜준다.  
        for(auto [l, gcdVal]: pre[r]) {  
            seg.update(1,1,n,l,gcdVal);  
        }  
  
        // r이 r인 쿼리들에 대한 정답을 계산해준다.  
        for(auto [l, qn]: queries[r]) {  
            ckmax(ans[qn], seg.query(1,1,n,l,r));  
        }  
    }  
  
    FOR(i,1,q) cout &lt;&lt; ans[i] &lt;&lt; endl;  
}  
  
int main(void) {  
    ios_base::sync_with_stdio(false);  
    cin.tie(nullptr);  
    cout.tie(nullptr);  
  
    int TC=1;  
//    cin &gt;&gt; TC;  
    FOR(tc, 1, TC) {  
//        cout &lt;&lt; &quot;Case #&quot; &lt;&lt; tc &lt;&lt; &quot;: &quot;;  
        solve();  
    }  
  
  
    return 0;  
}
```


# 참고
- [구사과님의 깃허브](https://github.com/koosaga/olympiad/blob/master/ICPC/Japan/icpc2024_i.cpp)</content:encoded></item><item><title><![CDATA[머신러닝 교과서 8장]]></title><description><![CDATA[자연어 처리(Natural Language Processing, NLP)의 하위 분야인 감성 분석(sentiment analysis)에 대해서 알아보는 장이다.]]></description><link>https://jinsoolve.netlify.app/posts/ml-textbook-9</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/ml-textbook-9</guid><pubDate>Wed, 08 Jan 2025 15:00:00 GMT</pubDate><content:encoded>자연어 처리(Natural Language Processing, NLP)의 하위 분야인 감성 분석(sentiment analysis)에 대해서 알아보는 장이다.

영화 리뷰를 읽고 각 리뷰가 긍정인지 부정인지 구별하는 예측기를 구축하자.

# 데이터 전처리
모델에 데이터를 주기 전에 모델이 잘 학습할 수 있도록 preprocessing을 해줘야 한다. 다음과 같은 개념을 알고 활용해보자.

---
BoW 모델을 사용하여 범주형 데이터를 수치로 변경해주겠다.
BoW의 과정은 다음과 같다.

1. 단어를 토큰 단위로 나누기
2. 중복을 제거해 단어 사전 생성
3. 문서에 포함된 단어의 개수를 나타내는 행렬 생성

아래의 내용들은 BoW 모델에 대한 지식들이다.
## tf(t,d)
텍스트나 단어 같은 범주형 데이터는 수치 형태로 인코딩해줘야 한다. BoW(Bag-of-Word)를 사용하자.

수로 이루어진 특성 벡터로 나타냄
행: 문서 d, 열: 단어 t -&gt; 값: 문서 d에 단어 t가 몇 개 있는가 = tf(t,d) 
즉 tf(t,d)의 행렬을 생성함.


- 1-gram or unigram: 하나의 단어만 저장, BoW의 경우가 그러하다.
- n-gram: 여러 개의 단어를 이어서 저장.
예를 들면, the sun is shining.이라는 문장을 the sun, sun is, is shining으로 저장

---
## tf-idf(t,d)
tf-idf(Term Frequency - Inverse Document Frequency)
문서마다 자주 나오는 단어는 중요하지 않을 가능성이 높다. 해당 단어의 가중치를 낮춰주자.
가중치를 낮추는 방법은 아래 처럼 $idf(t,d)$를 곱해주는 것이다.
- $tf-idf(t,d) = tf(t,d) \times idf(t,d)$
- $idf(t,d) = log\frac{n_d}{1+df(d,t)}$
- df(d,t): 단어 t가 포함된 문서 d의 개수
- $n_d$: 전체 문서 개수

1을 더해주거나 log를 취해주는 이유는 값을 원하는 범위로 맞추고 싶기 때문이다.
원래는 정규화를 한 후에 idf를 계산해야 하지만 사이킷런의 TfidfTransformer에서는 tf-idf 자체를 L2 정규화한다.

---

## 텍스트 데이터 정제
이모티콘을 제외하고 구두점 같은 필요없는 문자를 제거해준다.
여기서는 정규표현식으로 했으나 파이썬의 HTML 파서 모델이 더 다양한 기능을 제공한다.

---
## 문서를 토큰으로 나누기
문서를 토큰 즉, 단어로 나누는 가장 간단한 방법은 공백이지만 너무 대충이라 다른 방법을 찾아보자.

어간 추출 방법인 Porter stemmer 알고리즘을 사용할 수 있다. nlkt라이브러리에 구현되어 있다. 어간은 핵심 단어요소로 running에서 어간은 run이 된다. 
어간 추출 알고리즘은 초기 알고리즘인 Porter 말고도 발전된 Snowball, Lancaster 등이 있다.

다만 어간 추출은 약간의 오류가 있는데, thus -&gt; thu를 추출하는 경우가 그러하다. 이러한 오류를 없애고 정확하게 추출하는 알고리즘을 표제어 추출이라고 하는데 이는 계산이 어렵고 비용이 많이 든다.
실전에서는 어간추출과 표제어추출의 성능 차이가 크지 않기 때문에 어간추출을 많이 사용한다. (한글은 조사와 어미가 발달되어 있어 표제어 추출방식이 적합하다)

문장의 구조에 중요한 역할을 하지만 의미는 크지 않는 불용어(stop-word)를 제거할 때가 있다. &lt;sub&gt;예시로는 is, and, has, like가 있다.&lt;/sub&gt; 이미 tf-idf를 통해 가중치 값이 많이 낮았지만 이를 미리 제공된 불용어 리스트를 적용시켜 해결한다.

---
# 모델링
데이터 전처리를 끝냈으니, 이제 모델링을 해보자.

---
## 문서 분류를 위한 로지스틱 회귀 모델 훈련
이제 전처리(빈도 수 가중치 조절, 텍스트 정제, 토큰 나누기 등)을 마쳤으니 로지스틱 회귀 모델에 넣어 훈련시키자.
(각 단어의 값) -&gt; (주로 긍정/부정 결과이더라)

---
## 대용량 데이터 처리
NLP는 계산비용이 많고 그로 인한 하드웨어적 한계를 극복하기 위해 외부 메모리 학습(out-of-core learning)을 사용한다.

외부 메모리 학습은 작은 일괄(batch)로 분할아여 메모리에 한 번에 가져오지 않고 순차적으로 학습 시키는 과정이다.
데이터를 한 번에 가져오지 않는 만큼 tf-idf를 사용하기 위한 CountVectorizer와 TfidVectorizer를 사용할 수 없다.
대신 해시 함수를 사용하기 때문에 어휘사전이 필요 없는 HashingVectorizer를 사용한다.

모델은 확률적 경사 하강법(Stochastic Gradient Descent)를 사용한 SGDClassifier를 이용하여 데이터를 학습시켰다.

model1: 전체 데이터, GridSearch, Logistic 모델
model2: batch 데이터, SGD 모델

model1이 model2보다 계산 비용이 훨씬 높음에도 불구하고 성능 차이는 얼마나지 않는다.

online learning은 실시간으로 데이터를 작은 단위로 받아서 학습시키는 것이고
mini-batch learning은 데이터를 미니 배치로 나눠서 각 미니 배치에 대한 손실을 계산하고 이를 기반으로 가중치를 업데이트한다.
online learning보다 mini-batch가 조금 더 오래 걸리지만 성능이 좋은 걸로 알고 있다.

BoW모델을 대체할 수 있는 word2vec 모델도 있다.
word2vec은 신경망을 기반으로 한 비지도 학습 알고리즘으로 단어들간의 관계를 학습한 기법이다.

---
## 잠재 디리클레 할당을 사용한 토픽 모델링
비지도 데이터에 대해서 사용한다.
예를 들면, 뉴스 기사를 읽고 스포츠, 금융, 세계 뉴스, 정치 등으로 토픽을 분류하는 작업을 말한다.

인기 있는 토픽 분류 모델링 기법인 잠재 디리클레 할당(Latent Dirichlet Allocation, LDA)를 소개하겠다.
LDA는 BoW 행렬을 문서-토픽 행력과 단어-토픽 행렬으로 분리한다. 유일한 단점은 미리 토픽 개수를 정해야 한다는 것이다.

사이킷런에 구현된 LDA를 사용하면 토픽을 구별해낼 수 있다.</content:encoded></item><item><title><![CDATA[머신러닝 교과서 7장: 앙상블 학습]]></title><description><![CDATA[머신러닝 교과서 7장의 앙상블 학습에 대해서 알아보자.]]></description><link>https://jinsoolve.netlify.app/posts/ml-textbook-8</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/ml-textbook-8</guid><pubDate>Wed, 08 Jan 2025 15:00:00 GMT</pubDate><content:encoded>- 다수결 투표 기반 모델(보팅) -&gt; 최빈값을 선택
- 랜덤하게 복원추출하여(배깅) 과대적합 감소
- 앞선 모델의 오차를 학습하여 강력 모델 구축(부스팅)
# 7.1 앙상블 학습
앙상블 모델은 거의 항상 단일 모델보다 정확도가 높다.
# 7.2 다수결 투표를 사용한 분류 앙상블
모델 여러 개의 예측값을 가중치를 곱해서 앙상블 결과를 얻는다. 각각의 모델의 하이퍼파라미터를 GridSearch로 찾아 전체 앙상블 모델의 예측결과가 어떠한지 찾을 수 있다.
각각의 모델을 합칠 때 파이프라인을 잘 이용하자. 정규화나 표준화가 필요한 모델 같은 경우 파이프라인으로 전처리 과정을 합쳐준다.
# 7.3 배깅: 부트스트랩 샘플링을 통한 분류 앙상블
Bagging은 Bootstrap Aggregating 의 줄임말로, bootstrap(랜덤하게 복원추출)하여 각 분류기에 전달하여 결과를 앙상블하는 것을 말한다.
대표적으로는 결정트리를 bagging하게 샘플링하는 랜덤포레스트가 있다.

랜덤 포레스트는 분산을 최적화시키기에는 좋지만, 편향을 잡는 데에는 약하다. 모델이 너무 단순하기 때문이다. 이것이 배깅을 수행할때 편향이 낮은 모델, 예를 들어 가지치기하지 않은 결정트리를 분류기로 사용하여 앙상블을 만드는 이유다.
# 7.4 약한 학습기를 이용한 에이다부스트
adaboost(Adaptive Boosting)은 부스팅 앙상블 모델이다.

부스팅이란 앙상블 모델에서 하나의 모델을 훈련할 때 이전 모델의 결과를 참고해서 결과를 좋게 나오도록 하는 기법이다. 
여기서 사용하는 분류기들은 단순한 모델(약한 분류기, 예를 들면 깊지 않은 결정트리)를 사용한다.

Adaboost는 이전 분류기에서 제대로 분류하지 못한 샘플들에 대해 가중치를 높이고 잘 분류한 샘플들에는 가중치를 낮춰서 결국에는 모두 제대로 분류할 수 있도록 만든다.

AdaBoost는 잘못 분류된 점을 이용하여 약분류기 학습의 가중치를 결정하였다면, GBM은 Gradient Descent 기법을 이용하여 손실함수를 최소화하는 방향으로 학습을 합니다.

---

GBM의 매개변수에 대해서 간단하게 설명하겠다.
- n_estimators: 약한 학습기 수
- subsample: 해당 비율만큼 훈련 데이터에서 랜덤 샘플링(확률적 그래디언트가 된다)
- early stopping: 약한 학습기가 많다고 무조건 좋은 것이 아니라 감소하다가 어느 순간부터 증가하기 때문에 중간에 훈련을 멈출 수 있도록 하는 값

---

GBM에는 다양한 모델들이 있다.

- 일반적인 GradientBoostingClassifer
- XGBoost (eXtreme Gradient Boost)
	일반적인 GBM을 업그레이드 시킨 모델
    병렬처리, 효율적인 특징 중요도 계산등의 최적화가 되어 있음. (여러모로 전처리나 정규화, early stopping 등을 미리 제공함)
- HistGradientBoostingClassifier
	입력 특성을 256개의 구간으로 나누어 노드를 분할에 사용.
    일반적으로 샘플 개수가 1만 개보다 많은 경우 XGBoost보다 Hist가 더 효과적이다.
- LightGBM
	히스토그램 기반의 학습방법을 사용하여 데이터 분할을 최적화하고 속도 향상 시킴.
    HistGBM이 LGBM의 영향을 많이 받았다고 함.
</content:encoded></item><item><title><![CDATA[모델 평가와 하이퍼 파라미터 튜닝의 모범 사례]]></title><description><![CDATA[모델 평가와 하이퍼 파라미터 튜닝의 모범 사례에 대해서]]></description><link>https://jinsoolve.netlify.app/posts/ml-textbook-7</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/ml-textbook-7</guid><pubDate>Wed, 08 Jan 2025 15:00:00 GMT</pubDate><content:encoded>해당 내용은 머신러닝 교과서 with 파이썬, 사이킷런, 텐서플로의 장 내용을 기반으로 작성되었다.


# 6.1 파이프라인
![](https://i.imgur.com/z8WhuVW.png)


우리는 모델을 훈련시킬 때 데이터를 전처리한 후에 학습시킨다. 그럼 테스트 데이터셋이나 검증 데이터셋을 모델에 넣을 때도 같은 전처리 과정을 거쳐야 한다.
매번 같은 과정을 한 번에 표현할 수 있도록 도와주는 것이 파이프라인이다. 예시로 설명하겠다.
```py
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline

pipe_lr = make_pipeline(StandardScaler(),
                        PCA(n_components=2),
                        LogisticRegression(random_state=1))

pipe_lr.fit(X_train, y_train)
y_pred = pipe_lr.predict(X_test)
print(&apos;테스트 정확도: %.3f&apos; % pipe_lr.score(X_test, y_test))
```
위 코드는 정규화 -&gt; PCA 차원 축소 -&gt; 로지스틱회귀 순으로 전처리 및 모델링 하였다. 
이를 파이프라인을 이용해 test 데이터셋에 동일하게 적용시켜 예측값을 반환하는 코드다. 매우 편리하게 사용할 수 있다.

# 6.2 K-Fold 교차 검증을 사용한 모델 성능 평가
적절한 편향-분산 트레이드 오프를 찾고 하이퍼 파라미터 튜닝을 통해서 적절한 모델을 선택하려면 해당 모델의 성능이 어떠한지를 평가해야 한다.
이때 테스트 데이터셋으로 평가 및 모델 조정 과정을 여러번 하게 되면 테스트 데이터셋도 사실상 훈련 데이터처럼 학습되고 과대적합될 수 있다. 때문에 훈련 데이터 안에서 검증 데이터를 따로 뗴어내어 해당 검증 데이터만으로 모델을 조정하고 한 번도 사용하지 않은 테스트 데이터셋으로 모델이 일반화가 잘 되었는지 확인해야 건강한(?) 모델링이라 할 수 있다.

즉, 검증 데이터로 모델이 더 좋은 결과값을 얻도록 조정하고 깨끗한(?) 테스트 데이터로 모델의 일반화 능력을 확인하는 것이다.
그럼 이제 검증 과정에 대해서 자세히 알아보자.
## 6.2.1 홀드아웃
전통적이고 널리 알려져 있는 홀드아웃은 단순하게 훈련데이터에서 일부를 뗴어내고 이를 검증 데이터로 사용하는 것이다.
그러나 이 방법은 검증 데이터를 추출하는 경우마다 점수가 달라진다는 단점이 있다. 
이를 해결하기 위해 훈련 데이터를 k개로 나눠서 k번의 홀드아웃을 하는 방식이 있는데 이를 **k-fold 교차 검증**이라 한다.
## 6.2.2 K-Fold 교차 검증
k-fold는 k개로 나눠서 k번 진행한 홀드아웃에 대한 평균값을 반환하기 때문에 추출하는 케이스에 따라 값이 달라지는 정도가 훨씬 덜 하다는 정도가 있다.
또한 데이터를 k개로 나눌 때 중복을 허용하지 않고 샘플링하기 때문에 k번 홀드아웃한다고 해서 하나의 데이터를 여러 번 검증에 사용하지 않는다. (여러 번 사용하면 분산이 커지고 편향이 낮아짐. 즉, 과대적합될 수 있다.)

가장 기본적이고 많이 사용되는 k값은 10으로 대체적으로 좋은 성능을 보인다.
데이터가 너무 작다면 k를 증가시키는 것이 좋지만 과도하게 하면 과대적합(분산이 카지고, 편향이 작아짐)이 될 수 있으므로 주의해야 한다.
반면에 대규모 데이터인 경우 k값이 5정도만 돼도 충분하다.

참고로 k-fold를 통해서 데이터 검증을 마치고 모델 조정(= 모델 선택)을 마쳤다면 검증 데이터를 다시 훈련 데이터에 합쳐서 이 전체 훈련 데이터로 모델을 다시 훈련시키는 것이 좋다. 훈련 샘플이 많을 수록 모델 성능이 더 안정화되고 정확해지기 때문이다.
## 6.2.3 Stratified K-Fold
기존 K-Fold는 target 데이터인 클래스의 비율이 균일하지 않을 때 편향-분산 트레이드 오프가 안 좋아지는데, 이를 계층화 k-fold로 어느 정도 해결할 수 있다.
stratified k-fold는 데이터를 k개로 나눌 때 클래스 비율이 전체 훈련 데이터의 비율과 최대한 동일하게 나눈다. 
코드는 다음과 같다.
```py
import numpy as np
from sklearn.model_selection import StratifiedKFold

kfold = StratifiedKFold(n_splits=10).split(X_train, y_train)

scores = []
for k, (train, test) in enumerate(kfold):
    pipe_lr.fit(X_train[train], y_train[train])
    score = pipe_lr.score(X_train[test], y_train[test])
    scores.append(score)
    print(&apos;폴드: %2d, 클래스 분포: %s, 정확도: %.3f&apos; % (k+1,
          np.bincount(y_train[train]), score))
    
```
k-fold의 점수를 알려면 stratified k-fold를 사용한 후에 각 폴드에 대한 점수를 파이프라인을 이용하여 계산 후에 저장해야 한다. 이를 사이킷런에서 만든 함수가 있는데 cross_val_score이다.
## 6.2.4 cross_val_score
사이킷런에 cross_val_score는 stratified k-fold를 해서 검증 데이터셋을 준비한 후에 이에 대한 점수값을 출력해주는 함수다.
```py
from sklearn.model_selection import cross_val_score

# estimator에는 우리가 사용한 파이프라인을 넣어준다. 
# cv에는 몇 개의 폴드를 나눌 지를 정한다.
scores = cross_val_score(estimator=pipe_lr,
                         X=X_train,
                         y=y_train,
                         cv=10,
                         n_jobs=1)
print(&apos;CV 정확도 점수: %s&apos; % scores)
print(&apos;CV 정확도: %.3f +/- %.3f&apos; % (np.mean(scores), np.std(scores)))
```
cross_val_score의 기본 지표는 회귀: $R^2$, 분류: 정확도 이다. scoring 파라미터를 이용해서 이를 변경할 수 있다.
## 6.2.5 cross_validate
대체로 cross_val_score를 통해서 교차검증을 해결할 수 있지만 좀 더 많은 정보가 필요한 경우에 cross_validate함수를 사용한다. 모델 훈련 및 테스트 시간, 여러 성능 메트릭, 원하는 작업(예: 예측 결과 저장)을 수행할 수 있다.
```py
from sklearn.model_selection import cross_validate

scores = cross_validate(estimator=pipe_lr, 
                        X=X_train, 
                        y=y_train, 
                        scoring=[&apos;accuracy&apos;], 
                        cv=10, 
                        n_jobs=-1)
print(&apos;CV 정확도 점수: %s&apos; % scores[&apos;test_accuracy&apos;])
print(&apos;CV 정확도: %.3f +/- %.3f&apos; % (np.mean(scores[&apos;test_accuracy&apos;]), 
                                 np.std(scores[&apos;test_accuracy&apos;])))
```
## 6.2.6 cross_val_predict
cross_val_score가 예측값에 따른 점수를 반환했다면,
cross_val_predict는 바로 그 예측값을 반환한다.
```py
from sklearn.model_selection import cross_val_predict

# method=&apos;predict_proba&apos;: 예측값을 얼마의 확률로 결과를 얻었는지 알게 된다.
preds = cross_val_predict(estimator=pipe_lr,
                          X=X_train, 
                          y=y_train,
                          cv=10, 
                          method=&apos;predict_proba&apos;, 
                          n_jobs=-1)
preds[:10]
```
&gt; &lt;sub&gt;method=&apos;predict_proba&apos; 없을 때&lt;/sub&gt;
array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1])

&gt; &lt;sub&gt;method=&apos;predict_proba&apos; 있을 떄&lt;/sub&gt;
array([[9.93982352e-01, 6.01764759e-03],
       [7.64328337e-01, 2.35671663e-01],
       [9.72683946e-01, 2.73160539e-02],
       [8.41658121e-01, 1.58341879e-01],
       [9.97144940e-01, 2.85506043e-03],
       [9.99803660e-01, 1.96339882e-04],
       [9.99324159e-01, 6.75840609e-04],
       [2.12145074e-06, 9.99997879e-01],
       [1.28668437e-01, 8.71331563e-01],
       [7.76260670e-04, 9.99223739e-01]])

# 6.3 학습곡선과 검증곡선을 사용한 알고리즘 디버깅
학습곡선과 검증곡선을 그래프로 그려서 모델이 편향과 분산은 어떠한지, 과소적합인지, 과대적합인지를 해석할 수 있다.
물론 데이터 자체 수를 늘리는 것이 해결할 때도 많지만 이는 현실적으로 어렵고, 
데이터에 노이즈가 많거나 모델이 이미 최적화된 경우 데이터양을 늘리는 것으로는 해결이 어렵다.
이때 학습곡선과 검증곡선을 분석해 이를 해결해 볼 수 있다.
## 6.3.1 학습곡선으로 편향과 분산 문제 분석
![](https://velog.velcdn.com/images/jinsoolve/post/a6389860-b464-4d4e-96fb-237c30088045/image.png)

- 기대 정확도와 많이 떨어져 있을 수록 편향이 크다.
- 학습 곡선과 검증 곡선 사이의 간격이 크다 = 분산이 크다.

마지막 그림처럼 기대 정확도에 가까우면서 학습곡선과 검증곡선 사이의 간격이 좁을 때가 좋은 편향과 분산을 가질 때다.

이처럼 **학습데이터의 크기에 따른 학습데이터와 검증데이터의 성능을 그린 그래프를 학습곡선**이라 한다.

### 6.3.1.1 그래프 그리기
훈련 데이터셋의 크기에 학습곡선을 그려 보자.
```py
import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve


pipe_lr = make_pipeline(StandardScaler(),
                        LogisticRegression(penalty=&apos;l2&apos;, random_state=1,
                                           max_iter=10000))

train_sizes, train_scores, test_scores =\
                learning_curve(estimator=pipe_lr,
                               X=X_train,
                               y=y_train,
                               train_sizes=np.linspace(0.1, 1.0, 10),
                               cv=10,
                               n_jobs=1)

train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

plt.plot(train_sizes, train_mean,
         color=&apos;blue&apos;, marker=&apos;o&apos;,
         markersize=5, label=&apos;Training accuracy&apos;)

plt.fill_between(train_sizes,
                 train_mean + train_std,
                 train_mean - train_std,
                 alpha=0.15, color=&apos;blue&apos;)

plt.plot(train_sizes, test_mean,
         color=&apos;green&apos;, linestyle=&apos;--&apos;,
         marker=&apos;s&apos;, markersize=5,
         label=&apos;Validation accuracy&apos;) 

plt.fill_between(train_sizes,
                 test_mean + test_std,
                 test_mean - test_std,
                 alpha=0.15, color=&apos;green&apos;)

plt.grid()
plt.xlabel(&apos;Number of training examples&apos;)
plt.ylabel(&apos;Accuracy&apos;)
plt.legend(loc=&apos;lower right&apos;)
plt.ylim([0.8, 1.03])
plt.tight_layout()
# plt.savefig(&apos;images/06_05.png&apos;, dpi=300)
plt.show()
```
![](https://velog.velcdn.com/images/jinsoolve/post/a6a8b8ee-b1be-41fb-9a3b-e385ad38958c/image.png)

### 6.3.1.2 그래프 해석
훈련데이터의 크기가 250 이상일 때 좋은 편향, 분산을 갖게 되는 것으로 보인다.
## 6.3.2 검증곡선으로 과대적합, 과소적합 조사
**하이퍼 파라미터에 따른 훈련 데이터와 검증 데어티의 성능을 그린 그래프를 검증곡선**이라 한다.
### 6.3.2.1 그래프 그리기
이번에는 로지스틱 회귀의 파라미터 C의 값에 따른 검증 곡선을 그려보자.
```py
from sklearn.model_selection import validation_curve


param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]

# param_name=&apos;logisticregression__C&apos;: 로지스틱 회귀의 파라미터 C를 x축으로 사용한다.
train_scores, test_scores = validation_curve(
                estimator=pipe_lr, 
                X=X_train, 
                y=y_train, 
                param_name=&apos;logisticregression__C&apos;, 
                param_range=param_range,
                cv=10)

train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

plt.plot(param_range, train_mean, 
         color=&apos;blue&apos;, marker=&apos;o&apos;, 
         markersize=5, label=&apos;Training accuracy&apos;)

plt.fill_between(param_range, train_mean + train_std,
                 train_mean - train_std, alpha=0.15,
                 color=&apos;blue&apos;)

plt.plot(param_range, test_mean, 
         color=&apos;green&apos;, linestyle=&apos;--&apos;, 
         marker=&apos;s&apos;, markersize=5, 
         label=&apos;Validation accuracy&apos;)

plt.fill_between(param_range, 
                 test_mean + test_std,
                 test_mean - test_std, 
                 alpha=0.15, color=&apos;green&apos;)

plt.grid()
plt.xscale(&apos;log&apos;)
plt.legend(loc=&apos;lower right&apos;)
plt.xlabel(&apos;Parameter C&apos;)
plt.ylabel(&apos;Accuracy&apos;)
plt.ylim([0.8, 1.0])
plt.tight_layout()
# plt.savefig(&apos;images/06_06.png&apos;, dpi=300)
plt.show()
```
![](https://velog.velcdn.com/images/jinsoolve/post/c2df2e33-f940-4785-8fa1-ede4cc544673/image.png)
### 6.3.2.2 그래프 해석
- C가 너무 낮다 -&gt; 규제가 강하다 -&gt; 모델 단순화 -&gt; 과소적합 이 된다.
그래프를 보면 C가 낮을 때 훈련데이터와 검증데이터의 정확도가 얼마 차이 나고 이는 모델의 일반화 능력이 좋다는 것을 의미하지만 정확도 자체가 낮다. 따라서 과소적합이라고 해석할 수 있다.
- C가 너무 크다 -&gt; 규제가 약하다 -&gt; 모델 복잡도 증가 -&gt; 과대적합 이 된다.
그래프를 보면 C가 클 때 정확도 자체는 높아지지만 훈련데이터와 검증데이터의 정확도의 차이가 커진다. 이는 모델의 일반화 능력이 좋지 않다는 것을 의미하고 과대적합이라고 해석할 수 있다.
# 6.4 그리드 서치를 사용한 머신러닝 모델 세부 튜닝
하이퍼 파라미터를 튜닝하는 데 사용하는 gridsearch가 있다.

## 6.4.1 그리드 서치를 사용한 하이퍼파라미터 튜닝
**GridSearchCV**함수를 이용하여 교차검증 폴드의 평균 점수를 이용하여 하이퍼파라미터를 튜닝시킨다.

그러나 GridSearchCV는 너무 많은 계산 비용이 드는데 이를 **RandomizedSearchCV**를 이용해 어느 정도 해결할 수 있다. 특히 매개변수 탐색 범위가 넓거나 연속적인 매개변수를 탐색해야 할 때 효과적이다.

**HalvingGridSearchCV**는 몇 개의 샘플에 대해서 교차검증을 하고 좋은 결과 나온 후보만을 골라서 다시 교차검증을 하는 방식이다. GridSearchCV보다는 결과가 아주 미세하게 안 좋을 수 있지만 계산 비용이 훨씬 저렴하다.

## 6.4.2. 중첩 교차 검증을 사용한 알고리즘 선택
일반 교차 검증이 훈련 셋과 테스트 셋으로 나누어 검증한 것이라면,
중첩 교차 검증은 나누어진 훈련셋과 테스트셋들에 대해서 훈련셋을 다시 검증폴드와 훈련폴드로 나누어 교차검증을 진행한다.
![](https://velog.velcdn.com/images/jinsoolve/post/7b81bf22-5390-4f08-96ca-2ad69322f331/image.png)
중첩 교차 검증이 당연히 일반 교차 검증보다 뛰어날 수 밖에 없다.

# 6.5 여러 가지 성능 평가 지표
주로 &apos;accuracy&apos; 정확도 지표를 사용하여 모델을 평가하는 것이 모델 성능에 도움이 된다. 
주어진 모델이 적합한지 측정할 수 있는 다른 성능 지표도 여럿 있다. 정밀도(precision), 재현율(recall), F1-점수이  있다.
## 6.5.1 오차 행렬
![](https://velog.velcdn.com/images/jinsoolve/post/d028993e-89b1-4898-8438-861cc7bdbaff/image.png)
오차행렬은 예측값과 실제값에 대한 비교 행렬이다.
아래와 같은 코드로 사용할 수 있다. 
![](https://velog.velcdn.com/images/jinsoolve/post/cbe78099-bd1c-4f3b-af3b-f3ceda524de7/image.png)
## 6.5.2 분류 모델의 정밀도와 재현율 최적화

- 정밀도(PRE)는 모델이 Positive라고 예측한 값 중 실제 Positive인 비율을 뜻한다.  
	- $\frac{TP}{예측P}$ = $\frac{TP}{TP+FP}$
- 재현율(REC)는 실제 Positive인 경우에 모델이 Positive라고 예측한 비율을 뜻한다.
	- $\frac{TP}{실제P}$ = $\frac{TP}{FN+TP}$

악성 종양 감지 문제에서 악성 종양:1, 양성 종양:0 이라고 하자.
정밀도를 최적화시키면 건강한 환자에게 악성 종양을 감지하는 일이 줄어들고,
재현율을 최적화시키면 아픈 환자에게 건강하다고 하는 일이 줄어든다.

이러한 PRE와 REC 최적화 균형을 맞추기 위해 이를 조합한 F1-score 지표를 자주 사용한다.
$$
F1 = 2\frac{PRE \times REC}{PRE + REC}
$$

![](https://velog.velcdn.com/images/jinsoolve/post/c24b94f0-1262-4d74-92fb-7d4d873f995b/image.png)

아래와 같이 make_scorer()를 이용해 scorer 변수를 만들 수 있다.
pos_label=0: 레이블이 0이 양성 클래스가 되도록 바꿔준다.
![](https://velog.velcdn.com/images/jinsoolve/post/e29a11df-c762-4103-a54e-9a3e12d05a2d/image.png)

## 6.5.3 ROC 곡선 그리기
![](https://velog.velcdn.com/images/jinsoolve/post/23a67ae0-e71c-461e-bb0c-025070af0cc0/image.png)
ROC 곡선은 FPR, TPR을 축으로 해서 그리는 그래프이다.
- FPR은 거짓 양성 비율 즉, $\frac{FP}{실제 N}$
- TPR은 진짜 양성 비율 즉, $\frac{TP}{실제 P}$

auc(Area Under the ROC Curve)는 ROC curve의 밑면적을 말한다. 해당 값이 높을수록 높은 정확도를 갖는다.
roc_curve 함수를 사용하면 fpr, tpr를 얻을 수 있다. 이를 그래프를 그리는 사용하거나 auc함수에 넘겨줘서 roc_auc 점수를 얻을 수 있다.

roc_auc_score 함수를 사용하면 바로 ROC-AUC 점수를 얻을 수 있다.

## 6.5.4 다중 분류의 성능 지표
이진 분류 말고 다중 클래스에 대한 분류를 할 때의 성능지표에 대해서 말하겠다.

micro 평균은 기존의 PRE를 k개의 클래스에 대한 PRE로 만든 것이고 이는 각 샘플이나 예측에 동일한 가중치를 부여하고자 할 때 사용된다.

macro 평균은 단순하게 클래스별 PRE의 평균값이다. 가중치가 적용된 macro 평균은 레이블마다 샘플 개수가 다른 불균형한 클래스를 다룰 때 유용하다.

# 6.6 불균형한 클래스 다루기
불균형한 클래스를 처리하지 않고 모델을 훈련시키면 학습 알고리즘 자체에 영향을 끼친다.

이를 소수 클래스에서 발생한 예측 오류에 큰 벌칙을 부여하는 방식으로 해결해 볼 수 있다. 대부분의 분류기에 구현된 **class_weight=&apos;balanced&apos;**로 설정해서 조정할 수 있다.

혹은 소수클래스의 샘플을 늘리거나(**업샘플링**) 다수클래스의 샘플을 줄이는(**다운샘플링**) 방식을 사용하여 해결해 볼 수 있다.
다음은 다운샘플링에 대한 코드다. resample 함수를 사용한다.  업샘플링을 하려면 y_imb == 0과 y_imb == 1의 값을 뒤바꿔주면 된다.

&lt;sub&gt;또는 인공적인 훈련 샘플을 만들어 볼 수 있다. SMOTE(Synthetic Minority Over-sampling TEchinque)를 사용해 볼 수 있다. imbalanced-learn 파이썬 라이브러리에 구현되어 있다.&lt;/sub&gt;

# 6.7 요약
- 모델 평가
	- pipeline
    - k-fold
- 모델 세부 튜닝
	- 학습곡선, 검증곡선 -&gt; 문제 분석
	- GridSearchCV
    - RandomizedSearchCV
    - 성능지표 (오차행렬, 정밀도, 재현율, ROC 곡선 ..)
- 불균형 데이터 처리
	- class_weight 매개변수
    - upsampling &amp; downsampling</content:encoded></item><item><title><![CDATA[표준화와 정규화]]></title><description><![CDATA[표준화와 정규화에 대해서 간단히 알아보자.]]></description><link>https://jinsoolve.netlify.app/posts/ml-textbook-4</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/ml-textbook-4</guid><pubDate>Wed, 08 Jan 2025 15:00:00 GMT</pubDate><content:encoded>
# 표준화
```python
ex = np.array([0, 1, 2, 3, 4, 5])

print(&apos;표준화:&apos;, (ex - ex.mean()) / ex.std())
```
평균=0, 분산=1로 만든다.

# MinMaxScaler()
```python
from sklearn.preprocessing import MinMaxScaler

mms = MinMaxScaler()
X_train_norm = mms.fit_transform(X_train)
X_test_norm = mms.transform(X_test)
    ```

$$
x_{norm}^{(i)} = \frac{x^{(i)}-x_{min}}{x_{max}-x_{min}}
$$

정해진 범위의 값이 필요할 때 유용하게 사용된다.

표준화는 이상치에 덜 민감한 반면,
최소-최대 스케일 변환은 이상치 정보가 유지된다.

# StandardScaler()

sklearn으로 표준화함수를 사용할 수  있다.

```python
from sklearn.preprocessing import StandardScaler

stdsc = StandardScaler()
X_train_std = stdsc.fit_transform(X_train)
X_test_std = stdsc.transform(X_test)
```

# RobustScaler()

이상치가 많은 작은 데이터셋을 다룰 때 좋다.
혹은 과대적합되기 쉽다면 이 scaler가 좋다.

$$
x_{norm}^{(i)} = \frac{x^{(i)}-q_2}{q_3-q_1}
$$

RobustScaler는 특성 열마다 독립적으로 작용하며 중간 값을 뺀 다음 25백분위수와 75백분위수를 사용해서 데이터셋의 스케일을 조정한다.

```python
from sklearn.preprocessing import RobustScaler
rbs = RobustScaler()
X_train_robust = rbs.fit_transform(X_train)
X_test_robust = rbs.fit_transform(X_test)
```

# MaxAbsScaler()

각 특성별로 데이터를 최대 절댓값으로 나눈다. 각 특성의 최대값을 1로 스케일링한다.

```python
from sklearn.preprocessing import MaxAbsScaler
mas = MaxAbsScaler()
X_train_maxabs = mas.fit_transform(X_train)
X_test_maxabs = mas.fit_transform(X_test)
```

# Normalizer()

특성이 아니라 샘플 별로 정규화시킨다.

```python
from sklearn.preprocessing import Normalizer

nrm = Normalizer()
X_train_l2 = nrm.fit_transform(X_train)
```</content:encoded></item><item><title><![CDATA[차원 축소를 사용한 데이터 압축]]></title><description><![CDATA[차원 축소 기법에 대해서 알아보자.]]></description><link>https://jinsoolve.netlify.app/posts/ml-textbook-6</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/ml-textbook-6</guid><pubDate>Wed, 08 Jan 2025 15:00:00 GMT</pubDate><content:encoded>
&gt; 이 글은 **머신러닝 교과서 with 파이썬, 사이킷런, 텐서플로**의 5장 내용을 기반으로 작성되었다.


차원 축소를 하는 이유는 다양하다. 저장 공간을 줄이고 계산 효율을 높이고, 차원의 저주 문제를 감소시키는 등의 역할을 한다. 또한 중요한 특성만을 추출하여 변환 및 투영을 하면 성능이 올라가는 경우가 많다.

차원 선택이 원본 특성을 중요도에 따라 선택한다면,
차원 추출은 새로운 특성 공간으로 데이터를 변환하거나 투영한다.
# 5.1 PCA
PCA는 주성분 분석을 통해서 비지도 데이터를 선형으로 차원 축소하는 기법이다.
PCA는 작은 차원으로 축소시키는 과정이며 이때 주성분을 선택해야 한다. 주성분은 모든 주성분들이 서로 직교한다는(=상관관계가 없다) 가정 하에 가장 큰 분산을 갖도록 선택한다.


## 5.1.1 PCA 과정
d차원 -&gt; k차원으로의 PCA의 과정은 다음과 같다.

1. d차원 원본 데이터 표준화
2. 공분산 행렬 생성
	공분산 행렬이란, 두 특성 간의 상관관계를 뜻한다.
    수식은 다음과 같다.
    $\sigma_{jk} = \frac{1}{n-1}\sum_{i=1}^n(x_j^{(i)} - \mu_j)(x_k^{(i)} - \mu_j)$
    여기서 공분산 행렬의 shape은  [d,d]인 square matrix이다.
3. 공분산 행렬을 고유값, 고유벡터로 분해
	고유값은 상수, 고유벡터는 행렬인데 공분산 행렬을 고유값과 고유벡터의 곱으로 나타내는 것이다.
    고유값, 고유벡터에 대한 정의는 복잡하고 재미도 없으니 넘어가자
4. 고유값을 내림차순으로 정렬 후 가장 높은 k개 선택
5. 선택한 k개 고유벡터로 투영행열 W 생성
6. 투영행렬 W를 이용해 원본 데이터를 k차원의 새로운 데이터 차원으로 변환

## 5.1.2 PCA 구현 코드
### 5.1.2.1 직접 구현
```py
# 데이터 표준화
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train_std = sc.fit_transform(X_train)
X_test_std = sc.transform(X_test)

# 공분산 구하고 공분산의 고유값, 고유벡터를 구한다.
import numpy as np
cov_mat = np.cov(X_train_std.T)
eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)

# (고윳값, 고유벡터) 튜플의 리스트를 만듭니다
eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i])
               for i in range(len(eigen_vals))]
               
# 높은 값에서 낮은 값으로 (고윳값, 고유벡터) 튜플을 정렬합니다
eigen_pairs.sort(key=lambda k: k[0], reverse=True)

# 여기서는 2개의 특성을 골라서 수평으로 쌓아서 투영행렬 w를 구한다.
w = np.hstack((eigen_pairs[0][1][:, np.newaxis],
               eigen_pairs[1][1][:, np.newaxis]))
               
# 투영행렬 w를 이용해 pca로 차원축소 시키다.
X_train_pca = X_train_std.dot(w)
```
### 5.1.2.2 사이킷런 구현
```py
from sklearn.decomposition import PCA

# 여기서도 데이터를 표준화시켜야 한다.
# X_train_std는 표준화된 데이터다.

pca = PCA()
X_train_pca = pca.fit_transform(X_train_std)
```
PCA( n_componets=[0~1사이의 값] ) 매게변수를 줄 수 있는데 이는 설명된 분산의 비율을 지정해준 값에 맞게 주성분 개수를 선택한다.
&gt; 여기서 말하는 (설명된 분산 비율) = $\frac{(해당 \,주성분 \, 분산)}{(모든 \,주성분의 \,분산 \,합)}$ 이다. 
예를 들어 정렬된 주성분의 설명된 분산 비율이 차례로 0.4, 0.3, 0.2, 0.1 일때 
n_components = 0.9이라면 [0.4, 0.3, 0.2]인 주성분을 선택한다.

# 5.2 LDA
LDA는 지도 데이터를 선형으로 차원 축소하는 기법이다.
- PCA가 전체 데이터의 분산이 최대가 되도록 주성분을 설정한다고 한다면,
- LDA는 클래스 간 분산(between-class variance)과 클래스 내 분산(within-class variance)을 최대화하고 최소화하려고 노력한다. 
이렇게 하면 서로 다른 클래스 간의 거리가 최대화되며, 같은 클래스 내의 데이터 포인트 간의 거리는 최소화된다.
![](https://velog.velcdn.com/images/jinsoolve/post/47fe810f-fd31-4093-9734-9fc0ef5f3529/image.png)

참고로 LDA가 좀 더 분류에 최적화되어 있다고 한다.
그럼 이제 LDA의 과정과 구현을 알아보자.
## 5.2.1 LDA 과정
d차원의 원본데이터를 k차원의 데이터로 LDA 차원축소한다고 하자.

1. 원본 데이터 표준화
2. 각 클래스에 대해 d차원의(=특성 d개에 대한) 평균벡터 $m$ 계산
3. 클래스 간 산포행렬 $S_b$와 클래스 내 산포행렬 $S_w$를 계산
	산포행렬이란 데이터들 간의 분산과 공분산을 나타내는다. 이를 통해 데이터 분포를 알 수 있다.
    	수식은 다음과 같다. 
        $S_i = \sum_{x \in D_i}(x-m_i)^T(x-m_i)$, (여기서 $m_i$는 평균벡터다.)
4. $S_w^{-1}S_b$ 행렬의 고유값, 고유벡터 계산
5. 고유값 내림차순 정렬하여 가장 큰 k개의 고유벡터를 선택
6. 선택한 k개의 고유벡터로 변환 행렬 W 생성
	여기서 W의 각 열이 고유벡터가 된다. 즉 $d \times k$ 차원이다.
7. 변환 행렬 W로 원본 데이터 차원 축소
## 5.2.2 LDA 구현
### 5.2.2.1 직접 구현
```py
# 평균 벡터 구하기
mean_vecs = []
for label in range(1, 4):
    mean_vecs.append(np.mean(X_train_std[y_train == label], axis=0))
    
# 클래스 내 산포행렬 계산 
# (row - mv).dot((row - mv).T) 수식을 이용해서 계산했는데,
# 클래스가 균일하게 분포되어 있지 않을 경우 공분산을 사용하는 게 더 낫다.
d = 13 # 특성의 수
S_W = np.zeros((d, d))
for label, mv in zip(range(1, 4), mean_vecs):
    class_scatter = np.zeros((d, d))  # 각 클래스에 대한 산포 행렬
    for row in X_train_std[y_train == label]:
        row, mv = row.reshape(d, 1), mv.reshape(d, 1)  # 열 벡터를 만듭니다
        class_scatter += (row - mv).dot((row - mv).T)
    S_W += class_scatter  
    
# 공분산을 이용한 클래스 내 산포행렬 계산
d = 13  # 특성의 수
S_W = np.zeros((d, d))
for label, mv in zip(range(1, 4), mean_vecs):
    class_scatter = np.cov(X_train_std[y_train == label].T)
    S_W += class_scatter
      

# 클래스 간 산포행렬 계산
mean_overall = np.mean(X_train_std, axis=0)
mean_overall = mean_overall.reshape(d, 1)  # 열 벡터로 만들기
d = 13  # 특성 개수
S_B = np.zeros((d, d))
for i, mean_vec in enumerate(mean_vecs):
    n = X_train_std[y_train == i + 1, :].shape[0]
    mean_vec = mean_vec.reshape(d, 1)  # 열 벡터로 만들기
    S_B += n * (mean_vec - mean_overall).dot((mean_vec - mean_overall).T)

# 산포행렬의 고유값, 고유벡터 계산
eigen_vals, eigen_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))

# 고유값 내림차순으로 정렬
eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i])
               for i in range(len(eigen_vals))]
eigen_pairs = sorted(eigen_pairs, key=lambda k: k[0], reverse=True)

# 변환행렬 w 생성
w = np.hstack((eigen_pairs[0][1][:, np.newaxis].real,
              eigen_pairs[1][1][:, np.newaxis].real))
              
# LDA로 차원축소 적용하기
X_train_lda = X_train_std.dot(w)

```
### 5.2.2.2 사이킷런 구현
```py
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

# X_train_std는 정규화된 데이터다. 
# LDA는 정규화를 가정하고 하는 거라 반드시 해주는 것이 좋다.

# n_components LDA로 축소될 차원을 뜻한다.
# 여기서는 2차원으로 축소했다.
lda = LDA(n_components=2)
X_train_lda = lda.fit_transform(X_train_std, y_train)
```
# 5.3 커널 PCA (KPCA)
비선형적으로 분표되어 있는 데이터를 차원축소하기 위해 KPCA를 사용한다.

간단하게 설명하자면, 

1. 비선형적으로 분포되어 있는 데이터를 선형적으로 분리할 수 있는 고차원으로 데이터를 투영시킨다.
2. 고차원으로 투영된 데이터를 일반 PCA를 이용해 원하는 작은 차원으로 축소시킨다.


커널 svm과 마찬가지로 이런 방식으로 비선형 데이터 문제를 해결할 수 있다. 
다만, 여기서 문제가 고차원으로 투영시키는데 드는 계산 비용이 어마무시하다는 것이다. 이를 해결하기 위해 등장한 것이 **커널 기법(kernel trick)**이다.
## 5.3.1 커널 기법 (Kernel Trick)

~~커널 기법은 흑마법이다.~~ 그니까 증명을 읽었는데도 이해가 안 된다는 얘기다. ~~흑마법처럼 그냥 받아들이면 편하다.~~
그래도 최대한 요약해서 이해하기 쉽게 풀어보겠다.

데이터를 고차원으로 투영시키려면 &lt;u&gt;1) 데이터를 고차원 데이터로 변환&lt;/u&gt;한 후 &lt;u&gt;2) 변환된 데이터를 내적&lt;/u&gt;시켜야 한다. 이 과정에서 어마무시한 계산 비용이 소비된다. 
그러나 커널 기법을 사용하면 이렇게 계산을 직접적으로 하지 않고서 같은 결과를 얻을 수 있다.

&gt; 비유를 하자면, 우리가 1 ~ n 합을 구할 때 1부터 n까지 일일이 다 더해서 구할 수도 있지만 $\frac{n(n+1)}{2}$로 한 번에 계산할 수도 있다. &lt;sub&gt;아마 이게 커널 기법과 같은 메커니즘이 아닐까 한다.&lt;/sub&gt;

여기서 커널 기법을 우리가 직접 구현할 필요가 없고 이미 증명된 커널 기법을 사용하면 된다.

- 다항 커널
- 시그모이드 커널
- 가우시안 커널 (방사 기저 함수, RBF)

## 5.3.2 KPCA 파이썬으로 직접 구현
```py
from scipy.spatial.distance import pdist, squareform
from numpy import exp
from scipy.linalg import eigh
import numpy as np

def rbf_kernel_pca(X, gamma, n_components):
    &quot;&quot;&quot;
    RBF 커널 PCA 구현

    매개변수
    ------------
    X: {넘파이 ndarray}, shape = [n_samples, n_features]
        
    gamma: float
      RBF 커널 튜닝 매개변수
        
    n_components: int
      반환할 주성분 개수

    Returns
    ------------
     alphas: {넘파이 ndarray}, shape = [n_samples, k_features]
       투영된 데이터셋
     
     lambdas: list
       고윳값

    &quot;&quot;&quot;
    # MxN 차원의 데이터셋에서 샘플 간의 유클리디안 거리의 제곱을 계산합니다.
    sq_dists = pdist(X, &apos;sqeuclidean&apos;)

    # 샘플 간의 거리를 정방 대칭 행렬로 변환합니다.
    mat_sq_dists = squareform(sq_dists)

    # 커널 행렬을 계산합니다.
    K = exp(-gamma * mat_sq_dists)

    # 커널 행렬을 중앙에 맞춥니다.
    N = K.shape[0]
    one_n = np.ones((N, N)) / N
    K = K - one_n.dot(K) - K.dot(one_n) + one_n.dot(K).dot(one_n)

    # 중앙에 맞춰진 커널 행렬의 고윳값과 고유 벡터를 구합니다.
    # scipy.linalg.eigh 함수는 오름차순으로 반환합니다.
    eigvals, eigvecs = eigh(K)
    eigvals, eigvecs = eigvals[::-1], eigvecs[:, ::-1]

    # 최상위 k 개의 고유 벡터를 선택합니다(투영 결과).
    alphas = np.column_stack([eigvecs[:, i]
                              for i in range(n_components)])

    # 고유 벡터에 상응하는 고윳값을 선택합니다.
    lambdas = [eigvals[i] for i in range(n_components)]

    return alphas, lambdas
```
## 5.3.2.1 새로운 데이터 포인트 투영
훈련 데이터셋 외의 다른 데이터셋(훈련 데이터셋, 검증 데이터셋 등)을 모델에 넣을 때 마찬가지로 KPCA 처리하여 투영한 후에 넣어야 한다. 이때 위 코드에서 반환한 고윳값 $\lambda$를 이용하여 KPCA 처리해 준다.
코드는 다음과 같다.
```py
def project_x(x_new, X, gamma, alphas, lambdas):
    pair_dist = np.array([np.sum((x_new - row)**2) for row in X])
    k = np.exp(-gamma * pair_dist)
    return k.dot(alphas / lambdas)
```

## 5.3.3 KPCA 사이킷런 구현
```py
from sklearn.decomposition import KernelPCA

# 반달 모양 예시 데이터를 생성한다.
X, y = make_moons(n_samples=100, random_state=123)

# 사이킷런의 KernelPCA를 수행한다.
scikit_kpca = KernelPCA(n_components=2, kernel=&apos;rbf&apos;, gamma=15)
# 이를 데이터에 적용시킨다.
X_skernpca = scikit_kpca.fit_transform(X)
```
# 요약
특성 추출을 위한 세 개의 기본적인 차원 축소 기법에 대해서 다뤄봤다. 기본 PCA, LDA, 커널 PCA이다.

- 기본 PCA는 클래스 레이블 없이 직교하는 특성 축을 따라 분산이 최대가 되는 저차원으로 데이터를 투영한다.
- LDA는 클래스 레이블을 사용하여 선형 특성 공간에서 클래스 구분 능력을 최대화한다.
- 커널 PCA는 커널 트릭과 고차원 특성 공간으로의 가상 투영을 통하여 비선형 데이터셋을 저차원으로 극적으로 압축한다. 이를 통해 비선형 데이터를 선형적으로 구분할 수 있게 만든다.
</content:encoded></item><item><title><![CDATA[데이터 전처리]]></title><description><![CDATA[데이터 전처리에 대해서 정리해 보았다.]]></description><link>https://jinsoolve.netlify.app/posts/ml-textbook-5</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/ml-textbook-5</guid><pubDate>Wed, 08 Jan 2025 15:00:00 GMT</pubDate><content:encoded>해당 내용은 **머신러닝 교과서 with 파이썬, 사이킷런, 텐서플로**의 4장 내용을 기반으로 작성되었다.
# 4.1 누락된 데이터 다루기
- isnull().sum()등을 이용하여 누락된 값을 확인한다.

- 누락된 값이 있는 샘플(행)이나 특성(열)을 제거한다.

- 제거하면 데이터가 너무 적어질 수 있으니 대체를 시도하는 게 일반적이다.
누락값 대체는 fillna나 SimpleImputer를 주로 사용해서 대체한다. 
SimpleImputer에는 axis 파라미터가 없으므로 행 방향을 다루고 싶다면 FunctionTransformer를 사용할 수 있다.
누락값은 대체로 평균값이나 최빈값으로 대체해준다.
# 4.2 범주형 데이터 다루기
범주형 데이터를 바꿀 때는 &lt;u&gt;1) 순서가 있는 특성&lt;/u&gt;과 &lt;u&gt;2) 순서가 없는 특성&lt;/u&gt; 을 나눠서 다뤄야 한다.
## 4.2.1 순서가 있는 특성
size = M, L, XL 처럼 순서가 있는 특성들이 있다. 파이썬의 dictionary를 이용해 매핑해준다.
```python
size_mapping = {&apos;XL&apos;: 3,
                &apos;L&apos;: 2,
                &apos;M&apos;: 1}

df[&apos;size&apos;] = df[&apos;size&apos;].map(size_mapping)
df
```
### 클래스 레이블 인코딩
간혹, 클래스 레이블(=타깃값)이 정수로 되어 있지 않는 경우도 있다. 마찬가지로 매핑으로 해결한다.
```py
import numpy as np

# 클래스 레이블을 문자열에서 정수로 바꾸기 위해
# 매핑 딕셔너리를 만듭니다
class_mapping = {label: idx for idx, label in enumerate(np.unique(df[&apos;classlabel&apos;]))}
class_mapping

# 클래스 레이블을 문자열에서 정수로 바꿉니다
df[&apos;classlabel&apos;] = df[&apos;classlabel&apos;].map(class_mapping)
df
```
```py
from sklearn.preprocessing import LabelEncoder

# 사이킷런의 LabelEncoder을 사용한 레이블 인코딩
class_le = LabelEncoder()
y = class_le.fit_transform(df[&apos;classlabel&apos;].values)
y
```
2가지 방식으로 클래스 레이블 인코딩이 가능하다
## 4.2.2 순서가 없는 특성
color = blue, green, red 처럼 순서가 없는 특성들이 있다.
하지만 순서가 있는 특성처럼 매핑하면 값의 크기 차이가 생기고 이는 데이터의 노이즈로 이어질 수 있기 때문에 사용하지 않는다.
이는 one-hot encoding 을 이용해 해결한다. 

&lt;Callout type=&quot;info&quot;&gt;
**One-Hot Encoding이란?**

이진법을 사용해 encoding하는 기법이다.
color = blue, green, red를 one-hot encoding하면, color_blue, color_green, color_red 특성이 만들어진다. 
그래서 [color_blue, color_green, color_red]의 값이 [1,0,0] = blue, [0,1,0] = green, [0,0,1] = red가 되는 것이다. 
그런데 이때 2개만 있어도 색 표현이 가능하다. [1,0] = blue, [0,1] = green, [0,0] = red 이렇게 생각하면 되기 때문이다. 
다중 공산성으로 인해서 1개를 삭제해 주는 것이 좋다.
(다중 공산성이란, 특성 간의 상관관계가 높아서 (역행렬을 계산하기 어려워진다) 계산이 불안정해지는 현상을 뜻한다.)
&lt;/Callout&gt;


```py
# 하나의 열을 변경할 때
from sklearn.preprocessing import OneHotEncoder

X = df[[&apos;color&apos;, &apos;size&apos;, &apos;price&apos;]].values
color_ohe = OneHotEncoder()
color_ohe.fit_transform(X[:, 0].reshape(-1, 1)).toarray()

# 여러 개의 열 변경할 때
from sklearn.compose import ColumnTransformer

X = df[[&apos;color&apos;, &apos;size&apos;, &apos;price&apos;]].values
c_transf = ColumnTransformer([ (&apos;onehot&apos;, OneHotEncoder(), [0]),
                               (&apos;nothing&apos;, &apos;passthrough&apos;, [1, 2])])
c_transf.fit_transform(X)
```
근데 이것보다는 pandas의 get_dummies() 함수를 사용하는 것이 훨씬 쉽다.
```py
# columns=[&apos;size&apos;]: 매개변수로 변환하려는 특성을 구체적으로 정할 수 있다.
# drop_first=True: one-hot encoding으로 생성되는 첫번째 특성을 삭제한다. 
pd.get_dummies(df[[&apos;price&apos;, &apos;color&apos;, &apos;size&apos;]], columns=[&apos;size&apos;], drop_first=True)
```
# 4.3 데이터셋을 훈련 데이터셋과 테스트 데이터셋으로 나누기
```py
from sklearn.model_selection import train_test_split

X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values

# test_size=0.3: train:test = 70:30으로 나눈다.
# stratify=y: 각 클래스 레이블의 분포가 최대한 균일하도록 데이터를 분할한다.
X_train, X_test, y_train, y_test =\
    train_test_split(X, y, 
                     test_size=0.3, 
                     random_state=0, 
                     stratify=y)
```
# 4.4 특성 스케일 맞추기
- StandardSclaer
	$x_{standard}^{(i)} = \dfrac{x^{(i)} - \mu}{\sigma}$
- RobustScaler
	$x_{robust}^{(i)} = \dfrac{x^{(i)} - q_2}{q_3 - q_1}$
- MinMaxScaler
	$x_{norm}^{(i)} = \frac{x^{(i)}-x_{min}}{x_{max}-x_{min}}$
- MaxAbsScaler
	각 특성별로 데이터를 최대 절댓값으로 나눈다. 각 특성의 최대값을 1로 스케일링한다.
- Normalizer
	주로 거리 기반 데이터에 사용되고 L1, L2 norm기반으로 [0,1] 범위로 스케일링한다.
	L1은 차이의 절댓값을, L2는 차이의 제곱합에 루트를 씌운 값을 뜻한다.
# 4.5 유용한 특성 선택
과대적합을 해결할 수 있는 방법으로는 다음과 같다.
- 더 많은 훈련데이터 수집
- 규제로 복잡도 제한
- (파라미터가 적은) 간단한 모델 채택
- 데이터 차원 감소

훈련 데이터를 더 수집하는 것은 불가능할 때가 많다. 특성 선택을 통해 차원을 축소하면 데이터에 학습되는 파라미터가 줄어들어 모델이 간단해진다. 
## 4.5.1 L1 규제
![](https://velog.velcdn.com/images/jinsoolve/post/0690f8c1-9b9f-42f5-9a38-b4f5a876f565/image.png)L1은 1차식이기 떄문에 다이아몬드 형태로 그려진다. 그렇기 때문에 축에 가깝게 최적점이 형성되고 이것이 희소성(데이터가 0이나 매우 작은 값으로 구성)이 나타나는 이유다.
## 4.5.2 L2 규제
![](https://velog.velcdn.com/images/jinsoolve/post/6fa756ed-fcf5-459f-b2c1-9418cdcba7d6/image.png)$\lambda$가 커질수록 규제가 강해지므로 이는 원을 작게 만든다. 
## 4.5.3 순차 특성 선택 알고리즘
그리디하게 관련이 가장 높은 특성을 선택하는 알고리즘이다.
```py
from sklearn.feature_selection import SequentialFeatureSelector

sfs = SequentialFeatureSelector(knn, n_features_to_select=n_features, n_jobs=-1)
sfs.fit(X_train_std, y_train)
f_mask = sfs.support_ #선택된 특성의 True, False가 저장되어 있음
knn.fit(X_train_std[: f_mask], y_train)
```
원래는 직접 Class를 작성했는데 최근에 추가된 SequentialFeatureSelector를 사용하면 편할 듯 하다.
# 4.6 랜덤 포레스트의 특성 중요도 사용
RandomForestClassifer 에서는 모든 결정 트리에 불순도 감소로 특성 중요도 측정 가능하다.
이는 RandomForestClassifer 모델을 훈련한 후 feature\_importances_ 속성에서 확인 가능하다.

랜덤 포레스트로 모델 해석을 한다면 주의해야 할 점이 있다.
두 개 이상의 특성이 상관관계가 매우 높다면 하나의 특성은 매우 높은 순위를 갖지만 다른 특성 정보는 잡아내지 못할 수 있다.
## 4.6.1 SelectFromModel
학습된 랜덤 포레스트의 특성 중요도를 이용하여 특성을 골라낸다. threshold를 설정해서 그 이상으로 크다면 이를 골라낸다.
```py
from sklearn.feature_selection import SelectFromModel

sfm = SelectFromModel(forest, threshold=0.1, prefit=True)
X_selected = sfm.transform(X_train)
print(&apos;이 임계 조건을 만족하는 샘플의 수:&apos;, 
      X_selected.shape[1])
```
## 4.6.2 Recursive Feature Elimination
RFE를 사용하여 선택한 특성의 개수가 남을 때까지 재귀적으로 삭제한다.
rfe.ranking\_ 속성에는 기반모델이 선택한 특성의 우선순위 값이 들어있다.
훈련된 기반 모델(random forest)는 estimator_ 속성에 있다.
rfe.estimator_.importances\_에는 중요도 값이 들어있다.
```py
from sklearn.feature_selection import RFE

rfe = RFE(forest, n_features_to_select=5)
rfe.fit(X_train, y_train)

rfe.ranking_ # 기반모델이 선택한 중요도 순위가 저장되어 있다. 1을 가지면 선택된 것이다.
f_mask = rfe.support_ # 선택된 특성에 대한 True, False
importances = rfe.estimator_.feature_importances_ # 중요도 값이 저장되어 있다.
indices = np.argsort(importances)[::-1]
```</content:encoded></item><item><title><![CDATA[15장 심층 합성곱 신경망으로 이미지 분류]]></title><description><![CDATA[합성곱 신경망은 컴퓨터 비전을 위한 머신러닝 분야를 크게 발전시켰다.CNN(Convolutional Neural Network)는 뇌의 시각 피질이 물체를 인식할 때 동작하는 방식에서 영감을 얻었다고 한다.]]></description><link>https://jinsoolve.netlify.app/posts/ml-textbook-14</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/ml-textbook-14</guid><pubDate>Wed, 08 Jan 2025 15:00:00 GMT</pubDate><content:encoded>합성곱 신경망은 컴퓨터 비전을 위한 머신러닝 분야를 크게 발전시켰다.
CNN(Convolutional Neural Network)는 뇌의 시각 피질이 물체를 인식할 때 동작하는 방식에서 영감을 얻었다고 한다.

CNN은 low-level feature을 추출하는 것부터 시작해서 뒤쪽의 층으로 갈수록 high-level feature를 추출한다. 이미지를 픽셀 패치 단위로 나누어서 각 특징에 대해 학습하고 모든  가중치에 대해 업데이트 한다.
이는 희소 연결과 파라미터 공유 특성으로 인해 이미지 분류에 있어서 꽤나 효과적이다. 

CNN에서 이 특징을 추출하는 방식은 합성곱이다. 합성곱층과 서브샘플링(풀링)층, 완전 연결(FC)층을 이용해서 특징을 추출하고 파라미터를 학습시킨다.
## 합성곱
![](https://velog.velcdn.com/images/jinsoolve/post/d72b22f5-dc43-430f-965e-6adb7e3d8bd9/image.png)
위 그림은 3D 합성곱에 대한 설명이다. 합성곱은 padding, stride 값을 이용해 결과 층의 차원을 조절할 수 있다.
- padding은 입력층의 행과 열을 얼마나 늘릴지를 정하고 늘린 열과 층은 모두 0으로 채운다.
- stride는 합성곱을 할 때 몇 칸씩 건너뛰면서 계산할 지를 정한다.

![](https://velog.velcdn.com/images/jinsoolve/post/6c07df95-e323-42fa-9bfd-66567ec9d578/image.png)

결과 층의 차원은 다음과 같다.

## 서브샘플링
서브샘플링에는 두 종류의 풀링 연산이 있다. 
max-pooling, average-pooling(mean-pooling)

전통적으로 풀링은 겹치지 않는다고 가정한다. 따라서 스트라이드의 크기를 풀링의 크기와 같이 설정한다.
특성 맵의 크기를 줄이기 위해 풀링층을 사용하는 대신 스트다리이드 2인 합성곱 층을 사용하기도 한다.

## 드롭아웃
은닉층의 특정 은닉유닛을 랜덤하게 비활성화시켜 과대적합을 막는 기법이다. 주로 뒤쪽의 은닉층에 적용한다.
드롭아웃의 랜덤성에 따른 모델의 결과차이로 인해 앙상블 모델의 효과를 내기도 한다.(하지만 드롭아웃과 앙상블과의 관계가 아주 분명하지는 않다. 다만 그렇게 해석될 여지가 있다 정도로 보자.)

## 손실함수
![](https://velog.velcdn.com/images/jinsoolve/post/90f74a8b-ef16-4fc7-9238-89d2d9924be0/image.png)
분류를 위한 손실함수는 위와 같은 종류가 있다.

## Data augmentation
데이터 증식 기법은 이미지 분류 모델에서 데이터의 양을 늘려서 훈련성능을 높이기 위해 사용하는 기법이다.
데이터를 자르거나 뒤집거나, 대비, 명도, 채도 등을 바꾼다.
tf.image에서 crop_to_bounding_box(), flip_left_right(), adjust_contrast(), adjust_brightness() 등 다양한 augmentation 함수들이 있다.


---
모델을 형성하는 건 Sequential API를 이용해서 했다.</content:encoded></item><item><title><![CDATA[16장 순환 신경망으로 순차 데이터 모델링]]></title><description><![CDATA[데이터 간의 순서가 중요한 데이터를 시퀀스 데이터라고 한다. 시퀀스 데이터를 처리해주기 위해서는 RNN(Recurrent Neural Network) 모델을 사용해서 처리해야 한다.]]></description><link>https://jinsoolve.netlify.app/posts/ml-textbook-15</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/ml-textbook-15</guid><pubDate>Wed, 08 Jan 2025 15:00:00 GMT</pubDate><content:encoded>데이터 간의 순서가 중요한 데이터를 시퀀스 데이터라고 한다. 
시퀀스 데이터를 처리해주기 위해서는 RNN(Recurrent Neural Network) 모델을 사용해서 처리해야 한다.

# 시퀀스 데이터
Sequence 데이터(순차 데이터)는 말 그대로 데이터들 간의 의존성(순서)이 있는 데이터다.

시계열 데이터(time series)은 이러한 seq 데이터의 한 종류다. time step 즉 시간 차원이 데이터 들 간의 순서를 결정하는 데이터다.
시계열이 아닌 seq 데이터로는 텍스트, DNA 등이 있다.

## 시퀀스 모델링
![](https://velog.velcdn.com/images/jinsoolve/post/506a432b-8ac1-40db-ba6a-60ee0ba68fef/image.png)

- 다대일
- 일대다
- 다대다

가 있다. 입력이나 출력 둘 중 하나가 시퀀스가 아니라 벡터나 스칼라 데이터일 수 있다.

데이터들끼리의 연관이 있는 시퀀스 데이터인 만큼 모델에도 이전 정보를 기억하고 연결시켜주는 모델이 필요하다. 이것이 RNN이다.

---
# RNN
Recurrent Neural Network의 약자로, 재귀적으로 순환하는 신경망이다.
![](https://velog.velcdn.com/images/jinsoolve/post/bbe052d1-4c35-4e1e-a0db-8c964c1f4161/image.png)

![](https://velog.velcdn.com/images/jinsoolve/post/014c1bbc-cbef-4763-9c55-5a22e140ab0d/image.png)

이런 식으로 하나의 은닉층이 입력층과 동시에 이전 은닉층의 값을 받는다.

계산은 다음과 같이 이루어진다.
![](https://velog.velcdn.com/images/jinsoolve/post/61c4eb91-d652-407c-ad08-e28be804c2a4/image.png)

![](https://velog.velcdn.com/images/jinsoolve/post/60baffa1-8222-44e9-b9e4-74e722edae30/image.png)

이 계산을 한번에 하기 위해 벡터로 연결하여 계산하면 다음과 같다.
![](https://velog.velcdn.com/images/jinsoolve/post/8ba64ac3-ce8f-481d-97d4-bd0879fa9973/image.png)

![](https://velog.velcdn.com/images/jinsoolve/post/850c3a3b-f863-4186-a0dd-8f4a2ee4729e/image.png)


반면에, 은닉층이 아니라 출력층이 순환될 수도 있다. 아래 그림을 보자.
![](https://velog.velcdn.com/images/jinsoolve/post/73fb4bb9-f7ed-4beb-901f-f10571ac3afa/image.png)

## LSTM(Long Short-Term Memory)
RNN의 문제가 시퀀스 데이터를 받다보니 모델이 깊어질 수 밖에 없는데, 모델이 깊어지면 따라오는 문제가 바로 Gradient Vanishing(or Exploding) Problem 이다.

이를 해결하기 위해 등장한 모델이 LSTM이다.

LSTM은 은닉층의 가중치의 값을 W=1로 적절히 조절하기 위한 메모리 셀이 존재한다.
이 메모리셀에는 다음과 같은 게이트들이 있다.

- Input Gate
	현재 입력에 대한 정보를 얼마나 셀 상태에 저장할지 결정
- Forget Gate
	이전 상태의 정보 중 어떤 정보를 삭제할지 결정
- Output Gate
	현재 상태의 셀 정보를 다음 순서로 얼마나 전달할지 결정
    
시그모이드 함수와 tanh 함수를 사용하여 활성화하거나 차단된다.
2014년에 GRU라는 새로운 방식이 소개되었다. LSTM 보다 구조가 단순하여 계산 효율성이 높다. 동시에 일부 작업의 성능은 LSTM과 견줄 만하다.

---
# 텐서플로우로 RNN 구현
리뷰 문장을 가져와서 평점을 계산하는 모델을 구현해보겠다. 모델 구현의 순서는 다음과 같다.

1. 데이터셋(문장)을 불러온다.
2. 문장을 토큰화시킨다.(토큰: 의미가 있는 말의 단위)
3. 토근화된 데이터들을 인코딩한다.(인코딩: 모델이 학습하기 좋은 수치형 데이터로 전환)
4. mini batch로 size가 같도록 나눈다.
5. Sequential클래스로 양방향 LSTM을 구축한다.
6. compile &amp; fit
7. evaluate

문자형 데이터를 인코딩할 때 one-hot encoding을 사용한다. 하지만 one-hot encoding은 0이 굉장히 많게 인코딩되므로 차원이 커지고 이는 곧 차원의 저주()로 이어지고, 동시에 0이 대다수이므로 값이 sparse해지기 때문에 gradient vanishing을 야기시킨다.

이를 해결하기 위해 Embedding 기법을 사용한다. 임베딩은 실수값을 가진 고정된 길이의 벡터로 변환하는 기법이다.

tf.keras.layers.Embedding 을 사용한다.

## mini-batch와 padding
여러 요인으로 인해 계산 효율성을 높이고 성능도 좋게 하기 위해서는 batch(훈련데이터)를 작은 mini-batch로 나눈다. 
이떄 mini-batch화된 시퀀스 데이터를 효율적으로 텐서에 저장하기 위해서는 동일한 길이가 되어야 한다. 이를 위해 가장 긴 차원의 데이터의 길이로 padding을 해서 같은 길이로 만든다.

tensorflow의 padded_batch() 함수를 사용한다.

---
# Transformer 모델
지금까지 배운 모델들은 모두 seq2seq을 기반으로 구현한 모델들이다. 하지만 2017년에 NeurIPS 논문에 Transformer라는 모델이 소개되었는데 성능이 굉장히 좋다.

Transformer의 핵심 원리는 입력과 출력 시퀀스 사이의 전역 의존성(어텐션)을 모델링하는 것이다.
쉽게 얘기하면 입력 단어들 간의 의존성 즉, 관계(혹은 서로에 대한 중요도)를 가중치로 계산하여 출력 시퀀스를 만드는 것이다.
과정은 다음과 같다.

1. 입력 시퀀스들 간의 중요도 가중치 계산
2. 소프트맥스 함수로 가중치 정규화
3. 입력 시퀀스에 대한 가중치 합으로 출력 시퀀스 계산

이러한 기법을 셀프 어텐션 메커니즘이라 한다.

## 고급 셀프 어텐션 메커니즘
이러한 셀프 어텐션 메커니즘을 여러 기법으로 좀 더 강화시킬 수 있다. 한 번 살펴보자.

### query, key, value를 가진 셀프 어텐션 메커니즘
기본적인 셀프 어텐션은 출력을 계산할 때 학습되는 파라미터를 전혀 사용하지 않았다.
이를 query, key, value 시퀀스를 이용하여 모델을 업데이트 시키는 메커니즘이다.
좀 더 자세히 설명해보면, 기존의 중요도 가중치 즉, 시퀀스간의 어떤 정도로 연관이 되어 있는지의 값을 해당 값을 통해 학습시키고 이를 조정한다.

### 멀티-헤드 어텐션과 Residual Connection
여러 개의 셀프 어텐션 연산을 합친 멀티 헤드 어텐션이 있다.
![](https://velog.velcdn.com/images/jinsoolve/post/30186472-31c5-405d-8b67-74566f3bf2a4/image.png)

Residual Connection(or Skip Connection)은 깊은 신경망에서 얕은 레이어에서 깊은 레이어로 값을 전달하여 가중치를 조정하는 것이다. 이는 그레디언트 소실 문제를 완화시키고 학습을 좀 더 효율적으로 할 수 있도록 도와준다.

위의 멀티-헤드에서도 Res Connect이 사용되는데 이를 이용해 여러 개의 셀프 어텐션의 결과를 합친다.</content:encoded></item><item><title><![CDATA[13장 텐서플로를 사용한 신경망 훈련]]></title><description><![CDATA[복잡한 수학이나 구현 과정을 텐서플로우에서 이미 구현해 놓았다. 이를 사용하는 법을 알아보자.텐서플로우의 함수나 여러가지 기능들은 어느정도 생략하겠다. 사용하면서 익히는 것이 가장 좋다.]]></description><link>https://jinsoolve.netlify.app/posts/ml-textbook-13</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/ml-textbook-13</guid><pubDate>Wed, 08 Jan 2025 15:00:00 GMT</pubDate><content:encoded>복잡한 수학이나 구현 과정을 텐서플로우에서 이미 구현해 놓았다. 이를 사용하는 법을 알아보자.

텐서플로우의 함수나 여러가지 기능들은 어느정도 생략하겠다. 사용하면서 익히는 것이 가장 좋다.

# 13.3 텐서플로 데이터셋 API를 사용하여 입력 파이프라인 구축
Keras는 Tensorflow의 Wrapper이다. 텐서플로우 내의 구현된 keras를 이용하여 텐서플로우를 보다 쉽게 이용할 수 있다.

batch() 메소드를 이용하여 미니배치로 나누어 훈련시킨다. 훈련시킬 때는 fit() 메소드를 이용하여 훈련시킨다.

---

# 13.4 텐서플로로 신경망 모델 만들기
tf.keras.Sequential() 클래스를 이용해서 모델을 만들 수 있다. add() 메서드를 이용해서 층을 추가할 수 있다.
이외에도 tf.keras.Model() 클래스를 상속하여 모델을 정의할 수 있다. 

마지막으로 keras의 compile(), fit() 메서드를 사용하여 컴파일하고 훈련할 수 있다.

## tf.keras.Model()
밑바닥에서부터 모델을 재정의하여 사용할 수 있다. 불편하긴 하지만 그만큼 내가 원하는 방향으로 수정할 수 있다.
![](https://velog.velcdn.com/images/jinsoolve/post/0ea2acb7-91ec-4192-b248-2aaf8b412b6b/image.png)
위 그림처럼 객체를 만든 후 build() 메서드를 이용하여 모델 층과 파라미터를 마드는 방법을 변수 지연 생성이라고 한다.

compile과 fit 메서드를 사용해 보겠다.
![](https://velog.velcdn.com/images/jinsoolve/post/36f3da52-1e76-4960-9b8c-c1e9dbd35533/image.png)
![](https://velog.velcdn.com/images/jinsoolve/post/8d360bfe-b44e-4701-b7e9-2f292ea61300/image.png)
이런 식으로 모델을 컴파일 및 학습시킬 수 있다.

## tf.keras.Sequential()
텐서플로우에서 구현해 놓은 신경망 클래스이다. 사용하기 편하다.
구현된 Sequential 클래스 모델에 완전 연결 층 Dense 은닉층을 추가하여 구현한다.
![](https://velog.velcdn.com/images/jinsoolve/post/9f52ec20-643f-4baf-a7df-03965fad9357/image.png)
![](https://velog.velcdn.com/images/jinsoolve/post/2d89d695-6183-46a2-ace1-ef5358fca634/image.png)
손실함수, 최적화방식, 평가지표 등을 정해서 컴파일할 수 있다.
참고로 컴파일함수는 모델을 어떻게 학습시킬지에 대한 설정을 지정하는 메서드이다.

ModelCheckpoint 콜백을 사용하여 모델을 훈련시키면서 최고의 성능을 내는 가중치를 저장할 수 있다. 또한 지정된 에포크 횟수(patience 매개변수)동안 평가지표가 개선되지 않으면 EarlyStopping 콜백함수를 이용하여 훈련을 멈추게 할 수 있다.

텐서보드(TensorBoard)를 사용하여 학습과정 및 계산 그래프를 시각화할 수 있다.

---
# 13.5 다층 신경망의 활성화 함수 선택
다층 신경망은 이론적으로 미분만 가능하다면 어떤 함수이든지 활성화 함수로 선택할 수 있다.
다만, 각각의 특징에 대해 살펴보고 상황에 따라 더 좋은 활성화 함수를 선택할 수 있도록 하자.

인공 신경망은 대체로 (복잡한 문제를 해결하므로) 비선형 활성화 함수가 좋은 성능을 보인다. 그 종류를 확인해 보자.

## 로지스틱 함수
시그모이드 함수의 일종으로 로지스틱 회귀 모델에서 사용되는 함수와 동일한 함수이다.
시그모이드는 뉴런 개념을 가장 비슷하게 흉내낸 함수이다.

그러나 큰 음수 입력이 들어오거나 시그모이드 함수가 0에 가까운 출력을 내리면 신경망이 매우 느리게 학습하게 된다. 이는 지역 최솟값에 갇힐 가능성이 높아지게 만든다.

## 소프트맥스 함수
![](https://velog.velcdn.com/images/jinsoolve/post/44ab9923-ae21-4293-beec-69d62d3810b8/image.png)

간접적인 argmax 함수라고 할 수 있다.
모든 확률을 지수적으로 바꾼 후에 정규화($\frac{해당 값}{전체 값의 합}$)시킨 것과 같다.

참고로, 다중 클래스 분류에서 사용하는 비용함수는 크로스 엔트로피(cross entropy)함수이다. 로지스틱 비용함수는 이것의 이진 분류 버전이라 할 수 있다.
![](https://velog.velcdn.com/images/jinsoolve/post/e8911091-684a-4554-b36c-85d362d59be7/image.png)
이것의 도함수는 로지스틱 비용함수와 동일하게 결과가 유도된다.

## tanh 함수
스케일이 조정된 로지스틱 함수라 할 수 있다.
![](https://velog.velcdn.com/images/jinsoolve/post/68c40e1b-8579-4f5c-bb9a-0b3479c7fb1f/image.png)
![](https://velog.velcdn.com/images/jinsoolve/post/8eed07a1-5cb8-4402-9e1d-e3853182cebe/image.png)

## ReLU 함수
tanh함수나 로지스틱함수는 그래프의 특징 상 그레이디언트 소실 문제(vanishing gradient problem)를 가지고 있다.
이를 어느정도 해결한 함수가 ReLU이다.
![](https://velog.velcdn.com/images/jinsoolve/post/fdd42823-8bcf-4dc5-83b2-ba392fb1ce8a/image.png)
아래 그림을 통해 활성화 함수들을 한꺼번에 봐 보자.
![](https://velog.velcdn.com/images/jinsoolve/post/aaf56513-c224-4eaf-89a6-5714da6993db/image.png)
</content:encoded></item><item><title><![CDATA[11장 레이블되지 않은 데이터 다루기: 군집 분석]]></title><description><![CDATA[레이블이 없는 데이터들을 분석하여 비슷한 데이터들끼리 그룹으로 묶을 것이다.이를 군집으로 묶는다하여 클러스터링(clustering)이라 한다. - k-평균 알고리즘을 이용하여 클러스터 중심 찾기- 상향식 방법으로 계층적 군집 트리 만들기- 밀집도 기반의 군집 알고리즘을 사용하여 임의 모야을 가진 대상 구분하기]]></description><link>https://jinsoolve.netlify.app/posts/ml-textbook-11</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/ml-textbook-11</guid><pubDate>Wed, 08 Jan 2025 15:00:00 GMT</pubDate><content:encoded>레이블이 없는 데이터들을 분석하여 비슷한 데이터들끼리 그룹으로 묶을 것이다.
이를 군집으로 묶는다하여 클러스터링(clustering)이라 한다. 

- k-평균 알고리즘을 이용하여 클러스터 중심 찾기
- 상향식 방법으로 계층적 군집 트리 만들기
- 밀집도 기반의 군집 알고리즘을 사용하여 임의 모야을 가진 대상 구분하기

---

# 11.1 K-평균 알고리즘을 사용하여 유사한 객체 그룹핑
k-평균 알고리즘은 단순하고 쉽다는 장점이 있지만 그룹의 개수를 정하는 k의 값에 따라 성능이 말도 안되게 달라지고 데이터가 고차원일수록 이 k값을 찾기가 어렵다.

그룹의 대표하는 값을 프로토타입이라고 하는데 연속현 데이터에서는 평균(센트로이드)을 사용하고 범주형 데이터에서는 가장 대표되는 값이나 자주 등장하는 값(메도이드)으로 사용한다.

이 k값에 대한 성능을 평가하는 엘보우 방법이나 실루엣 그래프라는 것이 있다.

k-평균의 초기 센트로이드를 정할 때 똑똑하게 할당하지 못하면 (빈 클러스터가 생겨나거나 한다) 성능이 나빠지거나 수렴이 느려진다. 이를 똑똑하게 할당하기 위해서 k-평균++알고리즘을 이용한다.

## k-평균++ 알고리즘
초기 센트로이드가 서로 멀리 떨어지도록 위치한다.
초기 센트로이드를 랜덤하게 고르고 골라지지 않은 샘플들과의 거리를 이용하여 제대로된 센트로이드를 선택하도록 한다.

## 직접군집 vs 간접군집
한 개의 샘플이 하나의 클러스터에만 포함되어 있으면 직접군집,
한 개의 샘플이 여러 개의 클러스터에 속할 수 있으면 간접군집이라 한다.

간접군집의 대표적인 예시가 퍼지 k-평균으로, FCM 알고리즘이라고 불린다.
샘플이 각 클러스터에 속할 확률값을 이용하여 해당 값이 더는 최적화되기 전까지 최적화시킨다. 클러스터 속할 확률을 계산하는데 비용이 들지만 최적화 수렴할 때까지 반복 횟수가 적게 들기 때문에 k-평균과 비슷한 결과를 만든다.

## 엘보우 방법으로 최적의 클러스터 개수 찾기
k-평균 알고리즘을 평가 지표를 이용해 k값을 최적화 시키는 방법이 있다.

기본적인 성능 계산방법은 SSE(Sum of Squared Errors) 즉, 오차 제곱의 합을 통해서 계산한다. 이때 엘보우 방법은 k값에 따른 SSE의 값을 그래프로 그려봤을 때 급감하는 지점을 찾는 것이다.
![](https://velog.velcdn.com/images/jinsoolve/post/ac70ffc3-28d7-45d2-a264-959ba5c23be7/image.png)

## 실루엣 그래프로 군집 품질을 정량화
클러스터 내부의 점끼리의 거리 지표인 응집력과
다른 클러스터 간의 거리 지표인 분리도를 이용한다.

응집력과 분리도의 차이를 둘 중 큰 값으로 나눈 것이 실루엣 계수 값이다. -1과 1 사이의 값을 갖는다.
즉 응집력과 분리도가 비슷할수록 실루엣은 0에 가까워지고, 차이가 커지면 1에 가깝게 된다. 분리도는 클러스터간 분리가 얼마나 잘 되어 있는지를 나타내고 응집력은 클러스터안이 얼마나 잘 뭉쳐있는지를 나타내기 때문이다.
따라서 실루엣 계수가 0에서 멀리 떨어질수록 좋은 결과로 해석할 수 있다.

클러스터 계수를 일부로 잘못 설정한 예시를 보자.
![](https://velog.velcdn.com/images/jinsoolve/post/48fdc3da-6455-4fcd-b820-70b1c207bf72/image.png)
위의 결과는 좋지 않은 것처럼 보이지만 클러스터 2개로 할 수 있는 나름의 최선을 한 결과이다.

---
# 11.2 계층적인 트리로 클러스터 조직화

계층 군집에는 병합 계층 군집과 분할 계층 군집이 있다.

- 병합 계층 군집
	각 샘플을 독립적인 클러스터로 만들고, 하나의 클러스터가 될 때까지 병합시킨다.
- 분할 계층 군집
	전체 샘플을 포함하는 클러스터에서 샘플이 하나 남을 때까지 작은 클러스터로 나눈다.
    
여기서는 병합 계층 군집을 자세히 다루겠다.

## 병합 계층 군집
기본적으로 클러스터 간 거리가 짧은 클러스터부터 합치는데 
클러스터 간의 거리를 정의하는 방식이 2가지가 있다.
- 단일 연결
	각 클러스터(집단)간의 거리는 두 클러스터 내의 샘플들 중 가장 거리가 가까운 샘플의 거리가 된다.
- 완전 연결
	각 클러스터(집단)간의 거리는 두 클러스터 내의 샘플들 중 가장 거리가 먼 샘플들의 거리가 된다.

사이킷런에 AgglomerativeClustering 클래스가 구현되어 있다.
![](https://velog.velcdn.com/images/jinsoolve/post/69906749-52eb-4ead-a0e2-d495656c4a53/image.png)

---

# 11.3 DBSCAN을 사용하여 밀집도가 높은 지역 찾기
k평균 알고리즘이나 계층 알고리즘이 원형으로 모양을 가정한 후에 클러스터링을 했다면 DBSCAN은 밀집도를 기반으로 비교적 자유로운 모형을 만든다.
즉, 많이 모여있으면 무슨 모양이든지 해당 부분을 클러스터링한다. 

- 핵심 샘플
	특정 반경 $\epsilon$ 내부에 있는 샘플의 개수가 MinPts 이상이면 핵심 샘플이다.
- 경계 샘플
	$\epsilon$이내에 Minpts보다 이웃이 적지만 다른 핵심 샘플의 반경 $\epsilon$ 안에 있으면 경계 샘플이다.
- 잡음 샘플
	핵심도 경계 샘플도 아니면 잡음 샘플이다.
    
모양이 자유롭고 모든 샘플을 클러스터에 할당하지 않고 잡음 샘플을 구분한다.
![](https://velog.velcdn.com/images/jinsoolve/post/ad175116-ed05-4544-ac96-229075d42c77/image.png)
위 그림과 같은 반달 모양의 데이터가 있다고 하자.
![](https://velog.velcdn.com/images/jinsoolve/post/41ffe775-d6fc-48ac-963c-80b9654c70ea/image.png)
왼쪽은 k평균, 오른쪽은 계층 알고리즘이다.	
![](https://velog.velcdn.com/images/jinsoolve/post/6020f4cc-e38f-4813-842b-323844b61a6a/image.png)
반면 DBSCAN 은 잘 잡아낸다.

하지만 특성 개수가 늘어나면 여전히 차원의 저주로 인해 역효과 증가. (이건 k-평균과 계층군집도 마찬가지)

또한 두개의 하이퍼파라미터(Minpts와 $\epsilon$)을 최적화시켜야 한다.

## 그래프 기반 군집

프로토타입 기반 군집인 k-평균, 병합 계층 군집, 밀집도 기반군집(DBSCAN)을 살펴보았다.
그래프 기반 군집인 스펙트럴 군집 또한 소개하겠다.
유사도 행렬 또는 거리 행렬의 고유 벡터를 사용하여 클러스터 관계를 유도한다.


</content:encoded></item><item><title><![CDATA[머신러닝 교과서 10장]]></title><description><![CDATA[선형 회귀 모델에 대해 알아보자. 보스턴 집 가격 예측 문제를 예시로 들어서 설명하겠다.]]></description><link>https://jinsoolve.netlify.app/posts/ml-textbook-10</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/ml-textbook-10</guid><pubDate>Wed, 08 Jan 2025 15:00:00 GMT</pubDate><content:encoded>선형 회귀 모델에 대해 알아보자.

보스턴 집 가격 예측 문제를 예시로 들어서 설명하겠다.

---
데이터 셋을 상관관계나 산점도를 그려 분석한다.

---
사이킷런의 LinearRegression()을 사용해 모델링한다.
경사하강법으로 모델의 파라미터 최적화를 시킨다.

---
선형 회귀 모델은 이상치에 영향을 많이 받기 때문에 이상치를 제거하는 것이 좋다. 이상치를 감지하는데 사용할 수 있는 통계적 테스트가 많지만 이번에는 제거하는 방식 대신 RANSAC(RANdom SAmple Consensus) 알고리즘을 사용하겠다.
![](https://velog.velcdn.com/images/jinsoolve/post/930f6405-d028-428f-b1f1-0fa3d5373da0/image.png)

RANSAC을 통해 이상치의 잠재적인 영향을 감소시킨다.

---
선형 회귀 모델의 성능을 평가하는데 차이 혹은 차이^2을 이용한다.
이때 차이값을 비교하기 위해서는 표준화해야 한다.

$R^2$ 지표도 사용한다. 평균제곱오차(MSE)와의 차이는 다음과 같다.
![](https://velog.velcdn.com/images/jinsoolve/post/81123eae-a8a5-4752-b197-c77f03f2fcb0/image.png)![](https://velog.velcdn.com/images/jinsoolve/post/e7eaa5e1-3ab2-402f-9616-89be391edbb5/image.png)

---
과대적합을 방지하기 위해 규제를 사용한다.

---
선형 회귀 모델을 다항 회귀로 변환 -&gt; 비선형도 가능해진다
PolynomialFeatures를 이용하여 feature들을 추가한다. 
2차, 3차 등의 다항식을 추가시켜 모델을 학습시킨다.

다항 특성을 많이 추가할수록 모델의 복잡도가 높아지고 과대적합할 가능성이 높아진다.

다항 특성이 비선형 관계를 모델링하는데 언제나 최선의 선택인 것은 아니다. 두 변수의 관계가 지수함수와 유사하다면 로그 변환을 해줄수 있다.

---
랜덤 포레스트를 사용하여 비선형 관계 다루기

결정트리 회귀는 특성 변환이 필요하지 않다. 한 번에 하나의 특성만 고려하기 때문이다. 결정트리는 일반적인 경향을 잘 잡아내지만 예측이 불연속적이고 매끄럽지 못하다.![](https://velog.velcdn.com/images/jinsoolve/post/6cd45e15-fe2b-43a9-9c69-5de8be9979eb/image.png)

이를 랜덤포레스트를 활용하면 어느 정도 해결할 수 있다. 더 좋은 일반화성능을 가지며 분산을 낮춰준다. 앙상블 트리 개수가 랜덤포레스트의 유일한 하이퍼파리미터인 것이 장점이다.
![업로드중..](blob:https://velog.io/dbdd0612-275f-49f5-a67b-98136d15be5a)
잔차 그래프(residual plots)의 분포가 랜덤하지 않는다면(어느정도 규칙성을 띄고 있다면) 모델이 특성의 정보를 제대로 잡아내고 있지 못 한것이다. 그러나 단일 결정트리보다는 여전히 좋은 성능을 내고 있다.

SVM 회귀
커널 SVM으로 비선형 회귀문제를 해결할 수 있다. 사이킷런에 구현되어 있으니 참고하면 좋다.</content:encoded></item><item><title><![CDATA[12장 다층 인공 신경망을 밑바닥부터 구현]]></title><description><![CDATA[딥러닝은 인공 신경망을 효과적으로 학습시키기 위한 머신러닝의 하위분야이다. 아래 내용을 소개하겠다.- 다층 신경망 개념- 역전파 알고리즘- 이미지 분류를 위한 다층 신경망 훈련]]></description><link>https://jinsoolve.netlify.app/posts/ml-textbook-12</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/ml-textbook-12</guid><pubDate>Wed, 08 Jan 2025 15:00:00 GMT</pubDate><content:encoded>딥러닝은 인공 신경망을 효과적으로 학습시키기 위한 머신러닝의 하위분야이다. 아래 내용을 소개하겠다.
- 다층 신경망 개념
- 역전파 알고리즘
- 이미지 분류를 위한 다층 신경망 훈련

# 12.1 인공 신경망으로 복잡한 함수 모델링
인공 신경망은 뇌가 어떻게 복잡한 문제를 푸는지에 대한 가설과 모델을 기반으로 만들어졌다. 초기 연구는 이미 1940년대에 완료되었으나 역전파 알고리즘이 재발견되기 전까지는 관심을 받지 못 했다.

## 단일층 신경망
다층 신경망을 보기 전에 단일층에 대해 설명하겠다.
은닉층이 없는 신경망, 즉 입력층과 출력층만 존재하는 신경망을 뜻한다. 경사 하강법 최적화를 이용하여 모델을 학습 시킨다.

확률적 경사 하강법(stochastic gradient descent) 최적화를 사용할 수도 있다.
- 온라인 학습: 하나의 훈련 샘플
- 미니 배치 학습: 적은 수의 훈련 샘플

을 이용하여 일반적인 경사하강법에 비해 더 자주 가중치 업데이트하기 때문에 훈련이 빠르다.
또한 들쭉날쭉한 학습 특성이 다수 볼록함수를 가진 비선형 활성화 함수를 활용한 다층 신경망을 훈련시킬 때 큰 장점이 된다.
확률적 경사하강법에서 생기는 잡음이 지역 최솟값(전체 최솟값은 아닌데 지역에서 최솟값인 경우)를 탈출하는데 도움이 된다.

## 다층 신경망
여러 개의 단일 뉴런을 연결하여 만든 다층 피드포워드 신경망을 뜻한다. 다층 퍼셉트론(MultiLayer Perceptron, MLP) 혹은 심층 인공 신경망(deep artificial neural network)이라고도 한다. 
단일 신경망과 다른 점은 입력층과 출력층 사이에 여러 개의 은닉층이 존재한다.

&lt;sub&gt;
참고로, 퍼셉트론은 -1과 1로 출력하는 단순한 이진분류 모델이고 오분류된 샘플들에 대해 가충지를 조절한다.
반면에, 아달린은 연속 적인 실수값 출력을 생성하고 경사하강법을 이용하여 최적화 시킨다.
둘 다 분류를 하는 단순한 모델이다.
&lt;/sub&gt;

은닉층이 많아질수록 역전파로 계산하는 오차 그레디언트가 점점 작아지는 그레이디언트 소실 문제가 나타난다. 이런 문제를 해결하기 위해 개발된 기법들이 딥러닝이 되었다.

### 정방향 계산 및 역전파 최적화
정방향 계산의 순서는 다음과 같다.
1. 신경망의 흐름대로 전파시켜 값을 출력한다.
2. 오차 값을 계산한다.
3. 가중치에 대한 도함수를 찾아 오차를 역전파하여 모델을 최적화시킨다.

여기서 사용하는 활성화 함수는 미분 가능해야 한다.
복잡한 문제를 해결하기 위해서는 활성함수를 비선형으로 사용해야 한다. (시그모이드 활성화 같은 함수)

MLP(다층 퍼셉트론)는 대표적인 피드포워드 인공 신경망이다.
여기서 피드포워드란 각 층에서 입력을 순환시키지 않고 다음 층으로 전달한다는 의미이다.

---
# 12.2 손글씨 숫자 분류
신경망을 이용해서 손글씨 숫자 분류 모델을 구현해 보자.

1. 데이터를 받아와서 savez나 pickle 같은 함수로 파이썬이 빠르게 읽을 수 있는 포맷으로 저장한다.
2. 신경망을 구현한다.
	1. sigmoid: 활성화 함수
    2. forward: 정방향 전파하여 out layer 반환
    3. compute_cost: 비용 함수 J 계산
    4. predict: 클래스 레이블에 대한 예측값 반환
    5. fit: 위 함수들을 이용해서 모델 훈련 및 평가
    
신경망 훈련에서는 훈련 정확도와 검증 정확도를 반드시 비교해야 한다. 모델의 최적화 여부나 과대적합 여부를 확인하는데 필수다.
아래 그림을 보면 에포크가 늘어날 수록 training과 validation 데이터들의 정확도를 확인할 수 있다.
![](https://velog.velcdn.com/images/jinsoolve/post/62af6607-294c-4c2d-bb67-c19a09de4837/image.png)

일반적으로 신경망이 머신러닝 모델들에 비해 비용이 많이 든다. 
따라서 어떤 조건이 되면 일찍 중지하고 다른 하이퍼파라미터 설정을 하는 것이 좋다.

## 과대적합
모델의 과대적합을 줄이는 방법은 규제 강도를 높이는 방법이 있다. 규제 파라미터의 값을 증가시키는 것.
혹은 드롭아웃(dropout)기법을 사용하는 것이 있다.

더 좋은 성능을 낼 수 있는 방법으로는
- skip-connection
- 학습률 스케줄러
- 인셉션 v3구조 처럼 신경망의 앞쪽 층에 손실함수 연결

이 있다.

---
# 12.3 인공 신경망 훈련
신경망 훈련의 핵심인 역전파 알고리즘에 대해서 설명하겠다.
역전파 알고리즘은 다층 신경망에서 복잡한 비용함수의 편미분을 효율적으로 계산하기 위한 방법이라 말할 수 있다.

일반적인 신경망의 비용함수 곡면은 볼록 함수가 아니거나 파라미터에 대해 매끄럽지 않다. 고차원 비용함수의 곡면에는 전역 최솟값을 찾기 위해 넘어야 할 지역 최솟값이 많다.

역방향은 오른쪽에서 왼쪽으로 진행하다보니 행렬-벡터 곱셈이 되어서 왼쪽에서 오른쪽으로 진행(행렬-행렬 곱셈)보다 훨씬 계산비용이 저렴하다.

## 역전파 과정
1. 출력층의 오차항(출력값-실제값) 계산
2. $\frac{\sigma J(W)}{\sigma W}$를 계산한다.
3. 은닉층의 오차항 계산
4. 2-3과정을 입력층의 J의 편미분을 계산할 때까지 반복한다.

그림으로 보면 쉽게 이해할 수 있다.
![](https://velog.velcdn.com/images/jinsoolve/post/42ff8c13-7fb0-43e5-9f8e-470cb7988417/image.png)

이렇게 구한 모든 층의 비용함수의 도함수(기울기) 값을 이용해서 매개변수 W를 업데이트 시킨다.
![](https://velog.velcdn.com/images/jinsoolve/post/54ac5810-f131-48c2-894b-3df7d3b1c6ee/image.png)


---
# 12.4 신경망의 수렴
미니 배치 방식을 사용하면 확률적이지만 매우 정확한 솔루션을 만들고 기본 경사하강법보다 훨씬 빠르게 수렴한다.
미니 배치 방식은 n개의 훈련샘플에서 k개의 부분집합에서 기울기를 계산한다.

다층 신경망은 아달린, 로지스틱 회귀, SVM 같은 알고리즘보다 훨씬 훈련시키기 어렵다.
최적화시켜야할 가중치가 훨씬 많고, 손실함수의 표면이 거칠어서 최적화 알고리즘이 지역 최솟값에 갇히기 쉽다.</content:encoded></item><item><title><![CDATA[가중치 w와 L2 규제]]></title><description><![CDATA[왜 norm을 사용하는 것이 과대적합을 해결할까? 그 이유를 살펴보자.]]></description><link>https://jinsoolve.netlify.app/posts/ml-textbook-2</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/ml-textbook-2</guid><pubDate>Sun, 05 Jan 2025 15:00:00 GMT</pubDate><content:encoded>모델이 과대적합이 되었을 때 우리는 norm을 통해 이를 해결한다. 
그런데 왜 norm을 사용하는 것이 과대적합을 해결할까? 그 이유를 살펴보자.
# 과대적합이란 무엇일까?
모델이 과도하게 훈련 데이터에만 적합되어 있는 것을 뜻한다. 즉, 모델을 일반적인 상황이 아닌 특수 상황에 과도하게 적합된 상태를 뜻한다.
![](https://i.imgur.com/WwKS7aV.png)


위 그림을 보면 과대적합되면 모델이 너무 복잡해 진다. 즉, 가중치 w 파라미터의 값이 너무 크게 된다는 의미가 된다.

  가중치 w가 커지면 결정 경게의 기울기가 증가하고 훈련 데이터 값 하나하나에 더 민감해진다. 따라서 가중치 w가 커지면 모델도 복잡해지는 것이다.
(또한 이는 데이터의 노이즈에도 민감하게 반응하여 모델 복잡성이 높아져도 예측 정확도가 향상되지 안헤 된다.)

어쨌든 그럼 w를 어느 정도 줄여주면 이는 모델의 복잡도의 감소로 이어진다. 이런 방법을 normalization 즉, 규제(표준화)라고 한다.
## 여기서 잠깐, 분산과 편향이란 무엇일까?
다음 사진을 보면 이를 잘 이해할 수 있다.
![](https://velog.velcdn.com/images/jinsoolve/post/b8bc9aa3-c415-486f-99e0-06e75cc9998a/image.png)

그럼 다음 사진은 분산이 작고, 편향이 크다... 하하
![](https://velog.velcdn.com/images/jinsoolve/post/63829c2e-6490-4308-94ff-02058f9fb299/image.jpeg)
## L2 Norm
자주 쓰는 norm 기법으로 L2 norm이 있다. (L1 norm도 있긴 한데 넘어가자)
$$
\frac{\lambda}{2}\| \mathbf{w} \|_2 = \frac{\lambda}{2}\sum_{i=1}^{n} w_i^2
$$
여기서 $\lambda$는 norm의 하이퍼파라미터이다.
이를 비용함수에 norm 항을 추가시킨다.
$$
J(w) = -\frac{1}{m} \sum_{i=1}^{m} [y^{(i)} \log(\sigma(x^{(i)})) + (1 - y^{(i)}) \log(1 - \sigma(x^{(i)}))] + \frac{\lambda}{2}\| \mathbf{w} \|_2
$$
이렇게 되면 w값이 크면 비용함수 또한 커지기 때문에 자연스럽게 w의 값이 줄어들고 이는 곧 모델의 복잡도 감소로 이어진다.
따라서 L2 norm이 과대적합 문제의 해결방안이 된다.
# 그럼 과소적합은 뭘까?
편향이 클 때를 과소적합이라고 한다. 모델이 너무 단순하여 훈련데이터를 충분히 설명하지 못 한다.
즉, 모델의 결정 경계에서 멀리 떨어져 있게 된다. 이는 편향이 큰 것으로 이어진다.
그럼 이를 해결하려면 어떻게 할까?
## 과소적합 해결 방법
단순히 과대적합의 해결방안을 반대로 하면 해결된다. 과대적합과는 반대로 모델의 복잡도가 너무 낮아서 나타나는 현상이니 이를 높여주면 해결되는 것이다.
+ 규제 풀기
규제로 인해 가중치 w가 낮다면 $\lambda$를 감소시켜서 규제를 완화시킨다. ($\lambda$가 크면 강한 규제, $\lambda$가 낮으면 약한 규제이다)
+ 모델 복잡성 증가
더 많은 파라미터를 추가하거나 더 복잡한 알고리즘을 선택할 수 있다. 대표적인 예시로는 다항 회귀 모델이 있다.
다항 회귀 모델이란 다음과 같다.
![](https://velog.velcdn.com/images/jinsoolve/post/9f6c52b2-4b3a-418c-8226-fe75d943660f/image.png)
+ 더 많은 데이터 수집
과소적합은 데이터 부족으로 인해서도 나타날 수 있다. 더 많은 훈련 데이터를 제공하면 모델은 더 일반화된 경향 즉 복잡도가 높아질 것이다.

이상으로 과대적합과 과소적합, 그리고 norm에 대해서 살펴보았다.</content:encoded></item><item><title><![CDATA[머신러닝 분류 모델들]]></title><description><![CDATA[머신러닝에 사용하는 다양한 분류 모델들에 대해서 얘기해 보겠다.]]></description><link>https://jinsoolve.netlify.app/posts/ml-textbook-3</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/ml-textbook-3</guid><pubDate>Sun, 05 Jan 2025 15:00:00 GMT</pubDate><content:encoded>
머신러닝에 사용하는 다양한 분류 모델들에 대해서 얘기해 보겠다.
# 서포트 벡터 머신 (SVM)
SVM(Support Vector Machine)이라 불리는 이 분류 모델은 클래스를 구분하는 hyper-plane(초평면)과 이 hyper-plane에 가장 가까운 훈련 샘플 사이의 거리로 정의한다.
이러한 샘플을 서포트 벡터라고 한다.
![](https://i.imgur.com/6A0ZkK8.png)


SVM의 최적화 대상은 마진을 최대화 하는 것이다. 여기서 마진이란, 양성 쪽 hyper-plane과 음성 쪽 hyper-plane 사이의 거리를 뜻한다.
마진을 최대화하면 일반화 오차가 낮아지는 경향이 있기 때문에 마진의 최대화가 SVM의 최적화로 이어진다.

&lt;Callout type=&quot;info&quot;&gt;
마진 최대화를 정확하게 이해하기 위해서는 초평면들의 수식 계산을 해야 하는데 결과적으로 $\frac{2}{\|w\|}$가 최대화하고 싶은 마진을 의미한다는 결론이 도출된다.
따라서 SVM의 목적 함수는 샘플이 정확하게 분류된다는 제약 조건 하에서 $\frac{2}{\|w\|}$를 최대화함으로써 마진을 최대화하는 것이다.
&lt;/Callout&gt;

결론적으로 SVM은 모든 양성 클래스 샘플은 양성 쪽 초평면 너머에, 모든 음성 클래스 샘플은 음성 쪽 초평면 너머에 있도록 하는 것이고 이를 수식으로 정리하면 다음과 같다.
$$
w_0 + w^Tx^{(i)} \geq 1 \; y^{(i)}=1일 \,때\\
w_0 + w^Tx^{(i)} \leq -1 \; y^{(i)}=-1일 \,때\\
i=1 ... N 까지
$$
## SVM의 슬랙 변수
SVM은 선형적으로 구분되지 않는 데이터에서 한계가 있습니다. 이를 해결하기 위해 슬랙 변수 $\zeta$를 사용하여 선형 제약을 완화시킨다.
이를 소프트 마진 분류라고 부른다. 슬랙 변수를 추가하여 수식화하면 다음과 같다.
$$
w_0 + w^Tx^{(i)} \geq 1-\zeta^{(i)} \; y^{(i)}=1일 \,때\\
w_0 + w^Tx^{(i)} \leq -1+\zeta^{(i)} \; y^{(i)}=-1일 \,때\\
i=1 ... N 까지
$$
## SVM의 하이퍼 파라미터 C
C 는 $\frac{1}{\lambda}$으로 규제 파라미터 $\lambda$의 역수이다. 이 C를 줄이면 규제가 강해지고 이는 SVM에서 분산은 줄어들고 편향은 커진다(마진이 줄어들기 떄문).
![](https://i.imgur.com/PTPS2i3.png)

```python
from sklearn.svm import SVC

# kernel=&apos;linear&apos;: 선형 분류 SVM을 사용하겠다.
# C=1.0 : 규제의 역수 하이퍼 파라미터
svm = SVC(kernel=&apos;linear&apos;, C=1.0, random_state=1)
svm.fit(X_train_std, y_train)

plot_decision_regions(X_combined_std, 
                      y_combined,
                      classifier=svm, 
                      test_idx=range(105, 150))
plt.xlabel(&apos;petal length [standardized]&apos;)
plt.ylabel(&apos;petal width [standardized]&apos;)
plt.legend(loc=&apos;upper left&apos;)
plt.tight_layout()
# plt.savefig(&apos;images/03_11.png&apos;, dpi=300)
plt.show()
```
![](https://velog.velcdn.com/images/jinsoolve/post/4f9fd9d8-403a-48f5-bc0d-110cbe8d5c9f/image.png)
# 커널 SVM
커널 SVM을 사용하면 비선형 분류 문제도 해결이 가능하다.
![](https://i.imgur.com/4QM5TD9.png)

위 그림처럼 비선형 분류 문제와 같은 경우는 일반적인 선형 SVM으로는 해결이 불가능한데 이를 **커널 방법**으로 해결할 수 있다.
## 커널 방법(Kernel Method)
쉽게 설명하면, 매핑 함수 $\phi$를 사용하여 비선형 데이터들을 선형 구분이 가능한 고차원 공간에 투영하는 것이다.
$$
\phi(x_1,x_2) = (z_1,z_2,z_3) = (x_1,x_2,x_1^2+x_2^2)
$$
![](https://i.imgur.com/7dM2ITm.png)

위에 그림에서 보는 것처럼 원래 선형으로 분리가 안 되는 데이터셋을 선형적으로 분리할 수 있게 되었다.
하지만 이러한 매핑 방식은 새로운 특성을 만드는 계산 비용이 매우 비싸다는 단점이 있다. 이를 커널 기법(kernel trick)으로 해결한다.
## 커널 기법(Kernel Trick)
수학적으로 여러 복잡한 과정이 있지만 여기서는 자세히 다루지 않고 간단한 과정을 요약하도록 하겠다.

&gt; 실전에서 필요한 것은 점곱 $x^{(i)T}x^{(j)}$를 $\phi(x^{(i)})^T\phi(x^{(j)})$로 바꾸는 것입니다.
&gt; 이떄 점곱을 계산하는데 드는 비용을 절감하기 위해 커널 함수를 정의한다.
$$
K(x^{(i)},x^{(j)}) = \phi(x^{(i)})^T\phi(x^{(j)})
$$
&gt; 가장 널리 사용되는 커널 중 하나는 **방사 기저 함수(Radial Basis Function, RBF)**이다. **가우시안 커널**이라고도 한다.
$$
K(x^{(i)},x^{(j)}) = exp(-\frac{\|x^{(i)}-x^{(j)}\|^2}{2\sigma^2}) = exp(-\gamma\|x^{(i)}-x^{(j)}\|^2)
$$
여기서 $\gamma = \frac{1}{2\sigma^2}$이고 이는 최적화 대상 파라미터가 아니다.

간단히 요약하면, 커널이란 용어를 샘플 간의 유사도 함수로 해석할 수 있도록 만드는 것이다. 실제 사용은 어떻게 하나 살펴보자.
```python
# kernel을 Radial Basis Function으로 선택해 커널 기법을 이용해 커널 svm을 사용한다.
# gamma는 위에서 사용한 파라미터로, 가우시안 구의 크기를 제한하는 매개변수이다.
svm = SVC(kernel=&apos;rbf&apos;, random_state=1, gamma=0.10, C=10.0)
svm.fit(X_xor, y_xor)
plot_decision_regions(X_xor, y_xor,
                      classifier=svm)

plt.legend(loc=&apos;upper left&apos;)
plt.tight_layout()
# plt.savefig(&apos;images/03_14.png&apos;, dpi=300)
plt.show()
```
![](https://velog.velcdn.com/images/jinsoolve/post/bd053514-a9f9-4e50-bb8a-c479de87b796/image.png)
## $\gamma$ 변수
$\gamma$를 크게 하면 결정경계는 샘플에 가까워지고 구불구불해진다.
### $\gamma$를 작게 했을 때
![](https://velog.velcdn.com/images/jinsoolve/post/0cb747b7-8f79-447e-ab75-a1e6719c35fc/image.png)
### $\gamma$를 크게 했을 떄
![](https://velog.velcdn.com/images/jinsoolve/post/7e54d6f7-0861-4e91-a19f-6ed54b6ecf9b/image.png)
# 결정 트리(Decision Tree) 학습
![](https://i.imgur.com/Xa5kfft.png)

결정 트리는 훈련데이터에 있는 특성을 기반으로 샘플의 클래스 레이블을 추정할 수 있는 일련의 질문을 학습한다.
이때 트리를 구성하는 방식은 **정보 이득**이 최대가 되는 방향으로 구성한다.
## 정보 이득을 어떻게 확인할까?
불순도 지표를 이용하여 확인한다. 불순도가 낮을 수록 정보 이득이 커진다. 대표적인 불순도 지표는 다음과 같다.
### 지니 불순도
$$
I_H(t) = -\sum_{i=1}^cp(i|t)log_2p(i|t)
$$
### 엔트로피
$$
I_G(t) = 1 - \sum_{i=1}^cp(i|t)^2
$$
### 분류오차
$$
I_E = 1 - max{p(i|t)}
$$
![](https://velog.velcdn.com/images/jinsoolve/post/8141719e-40e2-4358-b372-9bacb7364004/image.png)
이런 식으로 결정 트리가 완성된다.
# 랜덤 포레스트로 여러 개의 결정 트리 연결
1. n 개의 부트스트랩 샘플을 뽑는다.
2. 부트스트랩 샘플에서 결정 트리를 학습한다.
	a. 중복을 허용하지 않고 랜덤하게 d개의 특성을 선택한다.
    b. 정보 이득과 같은 목적함수를 기준으로 최선의 분할을 만드는 특성을 사용해서 노드를 분할한다.
3. 1~2를 k번 반복한다.
4. 각 트리의 예측을 모아 다수결 투표로 클래스 레이블을 할당한다.

랜덤 포레스트는 결정 트리만큼 해석이 쉽지는 않지만 하이퍼파라미터 튜닝에 많은 노력을 기울일 필요가 없다.
# 정리
모델 해석이 중요할 때는 결정트리가 사용하기 좋다.
로지스틱 회귀는 확률적 경사 하강법을 사용한 온라인 학습 뿐만 아니라 특정 이벤트 확률 에측에도 사용 가능</content:encoded></item><item><title><![CDATA[백준 17407 - 괄호 문자열과 쿼리]]></title><description><![CDATA[백준 17407 - 괄호 문자열과 쿼리]]></description><link>https://jinsoolve.netlify.app/posts/boj-17407</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/boj-17407</guid><pubDate>Tue, 31 Dec 2024 15:00:00 GMT</pubDate><content:encoded>
# [백준 17407 - 괄호 문자열과 쿼리](https://www.acmicpc.net/problem/17407)

## 풀이
`(`을 +1, `)`을 -1로 한 다음 누적합을 먼저 계산한다.  
이 누적합에 대한 lazy segment tree를 만든다.  

만약 x번째 문자를 `(`에서 `)`으로 바꾼다면 x ~ n번째(전체 문자길이가 n일 때) 문자까지 누적합을 전부 -2를 시켜주면 되고 그 반대의 경우는 +2를 시켜주면 된다. 위 내용은 구간 업데이트로 lazy update를 사용한다.  

문자를 변경했을 때 전체 괄호 누적합의 min이 0보다 크거나 같아야 하고, 전체 누적합의 결과가 0이면 해당 문자열은 올바른 괄호 문자열이다.  
즉, 누적합의 lazy seg에서 전체의 min이 0보다 크거나 같고, 마지막 누적합의 값이 0이기만 하면 된다.

## 코드
```cpp
#include &lt;bits/stdc++.h&gt;

#define endl &quot;\n&quot;
#define all(v) (v).begin(), (v).end()
#define For(i, a, b) for(int i=(a); i&lt;(b); i++)
#define FOR(i, a, b) for(int i=(a); i&lt;=(b); i++)
#define Bor(i, a, b) for(int i=(a)-1; i&gt;=(b); i--)
#define BOR(i, a, b) for(int i=(a); i&gt;=(b); i--)
#define ft first
#define sd second

using namespace std;
using ll = long long;
using lll = __int128_t;
using ulll = __uint128_t;
using ull = unsigned long long;
using ld = long double;
using pii = pair&lt;int, int&gt;;
using pll = pair&lt;ll, ll&gt;;
using ti3 = tuple&lt;int, int, int&gt;;
using tl3 = tuple&lt;ll, ll, ll&gt;;

template&lt;typename T&gt; using ve = vector&lt;T&gt;;
template&lt;typename T&gt; using vve = vector&lt;vector&lt;T&gt;&gt;;

template&lt;class T&gt; bool ckmin(T&amp; a, const T&amp; b) { return b &lt; a ? a = b, 1 : 0; }
template&lt;class T&gt; bool ckmax(T&amp; a, const T&amp; b) { return a &lt; b ? a = b, 1 : 0; }

const int INF = 987654321;
const int INF0 = numeric_limits&lt;int&gt;::max();
const ll LNF = 987654321987654321;
const ll LNF0 = numeric_limits&lt;ll&gt;::max();

struct Lazy {
    ll val, a, b; // a * val + b
};

class LazySegment {
public:
    vector&lt;Lazy&gt; tree; //tree[node] := a[start ~ end] 의 합

    LazySegment() {}
    LazySegment(int size) {
        this-&gt;resize(size);
    }
    void resize(int size) {
        size = (int) floor(log2(size)) + 2;
        size = pow(2, size);
        tree.resize(size, {0,1, 0});
    }
    ll init(vector&lt;ll&gt; &amp;a, int node, int start, int end) {
        if(start == end) return tree[node].val = a[start];
        return tree[node].val = min(init(a, 2*node, start, (start+end)/2), init(a, 2*node+1, (start+end)/2+1, end));
    }
    void update_lazy(int node, int start, int end) {
        if(tree[node].a == 1 &amp;&amp; tree[node].b == 0) return;
        tree[node].val = (tree[node].a*tree[node].val + tree[node].b);
        if(start != end) {
            for(auto i : {2*node, 2*node+1}) {
                tree[i].a = (tree[node].a * tree[i].a);
                tree[i].b = (tree[node].a * tree[i].b + tree[node].b);
            }
        }
        tree[node].a = 1, tree[node].b = 0;
    }
    void update(int node, int start, int end, int left, int right, ll a, ll b) {
        update_lazy(node, start, end);
        if(right &lt; start || end &lt; left) return;
        if(left &lt;= start &amp;&amp; end &lt;= right) {
            tree[node].a = (tree[node].a * a);
            tree[node].b = (tree[node].b + b);
            update_lazy(node, start, end);
            return;
        }
        update(node * 2, start, (start + end) / 2, left, right, a, b);
        update(node * 2 + 1, (start + end) / 2 + 1, end, left, right, a, b);
        tree[node].val = min(tree[2*node].val, tree[2*node+1].val);
    }
    ll query(int node, int start, int end, int left, int right) {
        update_lazy(node, start, end);
        if(right &lt; start || end &lt; left) return INF;
        if(left &lt;= start &amp;&amp; end &lt;= right) return tree[node].val;
        return min(query(node * 2, start, (start + end) / 2, left, right),
                   query(node * 2 + 1, (start + end) / 2 + 1, end, left, right));
    }
};

void solve() {
    string s; cin &gt;&gt; s;
    int n = s.length();
    LazySegment seg(n);
    vector&lt;ll&gt; a(n+1);
    a[0] = 0;
    FOR(i,1,n) {
        a[i] = a[i-1] + (s[i-1] == &apos;(&apos; ? 1 : -1);
    }
    seg.init(a,1,1,n);

    int m; cin &gt;&gt; m;
    int ans = 0;
    while(m--) {
        int x; cin &gt;&gt; x;
        if(s[x-1] == &apos;(&apos;) {
            s[x-1] = &apos;)&apos;;
            seg.update(1,1,n,x,n,1,-2);
        }
        else {
            s[x-1] = &apos;(&apos;;
            seg.update(1,1,n,x,n,1,2);
        }
        if(seg.query(1,1,n,1,n) == 0 and seg.query(1,1,n,n,n) == 0) ans++;
    }
    cout &lt;&lt; ans &lt;&lt; endl;
}

int main(void) {
    ios_base::sync_with_stdio(false);
    cin.tie(nullptr);
    cout.tie(nullptr);

    int TC=1;
//    cin &gt;&gt; TC;
    FOR(tc, 1, TC) {
//        cout &lt;&lt; &quot;Case #&quot; &lt;&lt; tc &lt;&lt; &quot;: &quot;;
        solve();
    }


    return 0;
}
```</content:encoded></item><item><title><![CDATA[김진수 포트폴리오]]></title><description><![CDATA[개발자 김진수를 소개합니다.]]></description><link>https://jinsoolve.netlify.app/posts/portfoilo</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/portfoilo</guid><pubDate>Wed, 25 Dec 2024 15:00:00 GMT</pubDate><content:encoded>
# Coming Soon
</content:encoded></item><item><title><![CDATA[김진수에 대하여]]></title><description><![CDATA[엔지니어 김진수를 소개합니다.]]></description><link>https://jinsoolve.netlify.app/posts/about_me</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/about_me</guid><pubDate>Wed, 25 Dec 2024 15:00:00 GMT</pubDate><content:encoded>



![](./CV.jpg)

# Recent Research Interest
- 모델 경량화, Quantization
- 모델 Reasoning 능력 향상
- 모델 경량화하면서 어떻게 Reasoning 능력을 향상시킬 수 있을까?

# Recent Algorithm Goals
- 구현, 시뮬레이션 문제들 많이 풀어보기
- 골드 문제 빠른 시간 내에 정확히 푸는 연습
- 플래티넘 상위 문제 푸는 연습</content:encoded></item><item><title><![CDATA[백준 1462 - 퀴즈쇼]]></title><description><![CDATA[백준 1462 - 퀴즈쇼]]></description><link>https://jinsoolve.netlify.app/posts/boj-1462</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/boj-1462</guid><pubDate>Wed, 25 Dec 2024 15:00:00 GMT</pubDate><content:encoded># [BOJ 1462 - 퀴즈쇼](https://www.acmicpc.net/problem/1462)

## 풀이
`dp[i][0]` := 1~i 까지 문제를 푸는데, i를 풀고 나서 남은 코인의 갯수가 0개일 때의 최댓값
`dp[i][1]` := 1~i 가지 문제를 푸는데, i를 풀고 나서 남은 코인의 갯수와 상관없이 최댓값

위 처럼 2가지 dp를 저장한다고 하자.

먼저 `dp[i][0]`를 생각해보자.
1. i번 문제를 맞춰서 보너스 코인을 받아 코인을 모두 소모함.
`dp[i-m][0] + (scoreAcc[i] - scoreAcc[i-m]) + bonus[i]`
1. i-m까지 문제를 풀고 나서 코인이 0개가 되었을 때의 최댓값  := `dp[i-m][0]`
2. i-m+1 ~ i 까지의 문제를 모두 맞추고 i번 째 문제에서 보너스 점수를 받음 := `(scoreAcc[i] - scoreAcc[i-m]) + bonus[i]`
2. i번 문제를 틀려서 모든 코인을 잃음.
`dp[i-1][1] - score[i]`
i-1까지 최댓값에 i번째 문제를 틀려서 점수를 빼준다.
위와 같이 2가지 케이스가 된다.

그럼 이번에는 `dp[i][1]`을 생각해보자.
1. i번 맞추기
1. i번을 맞춤으로써, M개 채움
2. i번을 맞춰도 M개를 채우지 못 함.
2. i번 틀리기
위와 같이 총 3개의 케이스가 있다.
그러나 우리는 `dp[i][0]`에서 이미 1-1번과 2번을 모두 계산했음을 알 수 있다.
그렇다면 우리는 1-2번만 해결하면 되고, 이는 `dp[i-1][1] + score[i]`이다.

여기서 의문의 생기는데 `dp[i-1][1] + score[i]`에서 i-1까지의 최댓값 상황에서 코인이 몇 개인지 알아야 i번째에 보너스를 더할지 안 더할지를 할 수 있지 않을까라는 의문이 생길 수 있다.
그러나, 만약 i번째에서 M개를 채웠다면 이는 결국 1-1이다. 그리고 점수들은 모두 음이 아닌 정수이므로 무조건 1-1의 경우가 저장된 `dp[i][0]`가 클 것이고 결국 `max`함수로 비교하면 이를 덮어씌울 것이다. 따라서 우리는 `dp[i-1][1]`이 몇 개의 코인을 모았는지 신경쓸 필요가 없다.

## 코드
```cpp
#include &lt;bits/stdc++.h&gt;

#define endl &quot;\n&quot;
#define all(v) (v).begin(), (v).end()
#define For(i, a, b) for(int i=(a); i&lt;(b); i++)
#define FOR(i, a, b) for(int i=(a); i&lt;=(b); i++)
#define Bor(i, a, b) for(int i=(a)-1; i&gt;=(b); i--)
#define BOR(i, a, b) for(int i=(a); i&gt;=(b); i--)
#define ft first
#define sd second

using namespace std;
using ll = long long;
using lll = __int128_t;
using ulll = __uint128_t;
using ull = unsigned long long;
using ld = long double;
using pii = pair&lt;int, int&gt;;
using pll = pair&lt;ll, ll&gt;;
using ti3 = tuple&lt;int, int, int&gt;;
using tl3 = tuple&lt;ll, ll, ll&gt;;

template&lt;typename T&gt; using ve = vector&lt;T&gt;;
template&lt;typename T&gt; using vve = vector&lt;vector&lt;T&gt;&gt;;

template&lt;class T&gt; bool ckmin(T&amp; a, const T&amp; b) { return b &lt; a ? a = b, 1 : 0; }
template&lt;class T&gt; bool ckmax(T&amp; a, const T&amp; b) { return a &lt; b ? a = b, 1 : 0; }

const int INF = 987654321;
const int INF0 = numeric_limits&lt;int&gt;::max();
const ll LNF = 987654321987654321;
const ll LNF0 = numeric_limits&lt;ll&gt;::max();

int n, m;
ve&lt;ll&gt; score, bonus, scoreAcc;
vve&lt;ll&gt; dp;

void solve() {
    cin &gt;&gt; n &gt;&gt; m;
    score = ve&lt;ll&gt;(n+1,0);
    bonus = ve&lt;ll&gt;(n+1,0);
    scoreAcc = ve&lt;ll&gt;(n+1,0);
    dp = vve&lt;ll&gt;(n+1, ve&lt;ll&gt;(2, -INF0));

    FOR(i,1,n) {
        cin &gt;&gt; score[i];
        scoreAcc[i] = score[i] + scoreAcc[i-1];
    }
    FOR(i,1,n) cin &gt;&gt; bonus[i];

    dp[0][0] = dp[0][1] = 0;
    FOR(i,1,n) {
        ckmax(dp[i][0], dp[i-1][1] - score[i]);
        if(i-m&gt;=0) ckmax(dp[i][0], scoreAcc[i]-scoreAcc[i-m] + bonus[i] + dp[i-m][0]);

        ckmax(dp[i][1], dp[i][0]);
        ckmax(dp[i][1], dp[i-1][1] + score[i]);
    }
    cout &lt;&lt; dp[n][1] &lt;&lt; endl;
}

int main(void) {
    ios_base::sync_with_stdio(false);
    cin.tie(nullptr);
    cout.tie(nullptr);

    int TC=1;
//    cin &gt;&gt; TC;
    FOR(tc, 1, TC) {
//        cout &lt;&lt; &quot;Case #&quot; &lt;&lt; tc &lt;&lt; &quot;: &quot;;
        solve();
    }


    return 0;
}
```</content:encoded></item><item><title><![CDATA[Why Does the Cost Function for Logistic Regression Look Like This?]]></title><description><![CDATA[Why Does the Cost Function for Logistic Regression Look Like This?]]></description><link>https://jinsoolve.netlify.app/posts/ml-textbook-1</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/ml-textbook-1</guid><pubDate>Wed, 25 Dec 2024 15:00:00 GMT</pubDate><content:encoded>
# Cost Function of Logistic Regression
The cost function of logistic regression is as follows:

$$
J(w) = -\frac{1}{m} \sum_{i=1}^{m}[y^{(i)}log(\sigma(z^{(i)})) + (1-y^{(i)})log(1-\sigma(z^{(i)}))]
$$

+ w: weights
+ $y^{(i)}$: the classification of the i-th data sample (0 or 1)
+ $z^{(i)}$: the log-odds of the i-th data sample
+ $\sigma(z^{(i)})$: the sigmoid of the log-odds of the i-th data sample (i.e., the probability that the i-th data sample belongs to the classification)

Why does the cost function look like this?
To understand this, we first need to understand the concept of log-odds.

# What is Log-Odds?
## What is Odds?
In linear regression, the prediction values range from $-\infin$ to $\infin$.
In logistic regression, however, there is a key difference: instead of predicting a value directly, it calculates a **log-odd**.

$$
odds = \frac{P(event \,occurring)}{P(event \,not \,occurring)} = \frac{P(y=1|x)}{1-P(y=1|x)} = \frac{p}{1-p}
$$

However, since the odds range from [0, $\infin$] and are asymmetric, they are difficult to use as-is.

## What is Logit (=Log-Odds)?
To address this, a log function is applied as follows:

$$
logit(p) = log(odds) =  log\frac{p}{1-p}
$$

However, Logit still has a range of [$-\infin$, $\infin$], which makes it unsuitable for direct use as probabilities.

## Logistic Function (=Sigmoid Function)
The sigmoid function is used to map the range to [0, 1], making it suitable for representing probabilities.
Let $\eta = logit(p)$, then:

$$
\sigma(\eta) = \frac{1}{1+\exp(-\eta)}
$$

The sigmoid function helps transform the logit into a probability range. For better understanding, let’s illustrate this with a graph:
![](./1.png)

This process leads to the well-known sigmoid function graph.
Now, we have some understanding of what $\sigma(z^{(i)})$ means.
But what does $-log(\sigma(z^{(i)}))$ represent?

# What does $-log(\sigma(z^{(i)}))$ represent?
Let’s plot the graph of $y=-log(x)$ (with log = ln):
![](./2.png)

Since $\sigma(z^{(i)})$ ranges from [0, 1], it looks like this:
![](./3.png)

The closer the prediction is to 1, the smaller the value of $-log(\sigma(z^{(i)}))$. Conversely, the further the prediction is from 1, the larger the value.
In other words, this represents a **loss function**: the worse the prediction, the higher the loss.

(Note that the cost function is the average of the loss functions across the entire dataset.)

$$
L(w) = y^{(i)}log(\sigma(z^{(i)})) + (1-y^{(i)})log(1-\sigma(z^{(i)}))
$$

$$
J(w) = \frac{1}{m}\sum_{i=1}^{m}L(w) = -\frac{1}{m} \sum_{i=1}^{m}[y^{(i)}log(\sigma(z^{(i)})) + (1-y^{(i)})log(1-\sigma(z^{(i)}))]
$$

Similarly, $-log(1-\sigma(z^{(i)}))$ can be considered the loss function when predicting 0.
![](./4.png)

Thus, for all m data samples:
When a sample belongs to class 1, add $-log(\sigma(z^{(i)}))$.
When a sample belongs to class 0, add $-log(1-\sigma(z^{(i)}))$.
This gives the total loss value.

Hence, the following equation holds:

$$
J(w) = -\frac{1}{m} \sum_{i=1}^{m}[y^{(i)}log(\sigma(z^{(i)})) + (1-y^{(i)})log(1-\sigma(z^{(i)}))]
$$

# References
- https://velog.io/@hyesoup/로지스틱-회귀-Logistic-Regression
- https://soobarkbar.tistory.com/12
- https://lucy-the-marketer.kr/ko/growth/logistic-regression/</content:encoded></item><item><title><![CDATA[로지스틱 회귀의 비용 함수는 왜 이렇게 생겼을까?]]></title><description><![CDATA[로지스틱 회귀의 비용 함수는 왜 이렇게 생겼을까?]]></description><link>https://jinsoolve.netlify.app/posts/ml-textbook-1</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/ml-textbook-1</guid><pubDate>Wed, 25 Dec 2024 15:00:00 GMT</pubDate><content:encoded>

# 로지스틱 회귀의 비용함수
로지스틱 회귀의 비용함수는 다음과 같이 생겼다.

$$
J(w) = -\frac{1}{m} \sum_{i=1}^{m}[y^{(i)}log(\sigma(z^{(i)})) + (1-y^{(i)})log(1-\sigma(z^{(i)}))]
$$

+ w: 가중치
+ $y^{(i)}$: i번째 데이터 샘플의 분류(0 혹은 1)
+ $z^{(i)}$: i번째 데이터 샘플의 log-oddo
+ $\sigma(z^{(i)}))$: i번째 데이터 샘플의 log-odd의 sigmoid (즉, i번째 데이터 샘플이 분류에 속할 확률)

그렇다면 비용함수는 왜 이렇게 생겨 먹은걸까?
이를 이해하기 위해서는 log-odd에 대해서 먼저 이해해야 한다.
# log-odd란 뭘까?
## Odds란?
선형 회귀에서는 $-\infin$ ~ $\infin$ 의 예측값을 반환한다.
로지스틱 회귀에서는 마지막 예측값 대신 **log-odd**라는 걸 구해야 한다는 차이가 있다.
$$
odds = \frac{P(event \,occurring)}{P(event \,not \,occurring)} = \frac{P(y=1|x)}{1-P(y=1|x)} = \frac{p}{1-p}
$$
하지만 Odds는 범위가 [0,$\infin$]이고, 비대칭성이기 때문에 사용하기 어렵다.
## Logit (=Log-odds)란?
이를 해결하기 위해 다음과 같이 로그함수를 취한다.
$$
logit(p) = log(odds) =  log\frac{p}{1-p}
$$
하지만 Logit은 확률로 사용하기 어려운 치역 범위 [$-\infin$,$\infin$]를 갖고 있기 때문에 마찬가지로 사용하기 어렵다.
## Logistic Function (=시그모이드 함수)
치역의 범위를 확률로 사용하기 좋게 [0,1]로 바꾸도록 도와주는 것이 이 시그모이드 함수이다.
$\eta = logit(p)$ 라 할 때
$$
\sigma(\eta) = \frac{1}{1+\exp(-\eta)}
$$
위와 같이 시그모이드 함수를 취한다.
이해하기 쉽게 그림으로 설명하자.
![](./1.png)
이러한 과정을 거쳐서 우리에게 익숙한(?) 시그모이드 함수의 그래프가 나온다.
이제 $\sigma(z^{(i)})$가 무엇을 의미하는지 대충 이해한 듯 하다. 그렇다면 $-log(\sigma(z^{(i)}))$는 무슨 의미일까?
# $-log(\sigma(z^{(i)}))$ 는 뭘 의미할까
$y=-log(x)$ 그래프를 한 번 그려보자. (단 log = ln이다.)
![](./2.png)
여기서 $\sigma(z^{(i)})$는 [0,1] 이므로 아래와 같다.
![](./3.png)
1임을 예측할수록 $-log(\sigma(z^{(i)}))$ 값이 줄어들고, 1임을 예측하지 못할수록 값이 커진다.
즉, 예측을 잘못할수록 값이 커진다. 이는 손실함수라고 말할 수 있을 것이다.

(참고로 비용함수는 모든 데이터셋의 손실함수에 대한 평균이다.)

$$
L(w) = y^{(i)}log(\sigma(z^{(i)})) + (1-y^{(i)})log(1-\sigma(z^{(i)}))
$$

$$
J(w) = \frac{1}{m}\sum_{i=1}^{m}L(w) = -\frac{1}{m} \sum_{i=1}^{m}[y^{(i)}log(\sigma(z^{(i)})) + (1-y^{(i)})log(1-\sigma(z^{(i)}))]
$$


그렇다면 $-log(1-\sigma(z^{(i)}))$도 같은 방식으로 0임을 예측하는 것의 손실함수라고 말할 수 있을 것이다.
![](./4.png)


따라서 모든 m개의 데이터 샘플에 대하여
데이터 샘플의 분류가 1일 때는 $-log(\sigma(z^{(i)}))$를 더하고,
데이터 샘플의 분류가 0일 때는 $-log(1-\sigma(z^{(i)}))$를 더하면 전체 손실값이 될 것이라는 것을 알 수 있다.

따라서 아래와 같은 식이 성립한다.
$$
J(w) = -\frac{1}{m} \sum_{i=1}^{m}[y^{(i)}log(\sigma(z^{(i)})) + (1-y^{(i)})log(1-\sigma(z^{(i)}))]
$$
# 참고
- https://velog.io/@hyesoup/로지스틱-회귀-Logistic-Regression
- https://soobarkbar.tistory.com/12
- https://lucy-the-marketer.kr/ko/growth/logistic-regression/
</content:encoded></item><item><title><![CDATA[2024년 여름 방학 계획]]></title><description><![CDATA[2024년 여름 방학 계획]]></description><link>https://jinsoolve.netlify.app/posts/2024-summer-vacation-plan</link><guid isPermaLink="false">https://jinsoolve.netlify.app/posts/2024-summer-vacation-plan</guid><pubDate>Fri, 05 Jul 2024 15:00:00 GMT</pubDate><content:encoded>

# 서론

드디어 2024년의 여름방학이 다가왔다.
군 휴학과 추가 1년 휴학으로 인해 3년 만의 복학 후 첫 학기라 너무 어색했지만 그래도 나름 잘 해쳐온 것 같다.
너무 오랜만에 대학교 학점 따기를 해서 중간고사 때는 많이 헤맸지만 다행히 기말고사 때 재활이 돼서 많이 복구할 수 있었다.  (~~무엇보다 중간고사 첫 시험 때 애플펜슬만 들고 간 것은... 허허~~)

3학년 1학기 때 경험한 바에 따르면 학기 중에는 개인 공부에 몰입할 수 있는 시간이 아무래도 줄어들 수 밖에 없었던 것 같다. 방학을 잘 활용하는 것이 내 실력적인 향상에 있어서 가장 중요한 시즌인 것 같다.

그만큼 방학 동안 시간활용을 잘 해서 최대한 많은 목표를 이뤄내야 할 것이다.
단순히 문제를 많이 풀고, 공부 시간을 늘리는 것도 좋지만 최근 내가 공부에 대해서 느끼는 것은 **많이 고민하고, 많이 실패하고, 그 안에서 많은 것을 배우고, 그리고 그것들을 잘 정리해서 내 것으로 만드는 것**이 무엇보다 중요한 것 같다.

지금까지 알고리즘에 (내 기준에) 나름 적지 않은 시간을 투자했지만 그 시간에 비해 실력이 아쉬운 것을 보면 확실히 이러한 부분들이 내게 부족했던 것 같다. 아무 생각없이 무작정 알고리즘을 푸는 것은 이제는 더 이상 크게 도움이 되지 않는 것 같다.

그 외로는, 공부 방법도 중요하지만 명확하고 현실 가능성이 있는 목표 또한 중요하다. 항상 열정적으로 무언가를 하기는 현실적으로 불가능하다고 생각한다. 하지만 현실적이고 명확한 목표는 꾸준히 일을 할 수 있게 해주는 것 같다.

그런 다짐(?)들과 생각들을 쓰면서 스스로를 명확하게 하기 위해 이렇게 일기 비슷한 계획을 쓰게 되었다.

# 본론
방학동안 해야 할 분야는 다음과 같다.
1. 알고리즘
2. 머신러닝
3. 영어

일단 1,2번에 대부분의 시간을 투자하고, 3번의 경우 최대한 자투리 시간을 잘 활용하는 게 핵심인 것 같다.

## 알고리즘
알고리즘은 일단 개인적인 실력 향상과 팀 연습이 있는데, 팀 연습의 경우 방학동안 주 2회 정도 학회 랩실에 모여서 팀원들이랑 지속적으로 시간을 잡고 문제셋을 풀어보고 있다.
끝나고 나서는 피드백을 통해 어떤 점이 아쉬었고 어떤 점을 개선할 수 있는 지를 생각했다. 팀으로서 손발이 잘 안 맞는 경우는 사실 몇 번 해보니 어느 정도 해결이 되었는데, 역시 개인적인 실력이 향상이 시급함을 다시 한 번 느꼈다.

그럼 개인적인 실력은 어떻게 향상해야 할까에 대해 많은 고민을 했는데 다행히 [raararaara](https://blog.naver.com/raararaara) 선배의 도움으로 가닥을 잡을 수 있게 되었다. 그의 어마무시한 알고리즘 실력 향상 커리큘럼을 듣고 나니... 내 실력 부족이 당연한 일이라는 걸 깨달았다. (주 3~4회의 버추얼과 그 코드의 리뷰 및 업솔빙, 그리고 typcial 문제들 계속 밀기 등...)

반성하고 초심으로 돌아가 차근차근 해 볼 예정이다.

### 1. 버추얼 및 라이브
주 2~3회의 버추얼 및 라이브를 생각하고 있다.
1. 월요일 코포 버추얼 스터디
2. 토요일 앳코더 라이브
3. ?요일 코포 라이브
4. leetcode weekly / biweekly contest
5. codechef

1~3은 웬만하면 무조건 기회가 될 때마다 할 예정이고, 4~5번 같은 경우 업솔빙 및 typical 문제들 밀기 등을 한 후 시간이 남으면 할 예정이다.

### 2. 업솔빙 및 코드리뷰
업솔빙의 경우는 정말 시간이 많이 들지만 이것만큼 중요한 것이 없다고 생각한다. 결정적으로 실력이 늘려면 업솔빙을 제대로 해야 한다.
하지만 실력에 비해 너무 어려운 문제를 업솔빙하는 것도 사실 가성비(?)가 좋지 않다고 생각한다. 그런 문제의 경우, &apos;그냥 그런 게 있대&apos; 이런 식의 리뷰가 될 가능성이 높기 때문이다.

난이도를 잘 보고 먹을(?) 수 있는 만큼 먹어보는 것이 좋다.
앳코더의 경우 민트 ~ 블루 정도까지 먹어보고,
코포는 블루~퍼플 정도까지 먹어볼 생각이다.

그리고 해당 아이디어들을 간단히 블로그나 깃헙에 정리해서 올릴 예정이다. 결국 정리해서 기억하지 못하면 의미가 없기 때문이다.

### 3. Typical 공부
사실 3번이 현재 가장 필요한 부분이다. 사실 알고리즘 대회라는 게 남들이 다 맞추는 걸 다 맞출 수 있어야 경쟁을 해 볼 수 있는 것이다. 사실 알고리즘의 경우 아는 것은 많지만 아직 숙련이 덜 되거나 바로 아이디어를 못 떠올릴 만큼 능숙하지 못 한 알고리즘들도 꽤 있기에 이를 정리해야 한다.

공부 방법은 다음과 같다.
1. 먼저 롸 선배가 추천한 AtcoderTypical90 문제를 쭉 밀어볼 생각이다.
하루에 최소 3문제 이상은 풀어서 한달 만에 끝내는 것이 목표다.
1. 알고리즘 정리
기존의 정리한 알고리즘들을 다시 깔끔하게 정리해서 블로그에 포스팅하는 것이 목표다.
그리고 각 코드의 템플릿을 깔끔하게 만들고 해당 알고리즘을 능숙하게 쓸 수 있도록 관련 문제를 여러개 풀어볼 예정이다.
1주일에 최소 1개이상은 포스팅할 예정이다.


## 머신러닝
머신러닝의 경우, 기초 공부를 1,2번 정도 가볍게 하긴 했으나 아직 부족한 부분들이 많다.
기본기를 잘 다지는 것이 목표다.

일단 [이유한님의 캐글 커리큘럼](https://kaggle-kr.tistory.com/32)을 방학동안 쭉 따라가 볼 생각이다.
어떤 식으로 모델링을 하고 데이터를 정제하는 지를 익숙하게 만들 생각이다. 그리고 가장 중요한 건 직접 캐글을 통해 퍼포먼스를 내보는 것이다. 읽고 정리하는 것과 직접 고민해서 모델을 훈련시키는 것은 완전히 다르기 때문이다.

캐글은 좋은 플랫폼이지만, 실전에 가까워서 사실 원론적인 공부에 있어서는 그리 좋지 않은 것 같다. 해당 모델을 사용하는 이유도 &apos;하다보니 잘 나와서 사용했다&apos; 라는 식의 생각이 많다. 따라서 직접 공부하면서 의문이 드는 부분을 정리해보고 관련 논문도 찾아볼 생각이다.

캐글을 공부하다가 나오는 기본 개념의 정리 같은 포스팅도 같이 공부할 생각이다.
또한 논문 리뷰 및 직접 구현을 할 계획이다. 머신러닝이 빠르게 발전하는 분야인 만큼 논문을 읽고 빠르게 적용할 줄 아는 능력이 필요하다고 생각하기 때문이다. 방학 동안에 관심 분야의 논문을 여러 개 읽고 직접 구현의 경우는 2개 이상 하는 것이 목표다.

우선 순위를 정리해보자.
1. 이유한님 캐글 커리큘럼은 대충 14개의 프로젝트로 이루어졌는데 각 프로젝트마다 3~4개 정도의 노트북들이 있다. 이를 하루에 한 프로젝트씩 끝내는 게 목표다.
빠르게 읽고 정리해보는 게 목표이므로 너무 상세하게 정리하는 시간을 낭비하지 않는 것이 중요하다.
2. 캐글 커리큘럼을 하면서 주마다 한 번씩 캐글 프로젝트를 직접 참여해서 처음부터 끝까지 작성해 보겠다.
끝나고 나면 어떤 부분을 생각 못 했는지 피드백을 하고, 해당 내용들을 정리해서 블로그에 포스팅할 예정이다.
3. 캐글 커리큘럼을 모두 끝내고 나면, 주 2회 이상 캐글 직접 구현을 하고 논문들을 읽기 시작할 것이다.
어떤 개념들이 있고, 어떤 것이 학회의 트렌드인지 파악할 수 있을 것이다. 논문 리뷰의 경우 주 2개 이상을 리뷰할 생각이고, 직접 구현은 그 중 하나를 잡아 직접 구현해 볼 것이다.
4. 마지막으로 위에서 공부하면서 내가 확실히 알아야 하는 지식들을 포스팅하여 정리할 것이다.

사실 머신러닝은 알고리즘보다 공부한 시간이 적어서 어떻게 공부하는 것이 옳거나 제대로인지는 잘 모르겠다.
하지만 중요한 건 머신러닝의 지식에 대해서 충분히 오랫동안 고민해보고 나만의 생각들을 정리를 잘 하는 것이라고 생각한다.
물론 초반에는 캐글의 퍼포를 올리기 위해 방법론들을 배우기 위해 노력하겠지만 기초를 다지고 나서는 좀 더 심화된 나만의 생각을 정리하는 것이 중요할 것이다.

## 영어
영어 즉, 회화의 경우는 사실 알고리즘이나 머신러닝으로도 충분히 시간을 많이 사용하기 때문에 추가적인 공부시간을 갖기 어렵다.
일단 기본적으로 주에 1회 정도 알고리즘 영어 스터디를 갖는데 이건 개인적으로 매우 만족 중이다. 처음 10분은 가볍게 small talk을 해서 일상적인 회화 연습을 하고 그 후부터는 해외 IT회사의 코테 면접처럼 문제 setter가 문제를 영어로 설명해주고 이를 영어로 대화하고 스터디원들과 서로 의논하면서 풀어가는 것이 스터디 내용이다.

처음 회화를 시작할 때 일단 말하는 것도 어색하고 무슨 말을 해야 할 지 모른다는 게 가장 큰 장벽이었다. (~~무엇보다 혼잣말을 하면서 연습해야 한다..~~) 그런데 영어 알고리즘 스터디를 하니 일단 무조건 말을 해야 하고, 거기에 알고리즘 관련 이야기니 나 같이 영어를 못 하는 사람도 일단 말할 게 생겨서 어떻게든 말해보려 하다보니 노하우도 생기고 자신감도 생겨서 반년 정도 하다보니 나름 기본적인 대화는 얼추 하는 것 같았다.

하지만 여전히 문법이나 풍부한 표현, 그리고 원어민들이 사용하는 표현 및 어휘들을 좀 더 공부할 필요성을 느꼈다. 스터디 덕분에 말하는 법의 기초는 어느 정도 뗀 것 같아서 그 이후의 스텝을 밟아볼 생각이다.

일단 유튜브 shorts 알고리즘에 영어 표현 관련 유튜브 채널을 넣어놨다 ㅋㅋ. 이게 생각보다 괜찮다. 지하철 이동할 때나 할 꺼 없을 때 이런 채널의 shorts영상을 보고 있으면 나름 얻어가는 것이 많다. 그리고 유튜브의 영상 중 영어로 된 영상을 자막을 달아주고 독특한 원어민스러운(?) 표현들을 알려주는 채널들도 꽤 많다. 위 채널을 활용하니 원어민들이 말하는 말들을 리스닝하는데 나름 도움이 되었다.

어휘의 경우, [voCat](https://vocat.devstory.co.kr/ko)이라는 앱을 사용하는데 개인적으로 매우 만족 중이다. 내가 저장한 어휘들을 내가 정해놓은 시간 간격마다 알람을 준다. 그럼 핸드폰의 알람을 보면서 해당 단어의 뜻을 다시 상기시킬 수 있다. 개인적으로 고등학교 때도 영단어는 이런 식으로 자투리 시간을 활용해서 암기했다.

그리고 마지막으로 가장 중요한 건 혼자말이나 생각을 영어로 혼자 중얼중얼거려보는 것이다. &apos;이런 말을 하려고 하는데 영어로 어떤 식으로 표현해야 맞는 걸까?&apos; 이걸 심심할 때마다 해보면 진짜 괜찮은 것 같다. 개인적으로 일본어를 한 때 이런 식으로 연습했는데, 전성기(?) 시절에는 일본 자유 여행을 가도 원어민과의 대화도 크게 문제는 없었다. (~~믈론 문맹이라 글은 못 읽었지만~~)

# 결론
계획을 주저리 주저리 쓰다보니 쓸데없이 길어진 것 같다. 그래도 내 나름대로의 생각과 목표들을 정리할 수 있어서 좋았다.
보는 사람은 정말 재미없고 지루한 글인 듯 하다가도 어차피 이 글을 누가 볼까 생각하니 없는 것 같아서 그냥 내 마음대로 적었다 ㅎㅎ.

후에 7월 말에서 8월 초 쯤 중간 점검 및 목표 달성률을 적기 위해서 또 포스팅할 생각이다.
내가 말한 목표들을 포기하지 않고 꾸준히 노력해서 이뤄냈으면 좋겠다.

마지막으로 최근에 내가 빠진 명언 2개를 적고 가겠다.

&gt; &quot;To get something you never had, you have to do something you never did&quot;

&gt; &quot;The magic you&apos;re looking for is in the work you&apos;re avoiding.&quot;

혹시 이 글을 보는 사람이 있다면, 위 명언들을 읽고 파이팅할 수 있었으면 좋겠다.</content:encoded></item></channel></rss>