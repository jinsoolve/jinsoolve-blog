{"componentChunkName":"component---src-templates-all-tag-post-page-tsx","path":"/tags/2/","result":{"data":{"allMdx":{"totalCount":47,"nodes":[{"frontmatter":{"thumbnail":null,"title":"백준 28129 - 2022 APC가 어려웠다고요?","updatedAt":null,"createdAt":"2025/03/28","description":null,"slug":"boj-28129","categories":["PS"],"tags":["다이나믹-프로그래밍","누적합"]},"excerpt":"위 포스트는 백준 28129 - 2022 APC가 어려웠다고요?의 풀이입니다. dp[i][j] := i번째 수가 j가 되는 경우의 수\n\ndp[i][j]=∑k=max(j−k,a[i−…"},{"frontmatter":{"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/f566903779599cee1d9c25328b1d3436/a5e5c/cover.png","srcSet":"/static/f566903779599cee1d9c25328b1d3436/13d64/cover.png 627w,\n/static/f566903779599cee1d9c25328b1d3436/5404f/cover.png 1253w,\n/static/f566903779599cee1d9c25328b1d3436/a5e5c/cover.png 2506w","sizes":"(min-width: 2506px) 2506px, 100vw"},"sources":[{"srcSet":"/static/f566903779599cee1d9c25328b1d3436/9ac18/cover.webp 627w,\n/static/f566903779599cee1d9c25328b1d3436/06a54/cover.webp 1253w,\n/static/f566903779599cee1d9c25328b1d3436/21a7f/cover.webp 2506w","type":"image/webp","sizes":"(min-width: 2506px) 2506px, 100vw"}]},"width":2506,"height":1368}}},"title":"Quantized Side Tuning 논문 리뷰","updatedAt":null,"createdAt":"2025/03/11","description":"Quantized Side Tuning: Fast and Memory-Efficient Tuning ofQuantized Large Language Models 논문에 대한 리뷰를 작성한 글입니다.","slug":"Quantized Side Tuning","categories":["paper"],"tags":null},"excerpt":"위 포스트는 Quantized Side Tuning: Fast and Memory-Efficient Tuning ofQuantized Large Language Models 논문에…"},{"frontmatter":{"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/2d55e147de7f0659e6dd23cf3fc2e0d6/aa5e3/cover.png","srcSet":"/static/2d55e147de7f0659e6dd23cf3fc2e0d6/453e9/cover.png 213w,\n/static/2d55e147de7f0659e6dd23cf3fc2e0d6/4261a/cover.png 426w,\n/static/2d55e147de7f0659e6dd23cf3fc2e0d6/aa5e3/cover.png 851w","sizes":"(min-width: 851px) 851px, 100vw"},"sources":[{"srcSet":"/static/2d55e147de7f0659e6dd23cf3fc2e0d6/e72da/cover.webp 213w,\n/static/2d55e147de7f0659e6dd23cf3fc2e0d6/c2584/cover.webp 426w,\n/static/2d55e147de7f0659e6dd23cf3fc2e0d6/ddbc9/cover.webp 851w","type":"image/webp","sizes":"(min-width: 851px) 851px, 100vw"}]},"width":851,"height":851}}},"title":"n-gram Language Models","updatedAt":null,"createdAt":"2025/03/10","description":null,"slug":"intro-to-nlp-2","categories":["ML"],"tags":["n-gram","sampling","model-evaluation","smoothing"]},"excerpt":"언어 모델이란, 결국에는 그 다음으로 어떤 단어가 오는 것이 가장 자연스러운지를 확률로 보고 가장 높은 확률의 단어를 선택해서 문장을 구성하는 방식이다. 위 포스트에서는 n…"},{"frontmatter":{"thumbnail":null,"title":"백준 1787 - 문자열의 주기 예측","updatedAt":null,"createdAt":"2025/03/05","description":null,"slug":"boj-1787","categories":["PS"],"tags":["다이나믹-프로그래밍","문자열","KMP"]},"excerpt":"위 포스트는 백준 1787 - 문자열의 주기 예측 의 해설입니다. 결국 부분 문자열에서 가장 짧으면서 일치하는 Prefix와 Suffix를 찾으면 된다. (해당 길이를 전체…"},{"frontmatter":{"thumbnail":null,"title":"백준 1055 - 끝이없음","updatedAt":null,"createdAt":"2025/03/05","description":null,"slug":"boj-1055","categories":["PS"],"tags":["구현","재귀"]},"excerpt":"위 포스트는 백준 1055 - 끝이없음의 해설입니다. 문자열이 재귀적으로 반복하는 것을 알 수 있다.\n\n이때 min과 max의 차이가 최대 100개 정도임을 알 수 있고, 우리는…"},{"frontmatter":{"thumbnail":null,"title":"백준 8872 - 빌라봉","updatedAt":null,"createdAt":"2025/03/04","description":null,"slug":"boj-8872","categories":["PS"],"tags":["트리","그래프-탐색","그래프-이론","깊이-우선-탐색"]},"excerpt":"위 포스트는 백준 8872 - 빌라봉 문제의 해설입니다. 위 문제에는 여러 개의 트리가 존재한다.\n 임의의 2개의 트리를 서로 이을 때 최대 시간이 최소가 되게 하기 위해서는 각…"},{"frontmatter":{"thumbnail":null,"title":"백준 6569 - 몬드리안의 꿈","updatedAt":null,"createdAt":"2025/02/28","description":null,"slug":"boj-6569","categories":["PS"],"tags":["다이나믹-프로그래밍","비트마스킹"]},"excerpt":"위 포스트는 백준 6569 - 몬드리안의 꿈에 대한 해설입니다. ...\t ...\t ...\t ...\t ...\t ...\t ...\t ... 채워짐\t 채워짐\t 채워짐\t 채워짐\t 채워짐\t 채워짐…"},{"frontmatter":{"thumbnail":null,"title":"백준 24979 - COW Operations","updatedAt":null,"createdAt":"2025/02/28","description":null,"slug":"boj-24979","categories":["PS"],"tags":["문자열","누적합","애드-혹"]},"excerpt":"위 포스트는 백준 24979 - COW Operations에 대한 해설입니다. 아이디어1#\n\n주어진 Operation을 해보면 아래와 같은 변환이 가능하다는 것을 알 수 있다.…"},{"frontmatter":{"thumbnail":null,"title":"백준 1646 - 피이보나치 트리","updatedAt":null,"createdAt":"2025/02/26","description":null,"slug":"boj-1646","categories":["PS"],"tags":["다이나믹-프로그래밍","트리","재귀"]},"excerpt":"위 포스트는 백준 1646 - 피이보나치 트리 문제에 대한 해설입니다. 관찰1#\n\nn번 피이보나치 트리의 루트의 왼쪽 서브트리는 n-2번 째 피이보나치 트리이고, 오른쪽 서브트리는…"}],"pageInfo":{"currentPage":2,"pageCount":6}},"ogimage":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/f689f8b3b021090c7d4495f61be56e2f/ee21c/og-image.png","srcSet":"/static/f689f8b3b021090c7d4495f61be56e2f/5fe58/og-image.png 256w,\n/static/f689f8b3b021090c7d4495f61be56e2f/5ca6c/og-image.png 512w,\n/static/f689f8b3b021090c7d4495f61be56e2f/ee21c/og-image.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/f689f8b3b021090c7d4495f61be56e2f/e818d/og-image.webp 256w,\n/static/f689f8b3b021090c7d4495f61be56e2f/7948c/og-image.webp 512w,\n/static/f689f8b3b021090c7d4495f61be56e2f/aa8b2/og-image.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":600}},"profileImage":null,"shortPosts":{"nodes":[{"frontmatter":{"title":"Word2Vec과 GloVe의 차이는 무엇일까?","updatedAt":null,"createdAt":"2025/04/27","slug":"diff-between-word2vec-and-glove","tags":null}},{"frontmatter":{"title":"Cross Entropy와 Softmax","updatedAt":null,"createdAt":"2025/04/27","slug":"cross-entropy-and-softmax","tags":null}},{"frontmatter":{"title":"Batch Normalization을 하는 이유는 뭘까?","updatedAt":null,"createdAt":"2025/04/25","slug":"why-do-batch-normalization","tags":null}},{"frontmatter":{"title":"삼격형의 두 변의 길이와 사잇각을 알 때, 나머지 한 변의 길이를 구하는 공식","updatedAt":null,"createdAt":"2025/02/20","slug":"formula-for-the-third-side-of-a-triangle-given-two-sides-and-the-included-angle","tags":["수학-공식"]}},{"frontmatter":{"title":"c++에서 내림차순에 대한 lower_bound와 upper_bound하기","updatedAt":null,"createdAt":"2025/02/10","slug":"descending-order-binary-search","tags":[]}},{"frontmatter":{"title":"세 점의 좌표가 주어졌을 때 삼각형의 면적 구하는 방법","updatedAt":null,"createdAt":"2025/01/13","slug":"how-to-calculate-triangle-area","tags":["수학-공식"]}},{"frontmatter":{"title":"C++에서 이진수의 비트 수를 세는 방법","updatedAt":null,"createdAt":"2025/01/13","slug":"count-bit-in-cpp","tags":[]}},{"frontmatter":{"title":"표준화와 정규화","updatedAt":null,"createdAt":"2025/01/09","slug":"ml-textbook-4","tags":["표준화","정규화"]}},{"frontmatter":{"title":"로지스틱 회귀의 비용 함수는 왜 이렇게 생겼을까?","updatedAt":null,"createdAt":"2024/12/26","slug":"ml-textbook-1","tags":["로지스틱","회귀","손실함수"]}}]},"featuredPosts":{"nodes":[{"frontmatter":{"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/076eddc91e0d4685f11210bb0b088b7d/06a5c/cover.png","srcSet":"/static/076eddc91e0d4685f11210bb0b088b7d/2ed3a/cover.png 142w,\n/static/076eddc91e0d4685f11210bb0b088b7d/f4be8/cover.png 284w,\n/static/076eddc91e0d4685f11210bb0b088b7d/06a5c/cover.png 568w","sizes":"(min-width: 568px) 568px, 100vw"},"sources":[{"srcSet":"/static/076eddc91e0d4685f11210bb0b088b7d/bcdd4/cover.webp 142w,\n/static/076eddc91e0d4685f11210bb0b088b7d/5d551/cover.webp 284w,\n/static/076eddc91e0d4685f11210bb0b088b7d/47346/cover.webp 568w","type":"image/webp","sizes":"(min-width: 568px) 568px, 100vw"}]},"width":568,"height":568}}},"title":"RNN","updatedAt":null,"createdAt":"2025/04/27","description":null,"slug":"intro-to-nlp-7","categories":["ML"],"tags":null},"excerpt":"언어 모델이라는 건, 사실 다음에 올 단어를 확률로 예측하는 것이다. 이러한 언어 모델들을 어떻게 발전시켜왔는 지 살펴보자.\n\n이미 이전 포스트에서 자세히 살펴보았던 내용이다.…"},{"frontmatter":{"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/85dad8627ae6845b62f5bb965c291b19/808a1/cover.png","srcSet":"/static/85dad8627ae6845b62f5bb965c291b19/74f11/cover.png 294w,\n/static/85dad8627ae6845b62f5bb965c291b19/bbc95/cover.png 588w,\n/static/85dad8627ae6845b62f5bb965c291b19/808a1/cover.png 1176w","sizes":"(min-width: 1176px) 1176px, 100vw"},"sources":[{"srcSet":"/static/85dad8627ae6845b62f5bb965c291b19/c143a/cover.webp 294w,\n/static/85dad8627ae6845b62f5bb965c291b19/e86a6/cover.webp 588w,\n/static/85dad8627ae6845b62f5bb965c291b19/8199d/cover.webp 1176w","type":"image/webp","sizes":"(min-width: 1176px) 1176px, 100vw"}]},"width":1176,"height":522}}},"title":"Word2Vec","updatedAt":null,"createdAt":"2025/04/26","description":null,"slug":"intro-to-nlp-4","categories":["ML"],"tags":null},"excerpt":"저번에 word vector에 대해서 알아봤는데, 이번에는 word vector의 프레임워크인 Word2Vec에 대해서 좀 더 알아보자. 먼저 Word2Vec이 어떤 식으로…"},{"frontmatter":{"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/3984361aed6a71f465c0ae457e7eff24/558a4/cover.png","srcSet":"/static/3984361aed6a71f465c0ae457e7eff24/0701a/cover.png 164w,\n/static/3984361aed6a71f465c0ae457e7eff24/4c324/cover.png 328w,\n/static/3984361aed6a71f465c0ae457e7eff24/558a4/cover.png 656w","sizes":"(min-width: 656px) 656px, 100vw"},"sources":[{"srcSet":"/static/3984361aed6a71f465c0ae457e7eff24/5da8b/cover.webp 164w,\n/static/3984361aed6a71f465c0ae457e7eff24/fab23/cover.webp 328w,\n/static/3984361aed6a71f465c0ae457e7eff24/31f9d/cover.webp 656w","type":"image/webp","sizes":"(min-width: 656px) 656px, 100vw"}]},"width":656,"height":381}}},"title":"Word Vectors","updatedAt":null,"createdAt":"2025/04/25","description":null,"slug":"intro-to-nlp-3","categories":["ML"],"tags":null},"excerpt":"이번 포스트에서는 Word Vector에 대해서 알아보자. 먼저 아래와 같은 고민을 해보자.\n 우리는 어떻게 컴퓨터에게 단어의 뜻을 이해시킬 수 있을까?\n\nWordNet?#\n\n예전에…"},{"frontmatter":{"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/f566903779599cee1d9c25328b1d3436/a5e5c/cover.png","srcSet":"/static/f566903779599cee1d9c25328b1d3436/13d64/cover.png 627w,\n/static/f566903779599cee1d9c25328b1d3436/5404f/cover.png 1253w,\n/static/f566903779599cee1d9c25328b1d3436/a5e5c/cover.png 2506w","sizes":"(min-width: 2506px) 2506px, 100vw"},"sources":[{"srcSet":"/static/f566903779599cee1d9c25328b1d3436/9ac18/cover.webp 627w,\n/static/f566903779599cee1d9c25328b1d3436/06a54/cover.webp 1253w,\n/static/f566903779599cee1d9c25328b1d3436/21a7f/cover.webp 2506w","type":"image/webp","sizes":"(min-width: 2506px) 2506px, 100vw"}]},"width":2506,"height":1368}}},"title":"Quantized Side Tuning 논문 리뷰","updatedAt":null,"createdAt":"2025/03/11","description":"Quantized Side Tuning: Fast and Memory-Efficient Tuning ofQuantized Large Language Models 논문에 대한 리뷰를 작성한 글입니다.","slug":"Quantized Side Tuning","categories":["paper"],"tags":null},"excerpt":"위 포스트는 Quantized Side Tuning: Fast and Memory-Efficient Tuning ofQuantized Large Language Models 논문에…"}]}},"pageContext":{"limit":9,"skip":9,"numPages":6,"currentPage":2}},"staticQueryHashes":["3461282698","445630073"],"slicesMap":{}}