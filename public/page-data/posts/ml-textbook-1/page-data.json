{"componentChunkName":"component---src-templates-post-page-tsx-content-file-path-users-jinsoo-git-jinsoolve-blog-content-ml-ml-textbook-1-content-mdx","path":"/posts/ml-textbook-1/","result":{"data":{"post":{"frontmatter":{"slug":"ml-textbook-1","title":"로지스틱 회귀의 비용 함수는 왜 이렇게 생겼을까?","locale":null,"description":"로지스틱 회귀의 비용 함수는 왜 이렇게 생겼을까?","categories":["ML","short"],"tags":["로지스틱","회귀","손실함수"],"createdAt":"2024/12/26","updatedAt":null,"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c8138af222d0a534b9d3f8006c1ec625/c497a/cover.png","srcSet":"/static/c8138af222d0a534b9d3f8006c1ec625/88418/cover.png 193w,\n/static/c8138af222d0a534b9d3f8006c1ec625/5b669/cover.png 385w,\n/static/c8138af222d0a534b9d3f8006c1ec625/c497a/cover.png 770w","sizes":"(min-width: 770px) 770px, 100vw"},"sources":[{"srcSet":"/static/c8138af222d0a534b9d3f8006c1ec625/e91f5/cover.webp 193w,\n/static/c8138af222d0a534b9d3f8006c1ec625/94672/cover.webp 385w,\n/static/c8138af222d0a534b9d3f8006c1ec625/75d07/cover.webp 770w","type":"image/webp","sizes":"(min-width: 770px) 770px, 100vw"}]},"width":770,"height":709.9999999999999}}}},"myTableOfContents":{"items":[{"depth":1,"title":"로지스틱 회귀의 비용함수","url":"#my-heading-1","items":[]},{"depth":1,"title":"log-odd란 뭘까?","url":"#my-heading-2","items":[{"depth":2,"title":"Odds란?","url":"#my-heading-3","items":[]},{"depth":2,"title":"Logit (=Log-odds)란?","url":"#my-heading-4","items":[]},{"depth":2,"title":"Logistic Function (=시그모이드 함수)","url":"#my-heading-5","items":[]}]},{"depth":1,"title":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo>−</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy=\"false\">(</mo><mi>σ</mi><mo stretchy=\"false\">(</mo><msup><mi>z</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">-log(\\sigma(z^{(i)}))</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.138em;vertical-align:-0.25em;\"></span><span class=\"mord\">−</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.888em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathnormal mtight\">i</span><span class=\"mclose mtight\">)</span></span></span></span></span></span></span></span></span><span class=\"mclose\">))</span></span></span></span> 는 뭘 의미할까","url":"#my-heading-6","items":[]},{"depth":1,"title":"참고","url":"#my-heading-7","items":[]}]}},"otherLocalePost":{"nodes":[{"frontmatter":{"locale":"en"}},{"frontmatter":{"locale":null}}]},"relatedPosts":{"nodes":[{"frontmatter":{"slug":"diff-between-word2vec-and-glove","title":"Word2Vec과 GloVe의 차이는 무엇일까?","description":null,"categories":["short"],"tags":null,"createdAt":"2025/04/27","updatedAt":null,"thumbnail":null},"excerpt":"그래서 Word2Vec과 GloVe의 차이는 대체 뭘까? 개인적으로 매우 헷갈리는 topic이라서 내 개인 이해를 정리해 보았다.\n\n먼저 standard한 Word2Vec과…"},{"frontmatter":{"slug":"cross-entropy-and-softmax","title":"Cross Entropy와 Softmax","description":null,"categories":["short"],"tags":null,"createdAt":"2025/04/27","updatedAt":null,"thumbnail":null},"excerpt":"Cross-entropy와 Softmax가 자꾸 헷갈려서 정리를 한 번 해보자. +) 추가로 sigmoid와 tanh 도 보자.\n\nSoftmax(zi)=ezi∑jezj\\text…"},{"frontmatter":{"slug":"intro-to-nlp-7","title":"RNN","description":null,"categories":["ML"],"tags":null,"createdAt":"2025/04/27","updatedAt":null,"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/076eddc91e0d4685f11210bb0b088b7d/06a5c/cover.png","srcSet":"/static/076eddc91e0d4685f11210bb0b088b7d/2ed3a/cover.png 142w,\n/static/076eddc91e0d4685f11210bb0b088b7d/f4be8/cover.png 284w,\n/static/076eddc91e0d4685f11210bb0b088b7d/06a5c/cover.png 568w","sizes":"(min-width: 568px) 568px, 100vw"},"sources":[{"srcSet":"/static/076eddc91e0d4685f11210bb0b088b7d/bcdd4/cover.webp 142w,\n/static/076eddc91e0d4685f11210bb0b088b7d/5d551/cover.webp 284w,\n/static/076eddc91e0d4685f11210bb0b088b7d/47346/cover.webp 568w","type":"image/webp","sizes":"(min-width: 568px) 568px, 100vw"}]},"width":568,"height":568}}}},"excerpt":"언어 모델이라는 건, 사실 다음에 올 단어를 확률로 예측하는 것이다. 이러한 언어 모델들을 어떻게 발전시켜왔는 지 살펴보자.\n\n이미 이전 포스트에서 자세히 살펴보았던 내용이다.…"},{"frontmatter":{"slug":"intro-to-nlp-8","title":"Fancy RNN","description":null,"categories":["ML"],"tags":null,"createdAt":"2025/04/27","updatedAt":null,"thumbnail":null},"excerpt":"이전 포스트에서 RNN에서 Vanishing Gradient로 인해 장기 의존성 문제가 있다는 사실을 이야기했다. 이런 Vanishing Gradient를 해결하기 위해 크게…"},{"frontmatter":{"slug":"intro-to-nlp-9","title":"Attention","description":null,"categories":["ML"],"tags":null,"createdAt":"2025/04/27","updatedAt":null,"thumbnail":null},"excerpt":"기존 RNN의 병목 현상을 해결하기 위해 Attention이 등장했다. Decoder에서 한 단어를 예상할 때, 해당 단어와 특별히 관련되어 있는 Encoder의 특정 단어를…"},{"frontmatter":{"slug":"intro-to-nlp-10","title":"Transformers","description":null,"categories":["ML"],"tags":null,"createdAt":"2025/04/27","updatedAt":null,"thumbnail":null},"excerpt":"Seq2Seq에서 RNN을 아예 빼버리고 attention으로 구성해보면 어떨까? → Transformer의 구조 전에 seq2seq에서 decoder의 query와…"},{"frontmatter":{"slug":"intro-to-nlp-4","title":"Word2Vec","description":null,"categories":["ML"],"tags":null,"createdAt":"2025/04/26","updatedAt":null,"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/85dad8627ae6845b62f5bb965c291b19/808a1/cover.png","srcSet":"/static/85dad8627ae6845b62f5bb965c291b19/74f11/cover.png 294w,\n/static/85dad8627ae6845b62f5bb965c291b19/bbc95/cover.png 588w,\n/static/85dad8627ae6845b62f5bb965c291b19/808a1/cover.png 1176w","sizes":"(min-width: 1176px) 1176px, 100vw"},"sources":[{"srcSet":"/static/85dad8627ae6845b62f5bb965c291b19/c143a/cover.webp 294w,\n/static/85dad8627ae6845b62f5bb965c291b19/e86a6/cover.webp 588w,\n/static/85dad8627ae6845b62f5bb965c291b19/8199d/cover.webp 1176w","type":"image/webp","sizes":"(min-width: 1176px) 1176px, 100vw"}]},"width":1176,"height":522}}}},"excerpt":"저번에 word vector에 대해서 알아봤는데, 이번에는 word vector의 프레임워크인 Word2Vec에 대해서 좀 더 알아보자. 먼저 Word2Vec이 어떤 식으로…"},{"frontmatter":{"slug":"why-do-batch-normalization","title":"Batch Normalization을 하는 이유는 뭘까?","description":null,"categories":["short"],"tags":null,"createdAt":"2025/04/25","updatedAt":null,"thumbnail":null},"excerpt":"각 batch 데이터마다 분포가 다르게 되면 모델의 학습이 어렵다. 즉, 전에는 0 ~ 20 데이터가 들어와서 그거대로 학습했는데, 이번 batch에는 2000 ~ 4000…"},{"frontmatter":{"slug":"intro-to-nlp-3","title":"Word Vectors","description":null,"categories":["ML"],"tags":null,"createdAt":"2025/04/25","updatedAt":null,"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/3984361aed6a71f465c0ae457e7eff24/558a4/cover.png","srcSet":"/static/3984361aed6a71f465c0ae457e7eff24/0701a/cover.png 164w,\n/static/3984361aed6a71f465c0ae457e7eff24/4c324/cover.png 328w,\n/static/3984361aed6a71f465c0ae457e7eff24/558a4/cover.png 656w","sizes":"(min-width: 656px) 656px, 100vw"},"sources":[{"srcSet":"/static/3984361aed6a71f465c0ae457e7eff24/5da8b/cover.webp 164w,\n/static/3984361aed6a71f465c0ae457e7eff24/fab23/cover.webp 328w,\n/static/3984361aed6a71f465c0ae457e7eff24/31f9d/cover.webp 656w","type":"image/webp","sizes":"(min-width: 656px) 656px, 100vw"}]},"width":656,"height":381}}}},"excerpt":"이번 포스트에서는 Word Vector에 대해서 알아보자. 먼저 아래와 같은 고민을 해보자.\n 우리는 어떻게 컴퓨터에게 단어의 뜻을 이해시킬 수 있을까?\n\nWordNet?#\n\n예전에…"},{"frontmatter":{"slug":"intro-to-nlp-2","title":"n-gram Language Models","description":null,"categories":["ML"],"tags":["n-gram","sampling","model-evaluation","smoothing"],"createdAt":"2025/03/10","updatedAt":null,"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/2d55e147de7f0659e6dd23cf3fc2e0d6/aa5e3/cover.png","srcSet":"/static/2d55e147de7f0659e6dd23cf3fc2e0d6/453e9/cover.png 213w,\n/static/2d55e147de7f0659e6dd23cf3fc2e0d6/4261a/cover.png 426w,\n/static/2d55e147de7f0659e6dd23cf3fc2e0d6/aa5e3/cover.png 851w","sizes":"(min-width: 851px) 851px, 100vw"},"sources":[{"srcSet":"/static/2d55e147de7f0659e6dd23cf3fc2e0d6/e72da/cover.webp 213w,\n/static/2d55e147de7f0659e6dd23cf3fc2e0d6/c2584/cover.webp 426w,\n/static/2d55e147de7f0659e6dd23cf3fc2e0d6/ddbc9/cover.webp 851w","type":"image/webp","sizes":"(min-width: 851px) 851px, 100vw"}]},"width":851,"height":851}}}},"excerpt":"언어 모델이란, 결국에는 그 다음으로 어떤 단어가 오는 것이 가장 자연스러운지를 확률로 보고 가장 높은 확률의 단어를 선택해서 문장을 구성하는 방식이다. 위 포스트에서는 n…"},{"frontmatter":{"slug":"formula-for-the-third-side-of-a-triangle-given-two-sides-and-the-included-angle","title":"삼격형의 두 변의 길이와 사잇각을 알 때, 나머지 한 변의 길이를 구하는 공식","description":null,"categories":["short"],"tags":["수학-공식"],"createdAt":"2025/02/20","updatedAt":null,"thumbnail":null},"excerpt":"위와 같은 삼각형이 존재할 때, 변의 길이 b, c와 그 사잇각 α\\alphaα를 알고 있다고 가정하자. 이때 a의 길이를 구하는 공식은 다음과 같다. a=b2+c2−2bccos…"},{"frontmatter":{"slug":"descending-order-binary-search","title":"c++에서 내림차순에 대한 lower_bound와 upper_bound하기","description":null,"categories":["short"],"tags":[],"createdAt":"2025/02/10","updatedAt":null,"thumbnail":null},"excerpt":"내림차순 정렬이 되어있을 때의 lower_bound와 upper_bound 사용법 lower_bound(v.begin(), v.end(), num, greater<int>());…"},{"frontmatter":{"slug":"how-to-calculate-triangle-area","title":"세 점의 좌표가 주어졌을 때 삼각형의 면적 구하는 방법","description":null,"categories":["short"],"tags":["수학-공식"],"createdAt":"2025/01/13","updatedAt":null,"thumbnail":null},"excerpt":"세 점의 좌표가 주어졌을 때 어떻게 빠르게 삼각형의 너비를 계산할 수 있을까? 세 점 A(x1,y1),B(x2,y2),C(x3,y3)A(x_1, y_1), B(x_2, y_2),…"},{"frontmatter":{"slug":"count-bit-in-cpp","title":"C++에서 이진수의 비트 수를 세는 방법","description":null,"categories":["short"],"tags":[],"createdAt":"2025/01/13","updatedAt":null,"thumbnail":null},"excerpt":"101101101101101101 이런 이진수가 주어질 때 C++에서 1의 개수를 세려면 어떻게 해야 할까? // unsigned int\n__builtin_popcount();…"},{"frontmatter":{"slug":"ml-textbook-9","title":"머신러닝 교과서 8장","description":"자연어 처리(Natural Language Processing, NLP)의 하위 분야인 감성 분석(sentiment analysis)에 대해서 알아보는 장이다.","categories":["ML"],"tags":["NLP","감정-분석"],"createdAt":"2025/01/09","updatedAt":null,"thumbnail":null},"excerpt":"자연어 처리(Natural Language Processing, NLP)의 하위 분야인 감성 분석(sentiment analysis)에 대해서 알아보는 장이다. 영화 리뷰를 읽고…"},{"frontmatter":{"slug":"ml-textbook-8","title":"머신러닝 교과서 7장: 앙상블 학습","description":"머신러닝 교과서 7장의 앙상블 학습에 대해서 알아보자.","categories":["ML"],"tags":["앙상블"],"createdAt":"2025/01/09","updatedAt":null,"thumbnail":null},"excerpt":"다수결 투표 기반 모델(보팅) -> 최빈값을 선택 랜덤하게 복원추출하여(배깅) 과대적합 감소 앞선 모델의 오차를 학습하여 강력 모델 구축(부스팅) 앙상블 모델은 거의 항상…"},{"frontmatter":{"slug":"ml-textbook-7","title":"모델 평가와 하이퍼 파라미터 튜닝의 모범 사례","description":"모델 평가와 하이퍼 파라미터 튜닝의 모범 사례에 대해서","categories":["ML"],"tags":["hyperparameter-tuning"],"createdAt":"2025/01/09","updatedAt":null,"thumbnail":null},"excerpt":"해당 내용은 머신러닝 교과서 with 파이썬, 사이킷런, 텐서플로의 장 내용을 기반으로 작성되었다. 우리는 모델을 훈련시킬 때 데이터를 전처리한 후에 학습시킨다. 그럼 테스트…"},{"frontmatter":{"slug":"ml-textbook-4","title":"표준화와 정규화","description":"표준화와 정규화에 대해서 간단히 알아보자.","categories":["ML","short"],"tags":["표준화","정규화"],"createdAt":"2025/01/09","updatedAt":null,"thumbnail":null},"excerpt":"ex = np.array([0, 1, 2, 3, 4, 5]) print('표준화:', (ex - ex.mean()) / ex.std())\n\n\n평균=0, 분산=1로 만든다.\n\nfr…"},{"frontmatter":{"slug":"ml-textbook-6","title":"차원 축소를 사용한 데이터 압축","description":"차원 축소 기법에 대해서 알아보자.","categories":["ML"],"tags":["PCA","LDA","차원축소"],"createdAt":"2025/01/09","updatedAt":null,"thumbnail":null},"excerpt":"이 글은 머신러닝 교과서 with 파이썬, 사이킷런, 텐서플로의 5장 내용을 기반으로 작성되었다. 차원 축소를 하는 이유는 다양하다. 저장 공간을 줄이고 계산 효율을 높이고…"},{"frontmatter":{"slug":"ml-textbook-5","title":"데이터 전처리","description":"데이터 전처리에 대해서 정리해 보았다.","categories":["ML"],"tags":null,"createdAt":"2025/01/09","updatedAt":null,"thumbnail":null},"excerpt":"해당 내용은 머신러닝 교과서 with 파이썬, 사이킷런, 텐서플로의 4장 내용을 기반으로 작성되었다. isnull().sum()등을 이용하여 누락된 값을 확인한다.\n\n누락된…"},{"frontmatter":{"slug":"ml-textbook-14","title":"15장 심층 합성곱 신경망으로 이미지 분류","description":"합성곱 신경망은 컴퓨터 비전을 위한 머신러닝 분야를 크게 발전시켰다.CNN(Convolutional Neural Network)는 뇌의 시각 피질이 물체를 인식할 때 동작하는 방식에서 영감을 얻었다고 한다.","categories":["ML"],"tags":null,"createdAt":"2025/01/09","updatedAt":null,"thumbnail":null},"excerpt":"합성곱 신경망은 컴퓨터 비전을 위한 머신러닝 분야를 크게 발전시켰다. CNN(Convolutional Neural Network)는 뇌의 시각 피질이 물체를 인식할 때 동작하는…"},{"frontmatter":{"slug":"ml-textbook-15","title":"16장 순환 신경망으로 순차 데이터 모델링","description":"데이터 간의 순서가 중요한 데이터를 시퀀스 데이터라고 한다. 시퀀스 데이터를 처리해주기 위해서는 RNN(Recurrent Neural Network) 모델을 사용해서 처리해야 한다.","categories":["ML"],"tags":null,"createdAt":"2025/01/09","updatedAt":null,"thumbnail":null},"excerpt":"데이터 간의 순서가 중요한 데이터를 시퀀스 데이터라고 한다. 시퀀스 데이터를 처리해주기 위해서는 RNN(Recurrent Neural Network) 모델을 사용해서 처리해야 한다…"},{"frontmatter":{"slug":"ml-textbook-13","title":"13장 텐서플로를 사용한 신경망 훈련","description":"복잡한 수학이나 구현 과정을 텐서플로우에서 이미 구현해 놓았다. 이를 사용하는 법을 알아보자.텐서플로우의 함수나 여러가지 기능들은 어느정도 생략하겠다. 사용하면서 익히는 것이 가장 좋다.","categories":["ML"],"tags":null,"createdAt":"2025/01/09","updatedAt":null,"thumbnail":null},"excerpt":"복잡한 수학이나 구현 과정을 텐서플로우에서 이미 구현해 놓았다. 이를 사용하는 법을 알아보자. 텐서플로우의 함수나 여러가지 기능들은 어느정도 생략하겠다. 사용하면서 익히는 것이…"},{"frontmatter":{"slug":"ml-textbook-11","title":"11장 레이블되지 않은 데이터 다루기: 군집 분석","description":"레이블이 없는 데이터들을 분석하여 비슷한 데이터들끼리 그룹으로 묶을 것이다.이를 군집으로 묶는다하여 클러스터링(clustering)이라 한다. - k-평균 알고리즘을 이용하여 클러스터 중심 찾기- 상향식 방법으로 계층적 군집 트리 만들기- 밀집도 기반의 군집 알고리즘을 사용하여 임의 모야을 가진 대상 구분하기","categories":["ML"],"tags":null,"createdAt":"2025/01/09","updatedAt":null,"thumbnail":null},"excerpt":"레이블이 없는 데이터들을 분석하여 비슷한 데이터들끼리 그룹으로 묶을 것이다. 이를 군집으로 묶는다하여 클러스터링(clustering)이라 한다.\n\nk-평균 알고리즘을 이용하여…"},{"frontmatter":{"slug":"ml-textbook-10","title":"머신러닝 교과서 10장","description":"선형 회귀 모델에 대해 알아보자. 보스턴 집 가격 예측 문제를 예시로 들어서 설명하겠다.","categories":["ML"],"tags":["Linear-Regression"],"createdAt":"2025/01/09","updatedAt":null,"thumbnail":null},"excerpt":"선형 회귀 모델에 대해 알아보자. 보스턴 집 가격 예측 문제를 예시로 들어서 설명하겠다.\n\n데이터 셋을 상관관계나 산점도를 그려 분석한다.\n\n사이킷런의…"},{"frontmatter":{"slug":"ml-textbook-12","title":"12장 다층 인공 신경망을 밑바닥부터 구현","description":"딥러닝은 인공 신경망을 효과적으로 학습시키기 위한 머신러닝의 하위분야이다. 아래 내용을 소개하겠다.- 다층 신경망 개념- 역전파 알고리즘- 이미지 분류를 위한 다층 신경망 훈련","categories":["ML"],"tags":null,"createdAt":"2025/01/09","updatedAt":null,"thumbnail":null},"excerpt":"딥러닝은 인공 신경망을 효과적으로 학습시키기 위한 머신러닝의 하위분야이다. 아래 내용을 소개하겠다. 다층 신경망 개념 역전파 알고리즘 이미지 분류를 위한 다층 신경망 훈련\n\n인…"},{"frontmatter":{"slug":"ml-textbook-2","title":"가중치 w와 L2 규제","description":"왜 norm을 사용하는 것이 과대적합을 해결할까? 그 이유를 살펴보자.","categories":["ML"],"tags":["규제","과대적합"],"createdAt":"2025/01/06","updatedAt":null,"thumbnail":null},"excerpt":"모델이 과대적합이 되었을 때 우리는 norm을 통해 이를 해결한다. 그런데 왜 norm을 사용하는 것이 과대적합을 해결할까? 그 이유를 살펴보자.\n\n모델이 과도하게 훈련 데이터에만…"},{"frontmatter":{"slug":"ml-textbook-3","title":"머신러닝 분류 모델들","description":"머신러닝에 사용하는 다양한 분류 모델들에 대해서 얘기해 보겠다.","categories":["ML"],"tags":["SVM","커널-SVM","결정-트리","랜덤-포레스트"],"createdAt":"2025/01/06","updatedAt":null,"thumbnail":null},"excerpt":"머신러닝에 사용하는 다양한 분류 모델들에 대해서 얘기해 보겠다. SVM(Support Vector Machine)이라 불리는 이 분류 모델은 클래스를 구분하는 hyper…"}]}},"pageContext":{"categories":["ML","short"],"tags":["로지스틱","회귀","손실함수"],"slug":"ml-textbook-1","id":"d75e01ef-0299-5095-8846-8aaf23c44151","myTableOfContents":{"items":[{"depth":1,"title":"로지스틱 회귀의 비용함수","url":"#my-heading-1","items":[]},{"depth":1,"title":"log-odd란 뭘까?","url":"#my-heading-2","items":[{"depth":2,"title":"Odds란?","url":"#my-heading-3","items":[]},{"depth":2,"title":"Logit (=Log-odds)란?","url":"#my-heading-4","items":[]},{"depth":2,"title":"Logistic Function (=시그모이드 함수)","url":"#my-heading-5","items":[]}]},{"depth":1,"title":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo>−</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy=\"false\">(</mo><mi>σ</mi><mo stretchy=\"false\">(</mo><msup><mi>z</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">-log(\\sigma(z^{(i)}))</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.138em;vertical-align:-0.25em;\"></span><span class=\"mord\">−</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.888em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathnormal mtight\">i</span><span class=\"mclose mtight\">)</span></span></span></span></span></span></span></span></span><span class=\"mclose\">))</span></span></span></span> 는 뭘 의미할까","url":"#my-heading-6","items":[]},{"depth":1,"title":"참고","url":"#my-heading-7","items":[]}]},"readingTime":{"text":"2 min read","minutes":1.492,"time":89520,"words":746},"frontmatter":{"slug":"ml-textbook-1","title":"로지스틱 회귀의 비용 함수는 왜 이렇게 생겼을까?","description":"로지스틱 회귀의 비용 함수는 왜 이렇게 생겼을까?","thumbnail":"./cover.png","categories":["ML","short"],"tags":["로지스틱","회귀","손실함수"],"createdAt":"2024/12/26","updatedAt":null,"featured":false,"locale":null}}},"staticQueryHashes":["3461282698"],"slicesMap":{}}