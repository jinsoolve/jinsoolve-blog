{"componentChunkName":"component---src-templates-featured-page-tsx","path":"/allFeaturedPosts/ML/3/","result":{"data":{"allMdx":{"totalCount":22,"nodes":[{"frontmatter":{"thumbnail":null,"title":"12장 다층 인공 신경망을 밑바닥부터 구현","updatedAt":null,"createdAt":"2025/01/09","description":"딥러닝은 인공 신경망을 효과적으로 학습시키기 위한 머신러닝의 하위분야이다. 아래 내용을 소개하겠다.- 다층 신경망 개념- 역전파 알고리즘- 이미지 분류를 위한 다층 신경망 훈련","slug":"ml-textbook-12","categories":["ML"]},"excerpt":"딥러닝은 인공 신경망을 효과적으로 학습시키기 위한 머신러닝의 하위분야이다. 아래 내용을 소개하겠다. 다층 신경망 개념 역전파 알고리즘 이미지 분류를 위한 다층 신경망 훈련\n\n인…"},{"frontmatter":{"thumbnail":null,"title":"가중치 w와 L2 규제","updatedAt":null,"createdAt":"2025/01/06","description":"왜 norm을 사용하는 것이 과대적합을 해결할까? 그 이유를 살펴보자.","slug":"ml-textbook-2","categories":["ML"]},"excerpt":"모델이 과대적합이 되었을 때 우리는 norm을 통해 이를 해결한다. 그런데 왜 norm을 사용하는 것이 과대적합을 해결할까? 그 이유를 살펴보자.\n\n모델이 과도하게 훈련 데이터에만…"},{"frontmatter":{"thumbnail":null,"title":"머신러닝 분류 모델들","updatedAt":null,"createdAt":"2025/01/06","description":"머신러닝에 사용하는 다양한 분류 모델들에 대해서 얘기해 보겠다.","slug":"ml-textbook-3","categories":["ML"]},"excerpt":"머신러닝에 사용하는 다양한 분류 모델들에 대해서 얘기해 보겠다. SVM(Support Vector Machine)이라 불리는 이 분류 모델은 클래스를 구분하는 hyper…"},{"frontmatter":{"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c8138af222d0a534b9d3f8006c1ec625/c497a/cover.png","srcSet":"/static/c8138af222d0a534b9d3f8006c1ec625/88418/cover.png 193w,\n/static/c8138af222d0a534b9d3f8006c1ec625/5b669/cover.png 385w,\n/static/c8138af222d0a534b9d3f8006c1ec625/c497a/cover.png 770w","sizes":"(min-width: 770px) 770px, 100vw"},"sources":[{"srcSet":"/static/c8138af222d0a534b9d3f8006c1ec625/e91f5/cover.webp 193w,\n/static/c8138af222d0a534b9d3f8006c1ec625/94672/cover.webp 385w,\n/static/c8138af222d0a534b9d3f8006c1ec625/75d07/cover.webp 770w","type":"image/webp","sizes":"(min-width: 770px) 770px, 100vw"}]},"width":770,"height":709.9999999999999}}},"title":"로지스틱 회귀의 비용 함수는 왜 이렇게 생겼을까?","updatedAt":null,"createdAt":"2024/12/26","description":"로지스틱 회귀의 비용 함수는 왜 이렇게 생겼을까?","slug":"ml-textbook-1","categories":["ML","short"]},"excerpt":"로지스틱 회귀의 비용함수는 다음과 같이 생겼다. J(w)=−1m∑i=1m[y(i)log(σ(z(i)))+(1−y(i))log(1−σ(z(i)))]J(w) = -\\frac{1}{m…"}],"pageInfo":{"currentPage":3,"pageCount":3}},"ogimage":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/f689f8b3b021090c7d4495f61be56e2f/ee21c/og-image.png","srcSet":"/static/f689f8b3b021090c7d4495f61be56e2f/5fe58/og-image.png 256w,\n/static/f689f8b3b021090c7d4495f61be56e2f/5ca6c/og-image.png 512w,\n/static/f689f8b3b021090c7d4495f61be56e2f/ee21c/og-image.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/f689f8b3b021090c7d4495f61be56e2f/e818d/og-image.webp 256w,\n/static/f689f8b3b021090c7d4495f61be56e2f/7948c/og-image.webp 512w,\n/static/f689f8b3b021090c7d4495f61be56e2f/aa8b2/og-image.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":600}},"profileImage":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/9379464f2e13cd33a2639502a84c1285/4f0b5/profile.jpg","srcSet":"/static/9379464f2e13cd33a2639502a84c1285/cb2c7/profile.jpg 103w,\n/static/9379464f2e13cd33a2639502a84c1285/66f23/profile.jpg 207w,\n/static/9379464f2e13cd33a2639502a84c1285/4f0b5/profile.jpg 413w","sizes":"(min-width: 413px) 413px, 100vw"},"sources":[{"srcSet":"/static/9379464f2e13cd33a2639502a84c1285/8e689/profile.webp 103w,\n/static/9379464f2e13cd33a2639502a84c1285/43852/profile.webp 207w,\n/static/9379464f2e13cd33a2639502a84c1285/f76dc/profile.webp 413w","type":"image/webp","sizes":"(min-width: 413px) 413px, 100vw"}]},"width":413,"height":531}},"shortPosts":{"nodes":[{"frontmatter":{"title":"RNN","updatedAt":null,"createdAt":"2025/04/27","slug":"intro-to-nlp-7","tags":null,"categories":["ML"]},"excerpt":"언어 모델이라는 건, 사실 다음에 올 단어를 확률로 예측하는 것이다. 이러한 언어 모델들을 어떻게 발전시켜왔는 지 살펴보자.\n\n이미 이전 포스트에서 자세히 살펴보았던 내용이다.…"},{"frontmatter":{"title":"Fancy RNN","updatedAt":null,"createdAt":"2025/04/27","slug":"intro-to-nlp-8","tags":null,"categories":["ML"]},"excerpt":"이전 포스트에서 RNN에서 Vanishing Gradient로 인해 장기 의존성 문제가 있다는 사실을 이야기했다. 이런 Vanishing Gradient를 해결하기 위해 크게…"},{"frontmatter":{"title":"Attention","updatedAt":null,"createdAt":"2025/04/27","slug":"intro-to-nlp-9","tags":null,"categories":["ML"]},"excerpt":"기존 RNN의 병목 현상을 해결하기 위해 Attention이 등장했다. Decoder에서 한 단어를 예상할 때, 해당 단어와 특별히 관련되어 있는 Encoder의 특정 단어를…"},{"frontmatter":{"title":"Transformers","updatedAt":null,"createdAt":"2025/04/27","slug":"intro-to-nlp-10","tags":null,"categories":["ML"]},"excerpt":"Seq2Seq에서 RNN을 아예 빼버리고 attention으로 구성해보면 어떨까? → Transformer의 구조 전에 seq2seq에서 decoder의 query와…"},{"frontmatter":{"title":"Word2Vec","updatedAt":null,"createdAt":"2025/04/26","slug":"intro-to-nlp-4","tags":null,"categories":["ML"]},"excerpt":"저번에 word vector에 대해서 알아봤는데, 이번에는 word vector의 프레임워크인 Word2Vec에 대해서 좀 더 알아보자. 먼저 Word2Vec이 어떤 식으로…"},{"frontmatter":{"title":"Word Vectors","updatedAt":null,"createdAt":"2025/04/25","slug":"intro-to-nlp-3","tags":null,"categories":["ML"]},"excerpt":"이번 포스트에서는 Word Vector에 대해서 알아보자. 먼저 아래와 같은 고민을 해보자.\n 우리는 어떻게 컴퓨터에게 단어의 뜻을 이해시킬 수 있을까?\n\nWordNet?#\n\n예전에…"},{"frontmatter":{"title":"n-gram Language Models","updatedAt":null,"createdAt":"2025/03/10","slug":"intro-to-nlp-2","tags":["n-gram","sampling","model-evaluation","smoothing"],"categories":["ML"]},"excerpt":"언어 모델이란, 결국에는 그 다음으로 어떤 단어가 오는 것이 가장 자연스러운지를 확률로 보고 가장 높은 확률의 단어를 선택해서 문장을 구성하는 방식이다. 위 포스트에서는 n…"},{"frontmatter":{"title":"머신러닝 교과서 8장","updatedAt":null,"createdAt":"2025/01/09","slug":"ml-textbook-9","tags":["NLP","감정-분석"],"categories":["ML"]},"excerpt":"자연어 처리(Natural Language Processing, NLP)의 하위 분야인 감성 분석(sentiment analysis)에 대해서 알아보는 장이다. 영화 리뷰를 읽고…"},{"frontmatter":{"title":"머신러닝 교과서 7장: 앙상블 학습","updatedAt":null,"createdAt":"2025/01/09","slug":"ml-textbook-8","tags":["앙상블"],"categories":["ML"]},"excerpt":"다수결 투표 기반 모델(보팅) -> 최빈값을 선택 랜덤하게 복원추출하여(배깅) 과대적합 감소 앞선 모델의 오차를 학습하여 강력 모델 구축(부스팅) 앙상블 모델은 거의 항상…"},{"frontmatter":{"title":"모델 평가와 하이퍼 파라미터 튜닝의 모범 사례","updatedAt":null,"createdAt":"2025/01/09","slug":"ml-textbook-7","tags":["hyperparameter-tuning"],"categories":["ML"]},"excerpt":"해당 내용은 머신러닝 교과서 with 파이썬, 사이킷런, 텐서플로의 장 내용을 기반으로 작성되었다. 우리는 모델을 훈련시킬 때 데이터를 전처리한 후에 학습시킨다. 그럼 테스트…"},{"frontmatter":{"title":"표준화와 정규화","updatedAt":null,"createdAt":"2025/01/09","slug":"ml-textbook-4","tags":["표준화","정규화"],"categories":["ML","short"]},"excerpt":"ex = np.array([0, 1, 2, 3, 4, 5]) print('표준화:', (ex - ex.mean()) / ex.std())\n\n\n평균=0, 분산=1로 만든다.\n\nfr…"},{"frontmatter":{"title":"차원 축소를 사용한 데이터 압축","updatedAt":null,"createdAt":"2025/01/09","slug":"ml-textbook-6","tags":["PCA","LDA","차원축소"],"categories":["ML"]},"excerpt":"이 글은 머신러닝 교과서 with 파이썬, 사이킷런, 텐서플로의 5장 내용을 기반으로 작성되었다. 차원 축소를 하는 이유는 다양하다. 저장 공간을 줄이고 계산 효율을 높이고…"},{"frontmatter":{"title":"데이터 전처리","updatedAt":null,"createdAt":"2025/01/09","slug":"ml-textbook-5","tags":null,"categories":["ML"]},"excerpt":"해당 내용은 머신러닝 교과서 with 파이썬, 사이킷런, 텐서플로의 4장 내용을 기반으로 작성되었다. isnull().sum()등을 이용하여 누락된 값을 확인한다.\n\n누락된…"},{"frontmatter":{"title":"15장 심층 합성곱 신경망으로 이미지 분류","updatedAt":null,"createdAt":"2025/01/09","slug":"ml-textbook-14","tags":null,"categories":["ML"]},"excerpt":"합성곱 신경망은 컴퓨터 비전을 위한 머신러닝 분야를 크게 발전시켰다. CNN(Convolutional Neural Network)는 뇌의 시각 피질이 물체를 인식할 때 동작하는…"},{"frontmatter":{"title":"16장 순환 신경망으로 순차 데이터 모델링","updatedAt":null,"createdAt":"2025/01/09","slug":"ml-textbook-15","tags":null,"categories":["ML"]},"excerpt":"데이터 간의 순서가 중요한 데이터를 시퀀스 데이터라고 한다. 시퀀스 데이터를 처리해주기 위해서는 RNN(Recurrent Neural Network) 모델을 사용해서 처리해야 한다…"}]},"featuredPosts":{"nodes":[{"frontmatter":{"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/076eddc91e0d4685f11210bb0b088b7d/06a5c/cover.png","srcSet":"/static/076eddc91e0d4685f11210bb0b088b7d/2ed3a/cover.png 142w,\n/static/076eddc91e0d4685f11210bb0b088b7d/f4be8/cover.png 284w,\n/static/076eddc91e0d4685f11210bb0b088b7d/06a5c/cover.png 568w","sizes":"(min-width: 568px) 568px, 100vw"},"sources":[{"srcSet":"/static/076eddc91e0d4685f11210bb0b088b7d/bcdd4/cover.webp 142w,\n/static/076eddc91e0d4685f11210bb0b088b7d/5d551/cover.webp 284w,\n/static/076eddc91e0d4685f11210bb0b088b7d/47346/cover.webp 568w","type":"image/webp","sizes":"(min-width: 568px) 568px, 100vw"}]},"width":568,"height":568}}},"title":"RNN","updatedAt":null,"createdAt":"2025/04/27","description":null,"slug":"intro-to-nlp-7","categories":["ML"]},"excerpt":"언어 모델이라는 건, 사실 다음에 올 단어를 확률로 예측하는 것이다. 이러한 언어 모델들을 어떻게 발전시켜왔는 지 살펴보자.\n\n이미 이전 포스트에서 자세히 살펴보았던 내용이다.…"},{"frontmatter":{"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/85dad8627ae6845b62f5bb965c291b19/808a1/cover.png","srcSet":"/static/85dad8627ae6845b62f5bb965c291b19/74f11/cover.png 294w,\n/static/85dad8627ae6845b62f5bb965c291b19/bbc95/cover.png 588w,\n/static/85dad8627ae6845b62f5bb965c291b19/808a1/cover.png 1176w","sizes":"(min-width: 1176px) 1176px, 100vw"},"sources":[{"srcSet":"/static/85dad8627ae6845b62f5bb965c291b19/c143a/cover.webp 294w,\n/static/85dad8627ae6845b62f5bb965c291b19/e86a6/cover.webp 588w,\n/static/85dad8627ae6845b62f5bb965c291b19/8199d/cover.webp 1176w","type":"image/webp","sizes":"(min-width: 1176px) 1176px, 100vw"}]},"width":1176,"height":522}}},"title":"Word2Vec","updatedAt":null,"createdAt":"2025/04/26","description":null,"slug":"intro-to-nlp-4","categories":["ML"]},"excerpt":"저번에 word vector에 대해서 알아봤는데, 이번에는 word vector의 프레임워크인 Word2Vec에 대해서 좀 더 알아보자. 먼저 Word2Vec이 어떤 식으로…"},{"frontmatter":{"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/3984361aed6a71f465c0ae457e7eff24/558a4/cover.png","srcSet":"/static/3984361aed6a71f465c0ae457e7eff24/0701a/cover.png 164w,\n/static/3984361aed6a71f465c0ae457e7eff24/4c324/cover.png 328w,\n/static/3984361aed6a71f465c0ae457e7eff24/558a4/cover.png 656w","sizes":"(min-width: 656px) 656px, 100vw"},"sources":[{"srcSet":"/static/3984361aed6a71f465c0ae457e7eff24/5da8b/cover.webp 164w,\n/static/3984361aed6a71f465c0ae457e7eff24/fab23/cover.webp 328w,\n/static/3984361aed6a71f465c0ae457e7eff24/31f9d/cover.webp 656w","type":"image/webp","sizes":"(min-width: 656px) 656px, 100vw"}]},"width":656,"height":381}}},"title":"Word Vectors","updatedAt":null,"createdAt":"2025/04/25","description":null,"slug":"intro-to-nlp-3","categories":["ML"]},"excerpt":"이번 포스트에서는 Word Vector에 대해서 알아보자. 먼저 아래와 같은 고민을 해보자.\n 우리는 어떻게 컴퓨터에게 단어의 뜻을 이해시킬 수 있을까?\n\nWordNet?#\n\n예전에…"}]}},"pageContext":{"limit":9,"skip":18,"numPages":3,"currentPage":3,"category":"ML"}},"staticQueryHashes":["3461282698","655531792"],"slicesMap":{}}